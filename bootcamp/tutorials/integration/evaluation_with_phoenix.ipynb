{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/integration/evaluation_with_phoenix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Arize Pheonix\n",
    "\n",
    "This guide demonstrates how to use [Arize Pheonix](https://phoenix.arize.com/) to evaluate a Retrieval-Augmented Generation (RAG) pipeline built upon [Milvus](https://milvus.io/).\n",
    "\n",
    "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
    "\n",
    "Arize Pheonix is a framework that helps you evaluate your RAG pipelines. There are existing tools and frameworks that help you build these pipelines but evaluating it and quantifying your pipeline performance can be hard. This is where Arize Pheonix comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have the following dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (2.4.6)\n",
      "Requirement already satisfied: openai in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (1.46.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.47.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: requests in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: pandas in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (2.2.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: arize-phoenix>=4.29.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (4.35.1)\n",
      "Collecting arize-phoenix>=4.29.0\n",
      "  Downloading arize_phoenix-4.36.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nest_asyncio in /Users/eureka/.local/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: setuptools>69 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (72.1.0)\n",
      "Requirement already satisfied: grpcio>=1.49.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (1.63.2)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (4.25.4)\n",
      "Requirement already satisfied: environs<=9.5.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (9.5.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pymilvus) (2.4.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/eureka/.local/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/eureka/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aioitertools in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.12.0)\n",
      "Requirement already satisfied: aiosqlite in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.20.0)\n",
      "Requirement already satisfied: alembic<2,>=1.3.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.13.2)\n",
      "Requirement already satisfied: arize-phoenix-evals>=0.13.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.16.0)\n",
      "Requirement already satisfied: arize-phoenix-otel>=0.4.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.5.1)\n",
      "Requirement already satisfied: cachetools in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.115.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.33 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.8.38.post1)\n",
      "Requirement already satisfied: jinja2 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (3.1.4)\n",
      "Requirement already satisfied: openinference-instrumentation-langchain>=0.1.26 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.1.28)\n",
      "Requirement already satisfied: openinference-instrumentation-llama-index>=2.2.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (3.0.2)\n",
      "Requirement already satisfied: openinference-instrumentation-openai>=0.1.11 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.1.14)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.12 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.1.18)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.9 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.1.10)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.48b0)\n",
      "Requirement already satisfied: psutil in /Users/eureka/.local/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (17.0.0)\n",
      "Requirement already satisfied: pyjwt in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (2.9.0)\n",
      "Requirement already satisfied: python-multipart in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.0.9)\n",
      "Requirement already satisfied: scikit-learn in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=4.29.0) (2.0.34)\n",
      "Requirement already satisfied: sqlean-py>=3.45.1 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (3.45.1)\n",
      "Requirement already satisfied: starlette in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.38.5)\n",
      "Requirement already satisfied: strawberry-graphql==0.236.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.236.0)\n",
      "Requirement already satisfied: umap-learn in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.5.6)\n",
      "Requirement already satisfied: uvicorn in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (0.30.6)\n",
      "Requirement already satisfied: wrapt in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from arize-phoenix>=4.29.0) (1.16.0)\n",
      "Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from strawberry-graphql==0.236.0->arize-phoenix>=4.29.0) (3.2.4)\n",
      "Requirement already satisfied: Mako in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from alembic<2,>=1.3.0->arize-phoenix>=4.29.0) (1.3.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/eureka/.local/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from environs<=9.5.0->pymilvus) (3.22.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from hdbscan>=0.8.33->arize-phoenix>=4.29.0) (1.4.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: opentelemetry-api in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openinference-instrumentation>=0.1.12->arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from openinference-instrumentation-langchain>=0.1.26->arize-phoenix>=4.29.0) (0.48b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eureka/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from scikit-learn->arize-phoenix>=4.29.0) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix>=4.29.0) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from jinja2->arize-phoenix>=4.29.0) (2.1.5)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.27.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.27.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp->arize-phoenix>=4.29.0) (1.27.0)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix>=4.29.0) (7.0.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from umap-learn->arize-phoenix>=4.29.0) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from umap-learn->arize-phoenix>=4.29.0) (0.5.13)\n",
      "Requirement already satisfied: click>=7.0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from uvicorn->arize-phoenix>=4.29.0) (8.1.7)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/eureka/.local/lib/python3.9/site-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn->arize-phoenix>=4.29.0) (0.43.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/eureka/.local/lib/python3.9/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix>=4.29.0) (3.20.1)\n",
      "Downloading openai-1.47.1-py3-none-any.whl (375 kB)\n",
      "Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading arize_phoenix-4.36.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas, openai, arize-phoenix\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.46.0\n",
      "    Uninstalling openai-1.46.0:\n",
      "      Successfully uninstalled openai-1.46.0\n",
      "  Attempting uninstall: arize-phoenix\n",
      "    Found existing installation: arize-phoenix 4.35.1\n",
      "    Uninstalling arize-phoenix-4.35.1:\n",
      "      Successfully uninstalled arize-phoenix-4.35.1\n",
      "Successfully installed arize-phoenix-4.36.0 openai-1.47.1 pandas-2.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pymilvus openai requests tqdm pandas \"arize-phoenix>=4.29.0\" nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use OpenAI as the LLM in this example. You should prepare the [api key](https://platform.openai.com/docs/quickstart) `OPENAI_API_KEY` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-*****************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RAG pipeline\n",
    "\n",
    "We will define the RAG class that use Milvus as the vector store, and OpenAI as the LLM.\n",
    "The class contains the `load` method, which loads the text data into Milvus, the `retrieve` method, which retrieves the most similar text data to the given question, and the `answer` method, which answers the given question with the retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    \"\"\"\n",
    "    RAG(Retrieval-Augmented Generation) class built upon OpenAI and Milvus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, openai_client: OpenAI, milvus_client: MilvusClient):\n",
    "        self._prepare_openai(openai_client)\n",
    "        self._prepare_milvus(milvus_client)\n",
    "\n",
    "    def _emb_text(self, text: str) -> List[float]:\n",
    "        return (\n",
    "            self.openai_client.embeddings.create(input=text, model=self.embedding_model)\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "\n",
    "    def _prepare_openai(\n",
    "        self,\n",
    "        openai_client: OpenAI,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "    ):\n",
    "        self.openai_client = openai_client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        self.SYSTEM_PROMPT = \"\"\"\n",
    "            Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "        \"\"\"\n",
    "        self.USER_PROMPT = \"\"\"\n",
    "            Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "        \"\"\"\n",
    "\n",
    "    def _prepare_milvus(\n",
    "        self, milvus_client: MilvusClient, collection_name: str = \"rag_collection\"\n",
    "    ):\n",
    "        self.milvus_client = milvus_client\n",
    "        self.collection_name = collection_name\n",
    "        if self.milvus_client.has_collection(self.collection_name):\n",
    "            self.milvus_client.drop_collection(self.collection_name)\n",
    "        embedding_dim = len(self._emb_text(\"demo\"))\n",
    "        self.milvus_client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            dimension=embedding_dim,\n",
    "            metric_type=\"IP\",\n",
    "            consistency_level=\"Strong\",\n",
    "        )\n",
    "\n",
    "    def load(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Load the text data into Milvus.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for i, line in enumerate(tqdm(texts, desc=\"Creating embeddings\")):\n",
    "            data.append({\"id\": i, \"vector\": self._emb_text(line), \"text\": line})\n",
    "        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n",
    "\n",
    "    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve the most similar text data to the given question.\n",
    "        \"\"\"\n",
    "        search_res = self.milvus_client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[self._emb_text(question)],\n",
    "            limit=top_k,\n",
    "            search_params={\"metric_type\": \"IP\", \"params\": {}},  # inner product distance\n",
    "            output_fields=[\"text\"],  # Return the text field\n",
    "        )\n",
    "        retrieved_texts = [res[\"entity\"][\"text\"] for res in search_res[0]]\n",
    "        return retrieved_texts[:top_k]\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieval_top_k: int = 3,\n",
    "        return_retrieved_text: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Answer the given question with the retrieved knowledge.\n",
    "        \"\"\"\n",
    "        retrieved_texts = self.retrieve(question, top_k=retrieval_top_k)\n",
    "        user_prompt = self.USER_PROMPT.format(\n",
    "            context=\"\\n\".join(retrieved_texts), question=question\n",
    "        )\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        if not return_retrieved_text:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            return response.choices[0].message.content, retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the RAG class with OpenAI and Milvus clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
    "\n",
    "my_rag = RAG(openai_client=openai_client, milvus_client=milvus_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "> As for the argument of `MilvusClient`:\n",
    "> - Setting the `uri` as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
    "> - If you have large scale of data, you can set up a more performant Milvus server on [docker or kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
    "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#free-cluster-details) in Zilliz Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "## Run the RAG pipeline and get results\n",
    "\n",
    "We use the [Milvus development guide](https://github.com/milvus-io/milvus/blob/master/DEVELOPMENT.md) to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline.\n",
    "\n",
    "Download it and load it into the rag pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 47/47 [00:12<00:00,  3.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md\"\n",
    "file_path = \"./Milvus_DEVELOPMENT.md\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "text_lines = file_text.split(\"# \")\n",
    "my_rag.load(text_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a query question about the content of the development guide documentation. And then use the `answer` method to get the answer and the retrieved context texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The hardware requirements specification to build and run Milvus from source code is as follows:\\n\\n- 8GB of RAM\\n- 50GB of free disk space',\n",
       " ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##',\n",
       "  'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##',\n",
       "  \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the hardware requirements specification if I want to build Milvus and run from source code?\"\n",
    "my_rag.answer(question, return_retrieved_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare some questions with its corresponding ground truth answers. We get answers and contexts from our RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eureka/miniconda3/envs/zilliz/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Answering questions: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the hardware requirements specificatio...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>The hardware requirements specification to bui...</td>\n",
       "      <td>If you want to build Milvus and run from sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the programming language used to write...</td>\n",
       "      <td>[CMake &amp; Conan\\n\\nThe algorithm library of Mil...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should be ensured before running code cov...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>Before running code coverage, it should be ens...</td>\n",
       "      <td>Before running code coverage, you should make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what is the hardware requirements specificatio...   \n",
       "1  What is the programming language used to write...   \n",
       "2  What should be ensured before running code cov...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...   \n",
       "1  [CMake & Conan\\n\\nThe algorithm library of Mil...   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The hardware requirements specification to bui...   \n",
       "1  The programming language used to write Knowher...   \n",
       "2  Before running code coverage, it should be ens...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  If you want to build Milvus and run from sourc...  \n",
       "1  The programming language used to write Knowher...  \n",
       "2  Before running code coverage, you should make ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "question_list = [\n",
    "    \"what is the hardware requirements specification if I want to build Milvus and run from source code?\",\n",
    "    \"What is the programming language used to write Knowhere?\",\n",
    "    \"What should be ensured before running code coverage?\",\n",
    "]\n",
    "ground_truth_list = [\n",
    "    \"If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.\",\n",
    "    \"The programming language used to write Knowhere is C++.\",\n",
    "    \"Before running code coverage, you should make sure that your code changes are covered by unit tests.\",\n",
    "]\n",
    "contexts_list = []\n",
    "answer_list = []\n",
    "for question in tqdm(question_list, desc=\"Answering questions\"):\n",
    "    answer, contexts = my_rag.answer(question, return_retrieved_text=True)\n",
    "    contexts_list.append(contexts)\n",
    "    answer_list.append(answer)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": question_list,\n",
    "        \"contexts\": contexts_list,\n",
    "        \"answer\": answer_list,\n",
    "        \"ground_truth\": ground_truth_list,\n",
    "    }\n",
    ")\n",
    "rag_results = Dataset.from_pandas(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Arize Phoenix\n",
    "\n",
    "We use Arize Phoenix to evaluate our retrieval-augmented generation (RAG) pipeline, focusing on two key metrics:\n",
    "\n",
    "- **Hallucination Evaluation**: Determines if the content is factual or hallucinatory (information not grounded in context), ensuring data integrity.\n",
    "  - **Hallucination Explanation**: Explains why a response is factual or not.\n",
    "  \n",
    "- **QA Evaluation**: Assesses the accuracy of model answers to input queries.\n",
    "  - **QA Explanation**: Details why an answer is correct or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phoenix Tracing Overview\n",
    "\n",
    "Phoenix provides **OTEL-compatible tracing** for LLM applications, with integrations for frameworks like **Langchain**, **LlamaIndex**, and SDKs such as **OpenAI** and **Mistral**. Tracing captures the entire request flow, offering insights into:\n",
    "\n",
    "- **Application Latency**: Identify and optimize slow LLM invocations and component performance.\n",
    "- **Token Usage**: Break down token consumption for cost optimization.\n",
    "- **Runtime Exceptions**: Capture critical issues like rate-limiting.\n",
    "- **Retrieved Documents**: Analyze document retrieval, score, and order.\n",
    "\n",
    "By utilizing Phoenix’s tracing, you can **identify bottlenecks**, **optimize resources**, and **ensure system reliability** across various frameworks and languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace.openai import OpenAIInstrumentor\n",
    "\n",
    "# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\n",
    "session = px.launch_app()\n",
    "\n",
    "# Initialize OpenAI auto-instrumentation\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](../../../images/phoenix01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_evals |██████████| 6/6 (100.0%) | ⏳ 00:03<00:00 |  1.56it/s\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n",
    "\n",
    "nest_asyncio.apply()  # This is needed for concurrency in notebook environments\n",
    "\n",
    "# Set your OpenAI API key\n",
    "eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "\n",
    "# Define your evaluators\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_evaluator = QAEvaluator(eval_model)\n",
    "\n",
    "# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n",
    "# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n",
    "# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\n",
    "df[\"context\"] = df[\"contexts\"]\n",
    "df[\"reference\"] = df[\"contexts\"]\n",
    "df.rename(columns={\"question\": \"input\", \"answer\": \"output\"}, inplace=True)\n",
    "assert all(\n",
    "    column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"]\n",
    ")\n",
    "\n",
    "# Run the evaluators, each evaluator will return a dataframe with evaluation results\n",
    "# We upload the evaluation results to Phoenix in the next step\n",
    "hallucination_eval_df, qa_eval_df = run_evals(\n",
    "    dataframe=df,\n",
    "    evaluators=[hallucination_evaluator, qa_evaluator],\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>contexts</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context</th>\n",
       "      <th>reference</th>\n",
       "      <th>hallucination_eval</th>\n",
       "      <th>hallucination_explanation</th>\n",
       "      <th>qa_eval</th>\n",
       "      <th>qa_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the hardware requirements specificatio...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>The hardware requirements specification to bui...</td>\n",
       "      <td>If you want to build Milvus and run from sourc...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>factual</td>\n",
       "      <td>To determine if the answer is factual or hallu...</td>\n",
       "      <td>correct</td>\n",
       "      <td>First, I will identify the key elements of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the programming language used to write...</td>\n",
       "      <td>[CMake &amp; Conan\\n\\nThe algorithm library of Mil...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "      <td>[CMake &amp; Conan\\n\\nThe algorithm library of Mil...</td>\n",
       "      <td>[CMake &amp; Conan\\n\\nThe algorithm library of Mil...</td>\n",
       "      <td>factual</td>\n",
       "      <td>To determine if the answer is factual or hallu...</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should be ensured before running code cov...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>Before running code coverage, it should be ens...</td>\n",
       "      <td>Before running code coverage, you should make ...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>factual</td>\n",
       "      <td>To determine if the answer is factual or hallu...</td>\n",
       "      <td>correct</td>\n",
       "      <td>To determine if the answer is correct, we need...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  what is the hardware requirements specificatio...   \n",
       "1  What is the programming language used to write...   \n",
       "2  What should be ensured before running code cov...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...   \n",
       "1  [CMake & Conan\\n\\nThe algorithm library of Mil...   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...   \n",
       "\n",
       "                                              output  \\\n",
       "0  The hardware requirements specification to bui...   \n",
       "1  The programming language used to write Knowher...   \n",
       "2  Before running code coverage, it should be ens...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  If you want to build Milvus and run from sourc...   \n",
       "1  The programming language used to write Knowher...   \n",
       "2  Before running code coverage, you should make ...   \n",
       "\n",
       "                                             context  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...   \n",
       "1  [CMake & Conan\\n\\nThe algorithm library of Mil...   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...   \n",
       "\n",
       "                                           reference hallucination_eval  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...            factual   \n",
       "1  [CMake & Conan\\n\\nThe algorithm library of Mil...            factual   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...            factual   \n",
       "\n",
       "                           hallucination_explanation  qa_eval  \\\n",
       "0  To determine if the answer is factual or hallu...  correct   \n",
       "1  To determine if the answer is factual or hallu...  correct   \n",
       "2  To determine if the answer is factual or hallu...  correct   \n",
       "\n",
       "                                      qa_explanation  \n",
       "0  First, I will identify the key elements of the...  \n",
       "1  To determine if the answer is correct, we need...  \n",
       "2  To determine if the answer is correct, we need...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = df.copy()\n",
    "results_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\n",
    "results_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\n",
    "results_df[\"qa_eval\"] = qa_eval_df[\"label\"]\n",
    "results_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zilliz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
