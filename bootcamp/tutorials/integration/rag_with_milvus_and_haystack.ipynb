{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/integration/rag_with_milvus_and_haystack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Milvus and Haystack\n",
    "\n",
    "This guide demonstrates how to build a Retrieval-Augmented Generation (RAG) system using Haystack and Milvus.\n",
    "\n",
    "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using a vector similarity search engine like Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
    "\n",
    "[Haystack](https://haystack.deepset.ai/) is the open source Python framework by deepset for building custom apps with large language models (LLMs). [Milvus](https://milvus.io/) is the world's most advanced open-source vector database, built to power embedding similarity search and AI applications.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have the following dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet pymilvus milvus-haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will use the models from OpenAI. You should prepare the [api key](https://platform.openai.com/docs/quickstart) `OPENAI_API_KEY` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-***********\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We use a popular online [essay](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt) to be used as private knowledge in our RAG, which is a good data source for a simple RAG pipeline.\n",
    "\n",
    "Download it and save it as a local text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "file_path = \"./paul_graham_essay.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create the indexing Pipeline\n",
    "\n",
    "Create an indexing pipeline that converts the text into documents, splits them into sentences, and embeds them. The documents are then written to the Milvus document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting markdown files to Documents: 100%|█| 1/\n",
      "Calculating embeddings: 100%|█| 12/12 [00:07<00:00\n",
      "E20240515 20:52:54.001194 4751750 milvus_local.cpp:189] [SERVER][GetCollection][] Collecton HaystackCollection not existed\n",
      "E20240515 20:52:54.001967 4751750 milvus_local.cpp:189] [SERVER][GetCollection][] Collecton HaystackCollection not existed\n",
      "E20240515 20:52:54.002050 4751750 milvus_local.cpp:189] [SERVER][GetCollection][] Collecton HaystackCollection not existed\n",
      "E20240515 20:52:54.002069 4751750 milvus_local.cpp:189] [SERVER][GetCollection][] Collecton HaystackCollection not existed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 368\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters import MarkdownToDocument\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "from milvus_haystack import MilvusDocumentStore\n",
    "from milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n",
    "\n",
    "document_store = MilvusDocumentStore(\n",
    "    connection_args={\"uri\": \"./milvus.db\"},\n",
    "    drop_old=True,\n",
    ")\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", MarkdownToDocument())\n",
    "indexing_pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2))\n",
    "indexing_pipeline.add_component(\"embedder\", OpenAIDocumentEmbedder())\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "indexing_pipeline.run({\"converter\": {\"sources\": [file_path]}})\n",
    "\n",
    "print(\"Number of documents:\", document_store.count_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create the retrieval pipeline\n",
    "\n",
    "Create a retrieval pipeline that retrieves documents from the Milvus document store using a vector similarity search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
      "----------\n",
      " They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
      "\n",
      "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\n",
      "----------\n",
      "\n",
      "\n",
      "So in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software, which in those days meant Windows software.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the first programs the author tried writing?\"\n",
    "\n",
    "retrieval_pipeline = Pipeline()\n",
    "retrieval_pipeline.add_component(\"embedder\", OpenAITextEmbedder())\n",
    "retrieval_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\n",
    "retrieval_pipeline.connect(\"embedder\", \"retriever\")\n",
    "\n",
    "retrieval_results = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n",
    "\n",
    "for doc in retrieval_results[\"retriever\"][\"documents\"]:\n",
    "    print(doc.content)\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create the RAG pipeline\n",
    "\n",
    "Create a RAG pipeline that combines the MilvusEmbeddingRetriever and the OpenAIGenerator to answer the question using the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG answer: The first programs the author tried writing were on the IBM 1401 used by the school district for data processing.\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "prompt_template = \"\"\"Answer the following query based on the provided context. If the context does\n",
    "                     not include an answer, reply with 'I don't know'.\\n\n",
    "                     Query: {{query}}\n",
    "                     Documents:\n",
    "                     {% for doc in documents %}\n",
    "                        {{ doc.content }}\n",
    "                     {% endfor %}\n",
    "                     Answer:\n",
    "                  \"\"\"\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
    "rag_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\n",
    "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipeline.add_component(\"generator\", OpenAIGenerator(api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\")),\n",
    "                                                        generation_kwargs={\"temperature\": 0}))\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "results = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "print('RAG answer:', results[\"generator\"][\"replies\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}