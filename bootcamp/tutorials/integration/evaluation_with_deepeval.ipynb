{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/integration/evaluation_with_deepeval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with DeepEval\n",
    "\n",
    "This guide demonstrates how to use DeepEval to evaluate a Retrieval-Augmented Generation (RAG) pipeline built upon Milvus.\n",
    "\n",
    "The RAG system combines a retrieval system with a generative model to generate new text based on a given prompt. The system first retrieves relevant documents from a corpus using Milvus, and then uses a generative model to generate new text based on the retrieved documents.\n",
    "\n",
    "DeepEval is a framework that helps you evaluate your RAG pipelines. There are existing tools and frameworks that help you build these pipelines but evaluating it and quantifying your pipeline performance can be hard. This is where DeepEval comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have the following dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pymilvus openai requests tqdm pandas deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If you are using Google Colab, to enable dependencies just installed, you may need to restart the runtime (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use OpenAI as the LLM in this example. You should prepare the api key `OPENAI_API_KEY` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-*****************\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RAG pipeline\n",
    "\n",
    "We will define the RAG class that use Milvus as the vector store, and OpenAI as the LLM. The class contains the load method, which loads the text data into Milvus, the `retrieve` method, which retrieves the most similar text data to the given question, and the `answer` method, which answers the given question with the retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "class RAG:\n",
    "    \"\"\"\n",
    "    RAG(Retrieval-Augmented Generation) class built upon OpenAI and Milvus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, openai_client: OpenAI, milvus_client: MilvusClient):\n",
    "        self._prepare_openai(openai_client)\n",
    "        self._prepare_milvus(milvus_client)\n",
    "    \n",
    "    def _emb_text(self, text: str) -> List[float]:\n",
    "        return (\n",
    "            self.openai_client.embeddings.create(input=text, model=self.embedding_model).data[0].embedding\n",
    "        )\n",
    "    \n",
    "    def _prepare_openai(\n",
    "        self,\n",
    "        openai_client: OpenAI,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "    ):\n",
    "        self.openai_client = openai_client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        self.SYSTEM_PROMPT = \"\"\"\n",
    "            Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "        \"\"\"\n",
    "        self.USER_PROMPT = \"\"\"\n",
    "            Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "        \"\"\"\n",
    "\n",
    "    def _prepare_milvus(\n",
    "        self, milvus_client: MilvusClient, collection_name: str = \"rag_collection\"\n",
    "    ):\n",
    "        self.milvus_client = milvus_client\n",
    "        self.collection_name = collection_name\n",
    "        if self.milvus_client.has_collection(self.collection_name):\n",
    "            self.milvus_client.drop_collection(self.collection_name)\n",
    "        embedding_dim = len(self._emb_text(\"demo\"))\n",
    "        self.milvus_client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            dimension=embedding_dim,\n",
    "            metric_type=\"IP\",\n",
    "            consistency_level=\"Strong\",\n",
    "        )\n",
    "    \n",
    "    def load(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Load the text data into Milvus.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for i,line in enumerate(tqdm(texts, desc=\"Creating embeddings\")):\n",
    "            data.append({\"id\": i, \"vector\": self._emb_text(line), \"text\": line})\n",
    "        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n",
    "    \n",
    "    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve the most similar text data to the given question.\n",
    "        \"\"\"\n",
    "        search_res = self.milvus_client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[self._emb_text(question)],\n",
    "            limit=top_k,\n",
    "            search_params={\"metric_type\": \"IP\", \"params\": {}},  #inner product distance\n",
    "            output_fields=[\"text\"], # Return the text field\n",
    "        )\n",
    "        retrieved_texts = [res[\"entity\"][\"text\"] for res in search_res[0]]\n",
    "        return retrieved_texts[:top_k]\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        question: str,\n",
    "        retrieval_top_k: int = 3,\n",
    "        return_retrieved_text: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Answer the given question with the retrieved knowledge.\n",
    "        \"\"\"\n",
    "        retrieved_texts = self.retrieve(question, top_k=retrieval_top_k)\n",
    "        user_prompt = self.USER_PROMPT.format(\n",
    "            context=\"\\n\".join(retrieved_texts), question=question\n",
    "        )\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        if not return_retrieved_text:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            return response.choices[0].message.content, retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the RAG class with OpenAI and Milvus clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
    "\n",
    "my_rag = RAG(openai_client=openai_client, milvus_client=milvus_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As for the argument of MilvusClient:\n",
    ">- Setting the uri as a local file, e.g../milvus.db, is the most convenient method, as it automatically utilizes Milvus Lite to store all data in this file.\n",
    ">- If you have large scale of data, you can set up a more performant Milvus server on docker or kubernetes. In this setup, please use the server uri, e.g.http://localhost:19530, as your uri.\n",
    ">- If you want to use Zilliz Cloud, the fully managed cloud service for Milvus, adjust the uri and token, which correspond to the Public Endpoint and Api key in Zilliz Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RAG pipeline and get results\n",
    "\n",
    "we use the [Milvus development guide](https://github.com/milvus-io/milvus/blob/master/DEVELOPMENT.md) to be as the private knowledge in our RAG, which is a good data source for a simple RAG pipeline.\n",
    "\n",
    "Download it and load it into the rag pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 47/47 [00:31<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md\"\n",
    "file_path = \"./Milvus_DEVELOPMENT.md\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "text_lines = file_text.split(\"# \")\n",
    "my_rag.load(text_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The hardware requirements specification for building and running Milvus from source code is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space',\n",
       " ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##',\n",
       "  'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##',\n",
       "  \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the hardware requirements specification if I want to build Milvus and run from source code?\"\n",
    "my_rag.answer(question, return_retrieved_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering questions: 100%|██████████| 3/3 [00:05<00:00,  1.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the hardware requirements specificatio...</td>\n",
       "      <td>[Hardware Requirements\\n\\nThe following specif...</td>\n",
       "      <td>The hardware requirements specification to bui...</td>\n",
       "      <td>If you want to build Milvus and run from sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the programming language used to write...</td>\n",
       "      <td>[CMake &amp; Conan\\n\\nThe algorithm library of Mil...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "      <td>The programming language used to write Knowher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should be ensured before running code cov...</td>\n",
       "      <td>[Code coverage\\n\\nBefore submitting your pull ...</td>\n",
       "      <td>Before running code coverage, it should be ens...</td>\n",
       "      <td>Before running code coverage, you should make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  what is the hardware requirements specificatio...   \n",
       "1  What is the programming language used to write...   \n",
       "2  What should be ensured before running code cov...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Hardware Requirements\\n\\nThe following specif...   \n",
       "1  [CMake & Conan\\n\\nThe algorithm library of Mil...   \n",
       "2  [Code coverage\\n\\nBefore submitting your pull ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The hardware requirements specification to bui...   \n",
       "1  The programming language used to write Knowher...   \n",
       "2  Before running code coverage, it should be ens...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  If you want to build Milvus and run from sourc...  \n",
       "1  The programming language used to write Knowher...  \n",
       "2  Before running code coverage, you should make ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "question_list = [\n",
    "    \"what is the hardware requirements specification if I want to build Milvus and run from source code?\",\n",
    "    \"What is the programming language used to write Knowhere?\",\n",
    "    \"What should be ensured before running code coverage?\",\n",
    "]\n",
    "ground_truth_list = [\n",
    "    \"If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.\",\n",
    "    \"The programming language used to write Knowhere is C++.\",\n",
    "    \"Before running code coverage, you should make sure that your code changes are covered by unit tests.\",\n",
    "]\n",
    "contexts_list = []\n",
    "answer_list = []\n",
    "for question in tqdm(question_list, desc=\"Answering questions\"):\n",
    "    answer, contexts = my_rag.answer(question, return_retrieved_text=True)\n",
    "    contexts_list.append(contexts)\n",
    "    answer_list.append(answer)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": question_list,\n",
    "        \"contexts\": contexts_list,\n",
    "        \"answer\": answer_list,\n",
    "        \"ground_truth\": ground_truth_list,\n",
    "    }\n",
    ")\n",
    "rag_results = Dataset.from_pandas(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Retriever\n",
    "\n",
    "To evaluate a retriever in large language model (LLM) systems, it’s essential to examine how effectively it ranks relevant information over irrelevant data, captures and retrieves contextually relevant information based on the input, and balances text chunk size and retrieval scope to minimize irrelevancies. These aspects together provide a comprehensive understanding of how well the retriever prioritizes, captures, and presents the most useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |██████████|100% (3/3) [Time Taken: 00:07,  2.57s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node, explicitly stating 'Knowhere is written in c++,' is ranked first, while the irrelevant nodes about 'integrating C++ and Go in Vscode' and 'prerequisites for different systems' are ranked lower., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output is perfectly supported by the retrieval context. Great job!, error: None)\n",
      "  - ✅ Contextual Relevancy (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because the context discusses integrating C++ and Go in Vscode but does not address the programming language used to write Knowhere., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the programming language used to write Knowhere?\n",
      "  - actual output: The programming language used to write Knowhere is C++.\n",
      "  - expected output: The programming language used to write Knowhere is C++.\n",
      "  - context: None\n",
      "  - retrieval context: [\"CMake & Conan\\n\\nThe algorithm library of Milvus, Knowhere is written in c++. CMake is required in the Milvus compilation. If you don't have it, please follow the instructions in the [Installing CMake](https://cmake.org/install/).\\n\\nConfirm that cmake is available:\\n\\n```shell\\n$ cmake --version\\n```\\n\\nNote: 3.25 or higher cmake version is required to build Milvus.\\n\\nMilvus uses Conan to manage third-party dependencies for c++.\\n\\nInstall Conan\\n\\n```shell\\npip install conan==1.64.1\\n```\\n\\nNote: Conan version 2.x is not currently supported, please use version 1.61.\\n\\n###\", 'Compiler Setup\\nYou can use Vscode to integrate C++ and Go together. Please replace user.settings file with below configs:\\n```bash\\n{\\n    \"go.toolsEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.testEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.buildFlags\": [\\n        \"-ldflags=-r=/Users/zilliz/workspace/milvus/internal/core/output/lib\"\\n    ],\\n    \"terminal.integrated.env.linux\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.useLanguageServer\": true,\\n    \"gopls\": {\\n        \"formatting.gofumpt\": true\\n    },\\n    \"go.formatTool\": \"gofumpt\",\\n    \"go.lintTool\": \"golangci-lint\",\\n    \"go.testTags\": \"test,dynamic\",\\n    \"go.testTimeout\": \"10m\"\\n}\\n```\\n\\n###', 'Prerequisites\\n\\nLinux systems (Recommend Ubuntu 20.04 or later):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\ngcc: 7.5\\nconan: 1.61\\n```\\n\\nMacOS systems with x86_64 (Big Sur 11.5 or later recommended):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\nMacOS systems with Apple Silicon (Monterey 12.0.1 or later recommended):\\n\\n```bash\\ngo: >= 1.21 (Arch=ARM64)\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\n###']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node with 'make sure your code change is covered by unit test' is at the top, while the irrelevant nodes focusing on 'setting up the environment' and 'pre-submission verification' are ranked lower., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output perfectly aligns with the retrieval context. Great job!, error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because the context is about setting up the Milvus deployment environment and running unit tests, which does not directly address what should be ensured before running code coverage., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What should be ensured before running code coverage?\n",
      "  - actual output: Before running code coverage, it should be ensured that the code change is covered by unit tests. Additionally, all pull request candidates should pass all Milvus unit tests, which requires bringing up the Milvus deployment environment first.\n",
      "  - expected output: Before running code coverage, you should make sure that your code changes are covered by unit tests.\n",
      "  - context: None\n",
      "  - retrieval context: ['Code coverage\\n\\nBefore submitting your pull request, make sure your code change is covered by unit test. Use the following commands to check code coverage rate:\\n\\nRun unit test and generate code coverage report:\\n\\n```shell\\n$ make codecov\\n```\\n\\nThis command will generate html report for Golang and C++ respectively.\\nFor Golang report, open the `go_coverage.html` under milvus project path.\\nFor C++ report, open the `cpp_coverage/index.html` under milvus project path.\\n\\nYou also can generate Golang coverage report by:\\n\\n```shell\\n$ make codecov-go\\n```\\n\\nOr C++ coverage report by:\\n\\n```shell\\n$ make codecov-cpp\\n```\\n\\n##', 'Unit Tests\\n\\nIt is required that all pull request candidates should pass all Milvus unit tests.\\n\\nBeforce running unit tests, you need to first bring up the Milvus deployment environment.\\nYou may set up a local docker environment with our docker compose yaml file to start unit testing.\\nFor Apple Silicon users (Apple M1):\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose -f docker-compose-apple-silicon.yml up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nFor others:\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nTo run only cpp test:\\n\\n```shell\\n$ make test-cpp\\n```\\n\\nTo run only go test:\\n\\n```shell\\n$ make test-go\\n```\\n\\nTo run a single test case (TestSearchTask in /internal/proxy directory, for example):\\n\\n```shell\\n$ source scripts/setenv.sh && go test -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\nIf using Mac with M1 chip\\n\\n```\\n$ source scripts/setenv.sh && go test -tags=dynamic -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\n##', 'Pre-submission Verification\\n\\nPre-submission verification provides a battery of checks and tests to give your pull request the best chance of being accepted. Developers need to run as many verification tests as possible locally.\\n\\nTo run all pre-submission verification tests, use this command:\\n\\n```shell\\n$ make verifiers\\n```\\n\\n##']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the first node directly provides the recommended hardware requirements for Milvus, while the irrelevant nodes discussing software requirements and verified OS types are ranked lower., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly matches the provided context, detailing the recommended hardware requirements as specified., error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because the context only outlines the operating system requirements and does not specify the hardware requirements for building Milvus from source code., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what is the hardware requirements specification if I want to build Milvus and run from source code?\n",
      "  - actual output: The hardware requirements specification to build and run Milvus from source code is:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space\n",
      "  - expected output: If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space.\n",
      "  - context: None\n",
      "  - retrieval context: ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##', 'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##', \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "Contextual Recall: 100.00% pass rate\n",
      "Contextual Relevancy: 33.33% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics_data=[MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.00 because the relevant node, explicitly stating 'Knowhere is written in c++,' is ranked first, while the irrelevant nodes about 'integrating C++ and Go in Vscode' and 'prerequisites for different systems' are ranked lower.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.011640000000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context explicitly states that \\'Knowhere is written in c++,\\' which directly answers the input question.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context is about integrating C++ and Go in Vscode and does not mention the programming language used to write Knowhere.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context lists prerequisites for different systems but does not mention which programming language is used to write Knowhere.\"\\n    }\\n]'), MetricData(name='Contextual Recall', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the expected output is perfectly supported by the retrieval context. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.008825, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The 1st node in the retrieval context mentions \\'Knowhere is written in c++.\\'\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.6666666666666666, reason='The score is 0.67 because the context discusses integrating C++ and Go in Vscode but does not address the programming language used to write Knowhere.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.010165, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context provides configurations for integrating C++ and Go in Vscode but does not mention the programming language used to write Knowhere.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, input='What is the programming language used to write Knowhere?', actual_output='The programming language used to write Knowhere is C++.', expected_output='The programming language used to write Knowhere is C++.', context=None, retrieval_context=[\"CMake & Conan\\n\\nThe algorithm library of Milvus, Knowhere is written in c++. CMake is required in the Milvus compilation. If you don't have it, please follow the instructions in the [Installing CMake](https://cmake.org/install/).\\n\\nConfirm that cmake is available:\\n\\n```shell\\n$ cmake --version\\n```\\n\\nNote: 3.25 or higher cmake version is required to build Milvus.\\n\\nMilvus uses Conan to manage third-party dependencies for c++.\\n\\nInstall Conan\\n\\n```shell\\npip install conan==1.64.1\\n```\\n\\nNote: Conan version 2.x is not currently supported, please use version 1.61.\\n\\n###\", 'Compiler Setup\\nYou can use Vscode to integrate C++ and Go together. Please replace user.settings file with below configs:\\n```bash\\n{\\n    \"go.toolsEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.testEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.buildFlags\": [\\n        \"-ldflags=-r=/Users/zilliz/workspace/milvus/internal/core/output/lib\"\\n    ],\\n    \"terminal.integrated.env.linux\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.useLanguageServer\": true,\\n    \"gopls\": {\\n        \"formatting.gofumpt\": true\\n    },\\n    \"go.formatTool\": \"gofumpt\",\\n    \"go.lintTool\": \"golangci-lint\",\\n    \"go.testTags\": \"test,dynamic\",\\n    \"go.testTimeout\": \"10m\"\\n}\\n```\\n\\n###', 'Prerequisites\\n\\nLinux systems (Recommend Ubuntu 20.04 or later):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\ngcc: 7.5\\nconan: 1.61\\n```\\n\\nMacOS systems with x86_64 (Big Sur 11.5 or later recommended):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\nMacOS systems with Apple Silicon (Monterey 12.0.1 or later recommended):\\n\\n```bash\\ngo: >= 1.21 (Arch=ARM64)\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\n###']),\n",
       " TestResult(success=False, metrics_data=[MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.00 because the relevant node with 'make sure your code change is covered by unit test' is at the top, while the irrelevant nodes focusing on 'setting up the environment' and 'pre-submission verification' are ranked lower.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.010225000000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context explicitly mentions \\'make sure your code change is covered by unit test\\' before running code coverage.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Although it talks about unit tests, it focuses more on setting up the environment and does not directly relate to ensuring code changes are covered by unit tests before running code coverage.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context is about pre-submission verification and does not mention ensuring code changes are covered by unit tests before running code coverage.\"\\n    }\\n]'), MetricData(name='Contextual Recall', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the expected output perfectly aligns with the retrieval context. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00733, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The first node mentions, \\'Before submitting your pull request, make sure your code change is covered by unit test.\\'\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.3333333333333333, reason='The score is 0.33 because the context is about setting up the Milvus deployment environment and running unit tests, which does not directly address what should be ensured before running code coverage.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.009420000000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context provided is about setting up the Milvus deployment environment and running unit tests. It does not directly address what should be ensured before running code coverage.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context focuses on pre-submission verification and running verification tests, but it does not address what should be ensured before running code coverage.\"\\n    }\\n]')], conversational=False, input='What should be ensured before running code coverage?', actual_output='Before running code coverage, it should be ensured that the code change is covered by unit tests. Additionally, all pull request candidates should pass all Milvus unit tests, which requires bringing up the Milvus deployment environment first.', expected_output='Before running code coverage, you should make sure that your code changes are covered by unit tests.', context=None, retrieval_context=['Code coverage\\n\\nBefore submitting your pull request, make sure your code change is covered by unit test. Use the following commands to check code coverage rate:\\n\\nRun unit test and generate code coverage report:\\n\\n```shell\\n$ make codecov\\n```\\n\\nThis command will generate html report for Golang and C++ respectively.\\nFor Golang report, open the `go_coverage.html` under milvus project path.\\nFor C++ report, open the `cpp_coverage/index.html` under milvus project path.\\n\\nYou also can generate Golang coverage report by:\\n\\n```shell\\n$ make codecov-go\\n```\\n\\nOr C++ coverage report by:\\n\\n```shell\\n$ make codecov-cpp\\n```\\n\\n##', 'Unit Tests\\n\\nIt is required that all pull request candidates should pass all Milvus unit tests.\\n\\nBeforce running unit tests, you need to first bring up the Milvus deployment environment.\\nYou may set up a local docker environment with our docker compose yaml file to start unit testing.\\nFor Apple Silicon users (Apple M1):\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose -f docker-compose-apple-silicon.yml up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nFor others:\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nTo run only cpp test:\\n\\n```shell\\n$ make test-cpp\\n```\\n\\nTo run only go test:\\n\\n```shell\\n$ make test-go\\n```\\n\\nTo run a single test case (TestSearchTask in /internal/proxy directory, for example):\\n\\n```shell\\n$ source scripts/setenv.sh && go test -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\nIf using Mac with M1 chip\\n\\n```\\n$ source scripts/setenv.sh && go test -tags=dynamic -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\n##', 'Pre-submission Verification\\n\\nPre-submission verification provides a battery of checks and tests to give your pull request the best chance of being accepted. Developers need to run as many verification tests as possible locally.\\n\\nTo run all pre-submission verification tests, use this command:\\n\\n```shell\\n$ make verifiers\\n```\\n\\n##']),\n",
       " TestResult(success=False, metrics_data=[MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the first node directly provides the recommended hardware requirements for Milvus, while the irrelevant nodes discussing software requirements and verified OS types are ranked lower.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00869, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context directly provides the recommended hardware requirements specification for Milvus to build and run from source code: \\'8GB of RAM\\' and \\'50GB of free disk space.\\'\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Although it talks about hardware and software requirements, it does not provide the specific hardware requirements needed to answer the question.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context talks about software requirements and verified OS types, not the hardware requirements for building and running Milvus from source code.\"\\n    }\\n]'), MetricData(name='Contextual Recall', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output perfectly matches the provided context, detailing the recommended hardware requirements as specified.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00773, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The sentence summarizes the context provided in the 1st node: \\'The following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\'\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The sentence matches the hardware requirement listed in the 1st node: \\'- 8GB of RAM.\\'\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The sentence matches the hardware requirement listed in the 1st node: \\'- 50GB of free disk space.\\'\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.3333333333333333, reason='The score is 0.33 because the context only outlines the operating system requirements and does not specify the hardware requirements for building Milvus from source code.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00799, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context outlines the operating system requirements but does not specify the hardware requirements for building Milvus from source code.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context provided only contains information about software requirements and verified OS types for Milvus development. It does not mention any hardware requirements specification.\"\\n    }\\n]')], conversational=False, input='what is the hardware requirements specification if I want to build Milvus and run from source code?', actual_output='The hardware requirements specification to build and run Milvus from source code is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space', expected_output='If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.', context=None, retrieval_context=['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##', 'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##', \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric()\n",
    "contextual_recall = ContextualRecallMetric()\n",
    "contextual_relevancy = ContextualRelevancyMetric()\n",
    "\n",
    "test_cases = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    test_case = LLMTestCase(\n",
    "        input=row[\"question\"],\n",
    "        actual_output=row[\"answer\"],\n",
    "        expected_output=row[\"ground_truth\"],\n",
    "        retrieval_context=row[\"contexts\"],\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# test_cases\n",
    "result = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Generation\n",
    "\n",
    "To evaluate the quality of generated outputs in large language models (LLMs), it's important to assess two key aspects. First, the relevance of the output should be evaluated, determining whether the prompt effectively guides the LLM to generate helpful and contextually appropriate responses. Second, the faithfulness of the generation must be measured to ensure that the model produces accurate information without hallucinations or contradictions, aligning with the factual content provided in the retrieval context. Together, these factors help ensure that the generated outputs are both relevant and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |██████████|100% (3/3) [Time Taken: 00:10,  3.52s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant to the question without any irrelevant statements. Great job!, error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what is the hardware requirements specification if I want to build Milvus and run from source code?\n",
      "  - actual output: The hardware requirements specification to build and run Milvus from source code is:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space\n",
      "  - expected output: If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\n",
      "\n",
      "- 8GB of RAM\n",
      "- 50GB of free disk space.\n",
      "  - context: None\n",
      "  - retrieval context: ['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##', 'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##', \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is perfectly relevant and contains no irrelevant statements. Great job!, error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions. Great job staying true to the retrieval context!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the programming language used to write Knowhere?\n",
      "  - actual output: The programming language used to write Knowhere is C++.\n",
      "  - expected output: The programming language used to write Knowhere is C++.\n",
      "  - context: None\n",
      "  - retrieval context: [\"CMake & Conan\\n\\nThe algorithm library of Milvus, Knowhere is written in c++. CMake is required in the Milvus compilation. If you don't have it, please follow the instructions in the [Installing CMake](https://cmake.org/install/).\\n\\nConfirm that cmake is available:\\n\\n```shell\\n$ cmake --version\\n```\\n\\nNote: 3.25 or higher cmake version is required to build Milvus.\\n\\nMilvus uses Conan to manage third-party dependencies for c++.\\n\\nInstall Conan\\n\\n```shell\\npip install conan==1.64.1\\n```\\n\\nNote: Conan version 2.x is not currently supported, please use version 1.61.\\n\\n###\", 'Compiler Setup\\nYou can use Vscode to integrate C++ and Go together. Please replace user.settings file with below configs:\\n```bash\\n{\\n    \"go.toolsEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.testEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.buildFlags\": [\\n        \"-ldflags=-r=/Users/zilliz/workspace/milvus/internal/core/output/lib\"\\n    ],\\n    \"terminal.integrated.env.linux\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.useLanguageServer\": true,\\n    \"gopls\": {\\n        \"formatting.gofumpt\": true\\n    },\\n    \"go.formatTool\": \"gofumpt\",\\n    \"go.lintTool\": \"golangci-lint\",\\n    \"go.testTags\": \"test,dynamic\",\\n    \"go.testTimeout\": \"10m\"\\n}\\n```\\n\\n###', 'Prerequisites\\n\\nLinux systems (Recommend Ubuntu 20.04 or later):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\ngcc: 7.5\\nconan: 1.61\\n```\\n\\nMacOS systems with x86_64 (Big Sur 11.5 or later recommended):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\nMacOS systems with Apple Silicon (Monterey 12.0.1 or later recommended):\\n\\n```bash\\ngo: >= 1.21 (Arch=ARM64)\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\n###']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because while the output partially addresses ensuring code coverage, it includes irrelevant information about setting up the Milvus deployment environment., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating the actual output is perfectly faithful to the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What should be ensured before running code coverage?\n",
      "  - actual output: Before running code coverage, it should be ensured that the code change is covered by unit tests. Additionally, all pull request candidates should pass all Milvus unit tests, which requires bringing up the Milvus deployment environment first.\n",
      "  - expected output: Before running code coverage, you should make sure that your code changes are covered by unit tests.\n",
      "  - context: None\n",
      "  - retrieval context: ['Code coverage\\n\\nBefore submitting your pull request, make sure your code change is covered by unit test. Use the following commands to check code coverage rate:\\n\\nRun unit test and generate code coverage report:\\n\\n```shell\\n$ make codecov\\n```\\n\\nThis command will generate html report for Golang and C++ respectively.\\nFor Golang report, open the `go_coverage.html` under milvus project path.\\nFor C++ report, open the `cpp_coverage/index.html` under milvus project path.\\n\\nYou also can generate Golang coverage report by:\\n\\n```shell\\n$ make codecov-go\\n```\\n\\nOr C++ coverage report by:\\n\\n```shell\\n$ make codecov-cpp\\n```\\n\\n##', 'Unit Tests\\n\\nIt is required that all pull request candidates should pass all Milvus unit tests.\\n\\nBeforce running unit tests, you need to first bring up the Milvus deployment environment.\\nYou may set up a local docker environment with our docker compose yaml file to start unit testing.\\nFor Apple Silicon users (Apple M1):\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose -f docker-compose-apple-silicon.yml up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nFor others:\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nTo run only cpp test:\\n\\n```shell\\n$ make test-cpp\\n```\\n\\nTo run only go test:\\n\\n```shell\\n$ make test-go\\n```\\n\\nTo run a single test case (TestSearchTask in /internal/proxy directory, for example):\\n\\n```shell\\n$ source scripts/setenv.sh && go test -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\nIf using Mac with M1 chip\\n\\n```\\n$ source scripts/setenv.sh && go test -tags=dynamic -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\n##', 'Pre-submission Verification\\n\\nPre-submission verification provides a battery of checks and tests to give your pull request the best chance of being accepted. Developers need to run as many verification tests as possible locally.\\n\\nTo run all pre-submission verification tests, use this command:\\n\\n```shell\\n$ make verifiers\\n```\\n\\n##']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. \n",
       "‼️  NOTE: You can also run evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response is perfectly relevant to the question without any irrelevant statements. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005895, verbose_logs='Statements:\\n[\\n    \"The hardware requirements specification to build and run Milvus from source code is:\",\\n    \"8GB of RAM\",\\n    \"50GB of free disk space\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.010945000000000002, verbose_logs='Truths:\\n[\\n    \"8GB of RAM is recommended for Milvus to build and run from source code.\",\\n    \"50GB of free disk space is recommended for Milvus to build and run from source code.\",\\n    \"Milvus can successfully build and run on Debian/Ubuntu.\",\\n    \"Milvus can successfully build and run on Amazon Linux.\",\\n    \"Milvus can successfully build and run on MacOS (x86_64).\",\\n    \"Milvus can successfully build and run on MacOS (Apple Silicon).\"\\n] \\n \\nClaims:\\n[\\n    \"The hardware requirements specification to build and run Milvus from source code is 8GB of RAM.\",\\n    \"The hardware requirements specification to build and run Milvus from source code is 50GB of free disk space.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, input='what is the hardware requirements specification if I want to build Milvus and run from source code?', actual_output='The hardware requirements specification to build and run Milvus from source code is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space', expected_output='If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.', context=None, retrieval_context=['Hardware Requirements\\n\\nThe following specification (either physical or virtual machine resources) is recommended for Milvus to build and run from source code.\\n\\n```\\n- 8GB of RAM\\n- 50GB of free disk space\\n```\\n\\n##', 'Building Milvus on a local OS/shell environment\\n\\nThe details below outline the hardware and software requirements for building on Linux and MacOS.\\n\\n##', \"Software Requirements\\n\\nAll Linux distributions are available for Milvus development. However a majority of our contributor worked with Ubuntu or CentOS systems, with a small portion of Mac (both x86_64 and Apple Silicon) contributors. If you would like Milvus to build and run on other distributions, you are more than welcome to file an issue and contribute!\\n\\nHere's a list of verified OS types where Milvus can successfully build and run:\\n\\n- Debian/Ubuntu\\n- Amazon Linux\\n- MacOS (x86_64)\\n- MacOS (Apple Silicon)\\n\\n##\"]),\n",
       " TestResult(success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer is perfectly relevant and contains no irrelevant statements. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00484, verbose_logs='Statements:\\n[\\n    \"The programming language used to write Knowhere is C++.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions. Great job staying true to the retrieval context!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.01551, verbose_logs='Truths:\\n[\\n    \"The algorithm library of Milvus, Knowhere is written in C++.\",\\n    \"CMake is required in the Milvus compilation.\",\\n    \"Milvus uses Conan to manage third-party dependencies for C++.\",\\n    \"Conan version 2.x is not currently supported.\",\\n    \"CMake version 3.25 or higher is required to build Milvus.\",\\n    \"Vscode can be used to integrate C++ and Go together.\",\\n    \"For Linux systems, the recommended prerequisites include Go version >= 1.21, CMake version >= 3.18, GCC version 7.5, and Conan version 1.61.\",\\n    \"For MacOS systems with x86_64, the recommended prerequisites include Go version >= 1.21, CMake version >= 3.18, LLVM version >= 15, and Conan version 1.61.\",\\n    \"For MacOS systems with Apple Silicon, the recommended prerequisites include Go version >= 1.21 (Arch=ARM64), CMake version >= 3.18, LLVM version >= 15, and Conan version 1.61.\"\\n] \\n \\nClaims:\\n[\\n    \"The programming language used to write Knowhere is C++.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, input='What is the programming language used to write Knowhere?', actual_output='The programming language used to write Knowhere is C++.', expected_output='The programming language used to write Knowhere is C++.', context=None, retrieval_context=[\"CMake & Conan\\n\\nThe algorithm library of Milvus, Knowhere is written in c++. CMake is required in the Milvus compilation. If you don't have it, please follow the instructions in the [Installing CMake](https://cmake.org/install/).\\n\\nConfirm that cmake is available:\\n\\n```shell\\n$ cmake --version\\n```\\n\\nNote: 3.25 or higher cmake version is required to build Milvus.\\n\\nMilvus uses Conan to manage third-party dependencies for c++.\\n\\nInstall Conan\\n\\n```shell\\npip install conan==1.64.1\\n```\\n\\nNote: Conan version 2.x is not currently supported, please use version 1.61.\\n\\n###\", 'Compiler Setup\\nYou can use Vscode to integrate C++ and Go together. Please replace user.settings file with below configs:\\n```bash\\n{\\n    \"go.toolsEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.testEnvVars\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.buildFlags\": [\\n        \"-ldflags=-r=/Users/zilliz/workspace/milvus/internal/core/output/lib\"\\n    ],\\n    \"terminal.integrated.env.linux\": {\\n        \"PKG_CONFIG_PATH\": \"${env:PKG_CONFIG_PATH}:${workspaceFolder}/internal/core/output/lib/pkgconfig:${workspaceFolder}/internal/core/output/lib64/pkgconfig\",\\n        \"LD_LIBRARY_PATH\": \"${env:LD_LIBRARY_PATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n        \"RPATH\": \"${env:RPATH}:${workspaceFolder}/internal/core/output/lib:${workspaceFolder}/internal/core/output/lib64\",\\n    },\\n    \"go.useLanguageServer\": true,\\n    \"gopls\": {\\n        \"formatting.gofumpt\": true\\n    },\\n    \"go.formatTool\": \"gofumpt\",\\n    \"go.lintTool\": \"golangci-lint\",\\n    \"go.testTags\": \"test,dynamic\",\\n    \"go.testTimeout\": \"10m\"\\n}\\n```\\n\\n###', 'Prerequisites\\n\\nLinux systems (Recommend Ubuntu 20.04 or later):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\ngcc: 7.5\\nconan: 1.61\\n```\\n\\nMacOS systems with x86_64 (Big Sur 11.5 or later recommended):\\n\\n```bash\\ngo: >= 1.21\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\nMacOS systems with Apple Silicon (Monterey 12.0.1 or later recommended):\\n\\n```bash\\ngo: >= 1.21 (Arch=ARM64)\\ncmake: >= 3.18\\nllvm: >= 15\\nconan: 1.61\\n```\\n\\n###']),\n",
       " TestResult(success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.6666666666666666, reason='The score is 0.67 because while the output partially addresses ensuring code coverage, it includes irrelevant information about setting up the Milvus deployment environment.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.006840000000000001, verbose_logs='Statements:\\n[\\n    \"Before running code coverage, it should be ensured that the code change is covered by unit tests.\",\\n    \"Additionally, all pull request candidates should pass all Milvus unit tests.\",\\n    \"This requires bringing up the Milvus deployment environment first.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement about bringing up the Milvus deployment environment is about setting up the environment, which is not directly related to ensuring code coverage before running it.\"\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions, indicating the actual output is perfectly faithful to the retrieval context. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.01874, verbose_logs='Truths:\\n[\\n    \"Before submitting your pull request, make sure your code change is covered by unit test.\",\\n    \"The command `$ make codecov` generates an HTML report for Golang and C++ respectively.\",\\n    \"For the Golang report, open the `go_coverage.html` under the Milvus project path.\",\\n    \"For the C++ report, open the `cpp_coverage/index.html` under the Milvus project path.\",\\n    \"You can generate a Golang coverage report by using the command `$ make codecov-go`.\",\\n    \"You can generate a C++ coverage report by using the command `$ make codecov-cpp`.\",\\n    \"All pull request candidates should pass all Milvus unit tests.\",\\n    \"Before running unit tests, you need to first bring up the Milvus deployment environment.\",\\n    \"You may set up a local docker environment with the docker compose yaml file to start unit testing.\",\\n    \"Apple Silicon users (Apple M1) should use the command `$ docker compose -f docker-compose-apple-silicon.yml up -d` from the `deployments/docker/dev` directory.\",\\n    \"To run only cpp test, use the command `$ make test-cpp`.\",\\n    \"To run only go test, use the command `$ make test-go`.\",\\n    \"To run a single test case (TestSearchTask in /internal/proxy directory, for example), use the command `$ source scripts/setenv.sh && go test -v ./internal/proxy/ -test.run TestSearchTask`.\",\\n    \"If using Mac with M1 chip, use the command `$ source scripts/setenv.sh && go test -tags=dynamic -v ./internal/proxy/ -test.run TestSearchTask`.\",\\n    \"Pre-submission verification provides a battery of checks and tests.\",\\n    \"Developers need to run as many verification tests as possible locally.\",\\n    \"To run all pre-submission verification tests, use the command `$ make verifiers`.\"\\n] \\n \\nClaims:\\n[\\n    \"Before running code coverage, it should be ensured that the code change is covered by unit tests.\",\\n    \"All pull request candidates should pass all Milvus unit tests.\",\\n    \"Passing all Milvus unit tests requires bringing up the Milvus deployment environment first.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, input='What should be ensured before running code coverage?', actual_output='Before running code coverage, it should be ensured that the code change is covered by unit tests. Additionally, all pull request candidates should pass all Milvus unit tests, which requires bringing up the Milvus deployment environment first.', expected_output='Before running code coverage, you should make sure that your code changes are covered by unit tests.', context=None, retrieval_context=['Code coverage\\n\\nBefore submitting your pull request, make sure your code change is covered by unit test. Use the following commands to check code coverage rate:\\n\\nRun unit test and generate code coverage report:\\n\\n```shell\\n$ make codecov\\n```\\n\\nThis command will generate html report for Golang and C++ respectively.\\nFor Golang report, open the `go_coverage.html` under milvus project path.\\nFor C++ report, open the `cpp_coverage/index.html` under milvus project path.\\n\\nYou also can generate Golang coverage report by:\\n\\n```shell\\n$ make codecov-go\\n```\\n\\nOr C++ coverage report by:\\n\\n```shell\\n$ make codecov-cpp\\n```\\n\\n##', 'Unit Tests\\n\\nIt is required that all pull request candidates should pass all Milvus unit tests.\\n\\nBeforce running unit tests, you need to first bring up the Milvus deployment environment.\\nYou may set up a local docker environment with our docker compose yaml file to start unit testing.\\nFor Apple Silicon users (Apple M1):\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose -f docker-compose-apple-silicon.yml up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nFor others:\\n\\n```shell\\n$ cd deployments/docker/dev\\n$ docker compose up -d\\n$ cd ../../../\\n$ make unittest\\n```\\n\\nTo run only cpp test:\\n\\n```shell\\n$ make test-cpp\\n```\\n\\nTo run only go test:\\n\\n```shell\\n$ make test-go\\n```\\n\\nTo run a single test case (TestSearchTask in /internal/proxy directory, for example):\\n\\n```shell\\n$ source scripts/setenv.sh && go test -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\nIf using Mac with M1 chip\\n\\n```\\n$ source scripts/setenv.sh && go test -tags=dynamic -v ./internal/proxy/ -test.run TestSearchTask\\n```\\n\\n##', 'Pre-submission Verification\\n\\nPre-submission verification provides a battery of checks and tests to give your pull request the best chance of being accepted. Developers need to run as many verification tests as possible locally.\\n\\nTo run all pre-submission verification tests, use this command:\\n\\n```shell\\n$ make verifiers\\n```\\n\\n##'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "\n",
    "answer_relevancy = AnswerRelevancyMetric()\n",
    "faithfulness = FaithfulnessMetric()\n",
    "\n",
    "test_cases = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    test_case = LLMTestCase(\n",
    "        input=row[\"question\"],\n",
    "        actual_output=row[\"answer\"],\n",
    "        expected_output=row[\"ground_truth\"],\n",
    "        retrieval_context=row[\"contexts\"],\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# test_cases\n",
    "result = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=[answer_relevancy, faithfulness]\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zilliz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
