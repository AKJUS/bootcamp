{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/quickstart/image_search_with_milvus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Search with Milvus\n",
    "\n",
    "In this notebook, we will show you how to use Milvus to search for similar images in a dataset. We will use a subset of the [ImageNet](https://www.image-net.org/) dataset, then search for an image of an Afghan hound to demonstrate this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "First, we need to load the dataset and unextract it for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/milvus-io/pymilvus-assets/releases/download/imagedata/reverse_image_search.zip\n",
    "!unzip -q -o reverse_image_search.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites\n",
    "\n",
    "To run this notebook, you need to have the following dependencies installed:\n",
    "- pymilvus>=2.4.2\n",
    "- timm\n",
    "- torch\n",
    "- numpy\n",
    "- sklearn\n",
    "- pillow \n",
    "\n",
    "To run Colab, we provide the handy commands to install the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install grpcio==1.60\n",
    "!pip install pymilvus==2.4.2\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Feature Extractor\n",
    "Then, we need to define a feature extractor which extracts embedding from an image using timm's ResNet-34 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.preprocessing import normalize\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import numpy as np\n",
    "from pymilvus import connections\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, modelname):\n",
    "        # Load the pre-trained model\n",
    "        self.model = timm.create_model(modelname, pretrained=True, num_classes=0, global_pool='avg')\n",
    "        self.model.eval()\n",
    "\n",
    "        # Get the input size required by the model\n",
    "        self.input_size = self.model.default_cfg['input_size']\n",
    "\n",
    "        config = resolve_data_config({}, model=modelname)\n",
    "        # Get the preprocessing function provided by TIMM for the model\n",
    "        self.preprocess = create_transform(**config)\n",
    "\n",
    "    def __call__(self, imagepath):\n",
    "        # Preprocess the input image\n",
    "        input_image = Image.open(imagepath).convert(\"RGB\")  # Convert to RGB if needed\n",
    "        input_image = self.preprocess(input_image)\n",
    "\n",
    "        # Convert the image to a PyTorch tensor and add a batch dimension\n",
    "        input_tensor = input_image.unsqueeze(0)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        # Extract the feature vector\n",
    "        feature_vector = output.squeeze().numpy()\n",
    "\n",
    "        return normalize(feature_vector.reshape(1,-1), norm=\"l2\").flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus Collection\n",
    "Then we need to create Milvus collection to store the image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"example.db\"\n",
    ")\n",
    "# Create a collection in quick setup mode\n",
    "client.create_collection(\n",
    "    collection_name=\"image_embeddings\",\n",
    "    vector_field_name=\"vector\",\n",
    "    dimension=512,\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    metric_type=\"COSINE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the Embeddings to Milvus\n",
    "We will extract embeddings of each image using the ResNet34 model and insert images from the training set into Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "extractor = FeatureExtractor('resnet34')\n",
    "\n",
    "root = './train'\n",
    "insert = True\n",
    "if insert is True:\n",
    "    for dirpath, foldername, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.JPEG'):\n",
    "                filepath = dirpath+ '/' +filename\n",
    "                image_embedding = extractor(filepath)\n",
    "                client.insert(\"image_embeddings\", {\"vector\": image_embedding, \"filename\": filepath})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the Image\n",
    "Now we can search the image using query embedding from a Afghan hound image in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7624790072441101 ./train/Afghan_hound/n02088094_5911.JPEG\n",
      "0.739625096321106 ./train/Afghan_hound/n02088094_6533.JPEG\n",
      "0.7271758913993835 ./train/Afghan_hound/n02088094_2164.JPEG\n",
      "0.7154390215873718 ./train/Bouvier_des_Flandres/n02106382_5429.JPEG\n",
      "0.6918680667877197 ./train/Bouvier_des_Flandres/n02106382_6653.JPEG\n",
      "0.680339515209198 ./train/Afghan_hound/n02088094_6565.JPEG\n",
      "0.6768788695335388 ./train/Afghan_hound/n02088094_1045.JPEG\n",
      "0.6709263920783997 ./train/Afghan_hound/n02088094_5532.JPEG\n",
      "0.6666043996810913 ./train/Afghan_hound/n02088094_7360.JPEG\n",
      "0.6618548631668091 ./train/soft-coated_wheaten_terrier/n02098105_400.JPEG\n"
     ]
    }
   ],
   "source": [
    "results = client.search(\"image_embeddings\", data=[extractor('./test/Afghan_hound/n02088094_4261.JPEG')], output_fields=[\"filename\"], search_params={\"metric_type\": \"COSINE\"})\n",
    "for result in results:\n",
    "    for hit in result:\n",
    "        print(hit[\"distance\"], hit[\"entity\"][\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the images are from the same category as the search image, which is the Afghan hound. This means that we found similar images to the search image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
