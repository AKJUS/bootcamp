{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anyscale partner blog - Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Use 48K IMDB dataset form Kaggle.\n",
    "\n",
    "Import csv and save to parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install pillow\n",
    "# !python -m pip install tqdm\n",
    "# !python -m pip install torch sentence-transformers langchain\n",
    "# !python -m pip install pymilvus 'pymilvus[model]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch - Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pprint, time, os\n",
    "\n",
    "# df = pd.read_csv(\"data/final_data.csv\")\n",
    "# print(df.shape)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate 'Name', 'Keywords', and 'Description' into 'text' column\n",
    "# df['text'] = df['Name'] + ' ' + df['Description'] + ' ' + df['ReviewBody']\n",
    "# df['text'] = df['text'].fillna('')\n",
    "\n",
    "# # Convert genres from string with commas in it to list of strings.\n",
    "# df['Genres'] = df['Genres'].str.split(',')\n",
    "\n",
    "# # Convert actors from string with commas in it to list of strings.\n",
    "# df['Actors'] = df['Actors'].str.split(',')\n",
    "\n",
    "# # Convert keywords from string with commas in it to list of strings.\n",
    "# df['Keywords'] = df['Keywords'].str.split(',')\n",
    "\n",
    "# # Extract out just the year from the date.\n",
    "# def extract_year(movie_date):\n",
    "#     try:\n",
    "#         return int(movie_date.split('-')[0])\n",
    "#     except Exception:\n",
    "#         return -1  # return -1 instead of None\n",
    "# df['MovieYear'] = df.DatePublished.apply(extract_year)\n",
    "\n",
    "# # Drop extra columns.\n",
    "# df.drop(columns=['RatingCount', 'BestRating', 'WorstRating'], inplace=True)\n",
    "# df.drop(columns=['DatePublished'], inplace=True)\n",
    "# df.drop(columns=['Description'], inplace=True)\n",
    "# df.drop(columns=['ReviewBody'], inplace=True)\n",
    "\n",
    "# # Inspect text.\n",
    "# print(f\"Example text length: {len(df.text[0])}\")\n",
    "# pprint.pprint(f\"Example text: {df.text[0]}\")\n",
    "\n",
    "# print(df.dtypes)\n",
    "# display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save top 100 rows as data/kaggle_imdb_small.csv.\n",
    "# df.head(100).to_csv('data/kaggle_imdb_small.csv', index=False)\n",
    "\n",
    "# # Save data as a parquet file.\n",
    "# df.to_parquet('data/kaggle_imdb.parquet')\n",
    "# df.head(100).to_parquet('data/kaggle_imdb_small.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ray.\n",
    "# !pip install -U \"ray[data,train,tune,serve,default]\"\n",
    "\n",
    "# Start Ray headnode local cluster.\n",
    "# The command will print out the Ray cluster address, \n",
    "#    which can be passed to start the worker nodes.\n",
    "# !ray start --head --port=6379\n",
    "\n",
    "# For multi-node cluster, start Ray worker nodes.\n",
    "# !ray start --address='http://127.0.0.1:8265'\n",
    "\n",
    "# Submit a Ray job using local .py script.\n",
    "# See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
    "# !export RAY_ADDRESS=\"http://127.0.0.1:8265\"\n",
    "# !ray job submit --working-dir . -- python ray_data_demo.py\n",
    "  \n",
    "#   See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \n",
    "#   for more information on submitting Ray jobs to the Ray cluster.\n",
    "  \n",
    "#   To terminate the Ray runtime, run\n",
    "#     ray stop\n",
    "  \n",
    "#   To view the status of the cluster, use\n",
    "#     ray status\n",
    "  \n",
    "#   To monitor and debug Ray, view the dashboard at \n",
    "#     127.0.0.1:8265\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data into ray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:56:03,165\tINFO worker.py:1567 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n",
      "2024-03-28 20:56:03,175\tINFO worker.py:1743 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80eae3d7e694be89f9da20d459221e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 20:56:03,609\tINFO dataset.py:2368 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2024-03-28 20:56:03,612\tINFO streaming_executor.py:115 -- Starting execution of Dataset. Full log is in /tmp/ray/session_2024-03-28_16-56-29_410372_93015/logs/ray-data.log\n",
      "2024-03-28 20:56:03,612\tINFO streaming_executor.py:116 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 100\n",
      "\n",
      "Metadata: \n",
      "Dataset(\n",
      "   num_rows=100,\n",
      "   schema={\n",
      "      id: int64,\n",
      "      url: string,\n",
      "      Name: string,\n",
      "      PosterLink: string,\n",
      "      Genres: list<element: string>,\n",
      "      Actors: list<element: string>,\n",
      "      Director: string,\n",
      "      Keywords: list<element: string>,\n",
      "      RatingValue: double,\n",
      "      ReviewAurthor: string,\n",
      "      ReviewDate: string,\n",
      "      duration: string,\n",
      "      text: string,\n",
      "      MovieYear: int64\n",
      "   }\n",
      ")\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091d5bb0502d412181e8b9bdf84a5410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(20) 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de04d15fbf39477f94a5135ae3bdcc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7456a8ef84b4581bf929505a3b646ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'url': 'https://www.imdb.com/title/tt2221420/',\n",
       "  'Name': 'Sallie Gardner at a Gallop',\n",
       "  'PosterLink': 'https://m.media-amazon.com/images/M/MV5BMjk0MzM1NDUtMTIyNi00Y2Y3LWEwNzEtMDQ1NmU4YzgzYWU2XkEyXkFqcGdeQXVyMjM3NTU0NTQ@._V1_.jpg',\n",
       "  'Genres': ['Documentary', 'Short'],\n",
       "  'Actors': ['Gilbert Domm', 'Sallie Gardner'],\n",
       "  'Director': 'Eadweard Muybridge',\n",
       "  'Keywords': ['19th century', '1870s', 'nature', 'horse', 'horse riding'],\n",
       "  'RatingValue': 7.4,\n",
       "  'ReviewAurthor': 'Cineanalyst',\n",
       "  'ReviewDate': '2013-11-12',\n",
       "  'duration': '1M',\n",
       "  'text': 'Sallie Gardner at a Gallop Sallie Gardner at a Gallop is a short starring Gilbert Domm and Sallie Gardner. The clip shows a jockey, Domm, riding a horse, Sally Gardner. The clip is not filmed but instead consists of 24 individual photographs shot in rapid... Sometimes ascribed as \"The Father of the Motion Picture\", Eadweard Muybridge undeniably accomplished exploiting and sometimes introducing a means of instantaneous and serial images to analyze and synthesize animal locomotion. In part, the reasons for and the claims made of his work support Virgilio Tosi\\'s thesis that cinema was invented out of the needs of scientific research. Conversely, they\\'re informed by Muybridge\\'s background as an artistic location photographer and, as Phillip Prodger suggests, in book sales and more useful to art than to science, as Marta Braun has demonstrated (see sources at bottom). Additionally, Muybridge quickly exploited their entertainment value via projection to audiences across the U.S. and Europe. Muybridge pursued both of these paths of invention: the path taken by Jules Janssen, Étienne-Jules Marey and others for science and the path taken by Ottomar Anschütz, Thomas Edison, the Lumiére brothers and others for fame and profit.\\n\\nMuybridge began taking instantaneous single photographs of multi-millionaire railroad magnate Leland Stanford\\'s horses in motion in 1872. It was disputed at the time whether all four of a horse\\'s legs were off the ground simultaneously at any time while running. Although no surviving photographs prove it, contemporary lithographs and paintings likely based on the photographs, indeed, show the moment of \"unsupported transit\". In between and interrupting these experiments, Muybridge was found not guilty of the admittedly premeditated fatal shooting of his wife\\'s lover and possibly her son\\'s father.\\n\\nPublication of Marey\\'s graphic measurements of a horse\\'s movements reignited Stanford\\'s interest in the gait of horses. In turn, Marey was convinced to switch to photography in his motion studies after witnessing Muybridge\\'s work (see \"Falling Cat\" (1894)). This work in \"automatic electro-photographs\" began in 1878 at Stanford\\'s Palo Alto Stock Farm. Multiple cameras were stored in a shed parallel to a track. A series of closing boards serving as shutters were triggered by tripped threads and electrical means. The wet collodion process of the time, reportedly, could need up to half a minute for an exposure. For the split-second shutter speeds required here, a white canvas background and powdered lime on the track provided more contrast to compensate for less light getting to the glass plates. Employees of Stanford\\'s Central Pacific Railroad and others helped in constructing this \"set\" and camera equipment.\\n\\nContrary to unattributed claims on the web, this so-called \"Sallie Gardner at a Gallop\" wasn\\'t the first series photographed by Muybridge. Six series of Muybridge\\'s first subjects were published on cards entitled \"The Horse in Motion\". The first is of the horse Abe Edgington trotting on 11 June 1878. Reporters were invited for the next two series on June 15th, and, as they reported, again, Abe went first—trotting and pulling the driver behind in a sulky, which is what tripped the threads. The second subject that day was Sallie Gardner running and, thus, the mare had to trip the threads. Reporters noted how this spooked her and how that was reflected in the negatives developed on the spot. As one article said, she \"gave a wild bound in the air, breaking the saddle girth as she left the ground.\" Based on such descriptions, it doesn\\'t seem that this series exists anymore. The animations on the web that are actually of Sallie are dated June 19th on \"The Horse in Motion\" card. Many animations claimed to be Sallie on YouTube, Wikipedia and elsewhere, as of this date, are actually of a mare named Annie G. and were part of Muybridge\\'s University of Pennsylvania work published in 1887, as the Library of Congress and other reliable sources have made clear. The early Palo Alto photographs aren\\'t as detailed and are closer to silhouettes. The 12 images of Gardner also include one where she\\'s stationary. The Morse\\'s Gallery pictures are entirely in silhouette, while the La Nature engravings of these same images show the rider in a white shirt.\\n\\nThe shot of the horse stationary, as Braun points out, was added later and is indicative of the artistic and un-scientific assemblages Muybridge made of his images—with the intent of publication, including in his own books. This was especially prominent in his Pennsylvania work, which included many nude models that were surely useful for art. Muybridge influenced artists from Realists like Thomas Eakins and Meissonier, Impressionists like Edgar Degas and Frederick Remington, to the more abstract works of Francis Bacon. His precedence has also been cited in the photography of Steven Pippin and Hollis Frampton, as well as the bullet-time effects in \"The Matrix\" (1999).\\n\\nMuybridge lectured on this relationship with art when touring with his Zoöpraxiscope, which was a combination of the magic lantern and phenakistoscope. With it, he projected, from glass disks, facsimiles of his photographs hand-painted by Erwin Faber. Without intermittent movement, the Zoöpraxiscope compressed the images, so elongated drawings were used instead of photographs. Muybridge and others also used his images for phenakistoscopes and zoetropes. The first demonstration of the Zoöpraxiscope was to Stanford and friends in the autumn of 1879. A public demonstration was given on 4 May 1880 for the San Francisco art association, and Muybridge continued these lectures for years—personally touring the U.S. and Europe. Although there were predecessors in animated projections as far back as 1847 by Leopold Ludwig Döbler, in 1853 by Franz von Uchatius, and with posed photographs by Henry Heyl in 1870, the chronophotographic and artistic basis offered some novelty for Muybridge\\'s presentations. They also led him to meet Edison and Marey and inspire the likes of Anschütz and others—those who took the next steps in the invention of movies.\\n\\n(Main Sources: \"The Inventor and the Tycoon\" by Edward Ball. \"Eadweard Muybridge\" and \"Picturing Time\" by Marta Braun. \"The Man Who Stopped Time\" by Brian Clegg. \"Man in Motion\" by Robert Bartlett Haas. \"The Father of the Motion Picture\" by Gordon Hendricks. \"The Stanford Years, 1872-1882\" edited by Anita Ventura Mozley. \"Time Stands Still\" by Phillip Prodger. \"Cinema Before Cinema\" by Virgilio Tosi.)',\n",
       "  'MovieYear': 1878}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray, pprint, time, os\n",
    "from ray.data import read_parquet\n",
    "\n",
    "# Start Ray before using ray functions.\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Example on doc page works.\n",
    "# s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/train/\"\n",
    "# ds = ray.data.read_images(s3_uri, mode=\"RGB\")\n",
    "\n",
    "# Now try with local parquet file.\n",
    "file_path = \"data/kaggle_imdb_small.parquet\"\n",
    "ds = ray.data.read_parquet(file_path) # doesn't work\n",
    "\n",
    "# Inspect the dataset.\n",
    "print(f\"Number rows: {ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(ds)\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embedding model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymilvus\n",
    "pymilvus.__version__\n",
    "\n",
    "# Must be >= 2.4.0 in order for following code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5123ee19723b4c23b95f6fbfb035e90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_dim: 1024\n"
     ]
    }
   ],
   "source": [
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "# Initialize a built-in Milvus sparse-dense-late-interaction-reranking encoder.\n",
    "# https://huggingface.co/BAAI/bge-m3\n",
    "encoder = BGEM3EmbeddingFunction(use_fp16=False, device=\"cpu\")\n",
    "dense_dim = encoder.dim[\"dense\"]\n",
    "print(f\"dense_dim: {dense_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test of the encoder.\n",
    "# from typing import List\n",
    "\n",
    "# df = pd.read_parquet(\"data/kaggle_imdb_small.parquet\")\n",
    "# print(df.shape)\n",
    "\n",
    "# # Get the documents to embed\n",
    "# docs = df['text'].to_list()\n",
    "\n",
    "# # Ensure docs is a list of strings\n",
    "# assert isinstance(docs, List)\n",
    "# assert all(isinstance(doc, str) for doc in docs)\n",
    "\n",
    "# # Encode the documents.\n",
    "# docs_embeddings = encoder(docs)\n",
    "\n",
    "# print(type(docs_embeddings['dense']), len(docs_embeddings['dense']), len(docs_embeddings['dense'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Milvus collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of server: Zilliz Cloud Vector Database(Compatible with Milvus 2.3)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1. CONNECT TO ZILLIZ CLOUD\n",
    "\n",
    "# !pip install pymilvus #python sdk for milvus\n",
    "from pymilvus import connections, utility\n",
    "import os\n",
    "TOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n",
    "\n",
    "# Connect to Zilliz cloud using endpoint URI and API key TOKEN.\n",
    "# TODO change this.\n",
    "CLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "CLUSTER_ENDPOINT=\"https://in03-48a5b11fae525c9.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "connections.connect(\n",
    "  alias='default',\n",
    "  #  Public endpoint obtained from Zilliz Cloud\n",
    "  uri=CLUSTER_ENDPOINT,\n",
    "  # API key or a colon-separated cluster username and password\n",
    "  token=TOKEN,\n",
    ")\n",
    "\n",
    "# Check if the server is ready and get collection name.\n",
    "print(f\"Type of server: {utility.get_server_version()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped collection: `imdb_metadata`\n",
      "Successfully created collection: `imdb_metadata`\n",
      "{'collection_name': 'imdb_metadata', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}], 'aliases': [], 'collection_id': 448076879578422052, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# Set the Milvus collection name.\n",
    "COLLECTION_NAME = \"imdb_metadata\"\n",
    "\n",
    "# Use no-schema Milvus client uses flexible json key:value format.\n",
    "# https://milvus.io/docs/using_milvusclient.md\n",
    "mc = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    # API key or a colon-separated cluster username and password\n",
    "    token=TOKEN)\n",
    "\n",
    "# Check if collection already exists, if so drop it.\n",
    "has = utility.has_collection(COLLECTION_NAME)\n",
    "if has:\n",
    "    drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# Create the collection using AUTOINDEX.\n",
    "mc.create_collection(COLLECTION_NAME, \n",
    "                     dense_dim,\n",
    "                     consistency_level=\"Eventually\", \n",
    "                     auto_id=True,  \n",
    "                     overwrite=True,\n",
    "                    )\n",
    "\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")\n",
    "print(mc.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def recursive_splitter_wrapper(text, chunk_size):\n",
    "\n",
    "    # Default chunk overlap is 10% chunk_size.\n",
    "    chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "\n",
    "    # Use langchain's convenient recursive chunking method.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks: List[str] = text_splitter.split_text(text)\n",
    "\n",
    "    # Replace special characters with spaces.\n",
    "    chunks = [text.replace(\"<br /><br />\", \" \") for text in chunks]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Use recursive splitter to chunk text.\n",
    "def imdb_chunk_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    BATCH_SIZE = 100\n",
    "    CHUNK_SIZE = 512\n",
    "\n",
    "    batch = df.head(BATCH_SIZE).copy()\n",
    "    print(f\"chunk size: {CHUNK_SIZE}\")\n",
    "    print(f\"original shape: {batch.shape}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Chunk the text review into chunk_size.\n",
    "    batch['chunk'] = batch['text'].apply(recursive_splitter_wrapper, chunk_size=CHUNK_SIZE)\n",
    "    # Explode the 'chunk' column to create new rows for each chunk.\n",
    "    batch = batch.explode('chunk', ignore_index=True)\n",
    "    batch['chunk'] = batch['chunk'].fillna('')\n",
    "    print(f\"new shape: {batch.shape}\")\n",
    "\n",
    "    # 2. Add embeddings as new column in df.\n",
    "    docs = batch['chunk'].to_list()\n",
    "    # Ensure docs is a list of strings\n",
    "    assert isinstance(docs, List)\n",
    "    assert all(isinstance(doc, str) for doc in docs)\n",
    "    # Encode the documents. bge-m3 dense embeddings will be already normalized.\n",
    "    embeddings = encoder(docs)\n",
    "    batch['vector'] = embeddings['dense']\n",
    "\n",
    "    # 4. Drop the original 'text' column, keep the new 'chunk' column.\n",
    "    batch.drop(columns=['text'], axis=1, inplace=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Chunking + embedding time for {BATCH_SIZE} docs: {end_time - start_time} sec\")\n",
    "    assert len(batch.vector[0]) == dense_dim\n",
    "    print(f\"type embeddings: {type(batch.vector)} of {type(batch.vector[0])}\")\n",
    "    print(f\"of numbers: {type(batch.vector[0][0])}\")\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test of the UDF function.\n",
    "# from typing import List\n",
    "\n",
    "# df = pd.read_parquet(\"data/kaggle_imdb_small.parquet\")\n",
    "\n",
    "# # Replace null values with empty strings.\n",
    "# df['text'] = df['text'].fillna('')\n",
    "\n",
    "# batch = imdb_chunk_text(df)\n",
    "# display(batch.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform: 100\n",
      "<class 'ray.data.dataset.Dataset'>\n",
      "After transform number rows: 100\n",
      "MapBatches(imdb_chunk_text)\n",
      "+- Dataset(\n",
      "      num_rows=100,\n",
      "      schema={\n",
      "         id: int64,\n",
      "         url: string,\n",
      "         Name: string,\n",
      "         PosterLink: string,\n",
      "         Genres: list<element: string>,\n",
      "         Actors: list<element: string>,\n",
      "         Director: string,\n",
      "         Keywords: list<element: string>,\n",
      "         RatingValue: double,\n",
      "         ReviewAurthor: string,\n",
      "         ReviewDate: string,\n",
      "         duration: string,\n",
      "         text: string,\n",
      "         MovieYear: int64\n",
      "      }\n",
      "   )\n"
     ]
    }
   ],
   "source": [
    "# verify row count\n",
    "print(f\"Before transform: {ds.count()}\")\n",
    "\n",
    "# Transform the data using the UDF.\n",
    "transformed_ds = ds.map_batches(imdb_chunk_text, batch_format=\"pandas\")\n",
    "\n",
    "# Inspect the dataset.\n",
    "print(type(transformed_ds))\n",
    "print(f\"After transform number rows: {transformed_ds.count()}\")\n",
    "# print(\"\\nMetadata: \")\n",
    "# error printing vectors?\n",
    "# # print(transformed_ds.materialize()) # doesn't work\n",
    "\n",
    "print(transformed_ds) # shows the old ds, not the transformed_ds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shutdown ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
