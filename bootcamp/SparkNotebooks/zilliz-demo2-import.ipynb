{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182d2c43-cc97-41f3-9e31-bdb8015335bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
    "import io.milvus.grpc.{DataType, ImportResponse}\n",
    "import io.milvus.param.bulkinsert.{BulkInsertParam, GetBulkInsertStateParam}\n",
    "import io.milvus.param.collection.{CreateCollectionParam, FieldType}\n",
    "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
    "\n",
    "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
    "\n",
    "import org.apache.hadoop.fs.s3a.S3AFileSystem\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "\n",
    "import java.io.{BufferedReader, DataOutputStream, InputStreamReader}\n",
    "import java.net.{HttpURLConnection, URI, URL}\n",
    "import java.util\n",
    "\n",
    "val sparkConf = new SparkConf().setMaster(\"local\")\n",
    "val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n",
    "\n",
    "// Fill in user's Zilliz Cloud credentials.\n",
    "val uri = \"https://in01-xxxxxx.aws-us-west-2.vectordb.zillizcloud.com:19535\"\n",
    "val clusterId = \"in01-xxxxxx\"\n",
    "val token = \"db_admin:xxxx\"\n",
    "val apiKey = \"xxxxx\"\n",
    "val region = \"aws-us-west-2\"\n",
    "// Specify the target Zilliz Cloud vector database collection name.\n",
    "val collectionName = \"databricks_zilliz_import_demo\"\n",
    "// This file simulates a dataframe from user's vector generation job or a Delta table that contains vectors.\n",
    "val inputFilePath = \"/Volumes/zilliz_test/default/sample_vectors/dim32_1k.json\"\n",
    "// User needs to create an external location on databricks with an S3 bucket and specify the directory in the bucket to store vector data.\n",
    "// The vectors will be output to the s3 bucket in specific format that can be loaded to Zilliz Cloud efficiently.\n",
    "val outputDir = \"s3://jiang-bulk-insert-test/zilliz_spark_demo/\"\n",
    "// The AWS access key and private key which grants only read access to the above s3 bucket. They will be used by Zilliz Cloud Import Data API to load data from the bucket.\n",
    "val s3ak = \"xxxxx\"\n",
    "val s3sk = \"xxxxx\"\n",
    "\n",
    "// 1. Create Zilliz Cloud vector db collection through SDK, and define the schema of the collection.\n",
    "val connectParam: ConnectParam = ConnectParam.newBuilder\n",
    "  .withUri(uri)\n",
    "  .withToken(token)\n",
    "  .build\n",
    "\n",
    "val client: MilvusClient = new MilvusServiceClient(connectParam)\n",
    "\n",
    "val field1Name: String = \"id_field\"\n",
    "val field2Name: String = \"str_field\"\n",
    "val field3Name: String = \"float_vector_field\"\n",
    "val fieldsSchema: util.List[FieldType] = new util.ArrayList[FieldType]\n",
    "\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withPrimaryKey(true)\n",
    "  .withAutoID(false)\n",
    "  .withDataType(DataType.Int64)\n",
    "  .withName(field1Name)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.VarChar)\n",
    "  .withName(field2Name)\n",
    "  .withMaxLength(65535)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.FloatVector)\n",
    "  .withName(field3Name)\n",
    "  .withDimension(32)\n",
    "  .build\n",
    ")\n",
    "\n",
    "// create collection\n",
    "val createParam: CreateCollectionParam = CreateCollectionParam.newBuilder\n",
    "  .withCollectionName(collectionName)\n",
    "  .withFieldTypes(fieldsSchema)\n",
    "  .build\n",
    "\n",
    "val createR: R[RpcStatus] = client.createCollection(createParam)\n",
    "\n",
    "// 2. Read data from file to build vector dataframe. The schema of the dataframe must logically match the schema of vector db.\n",
    "val df = spark.read\n",
    "  .schema(new StructType()\n",
    "    .add(field1Name, IntegerType)\n",
    "    .add(field2Name, StringType)\n",
    "    .add(field3Name, ArrayType(FloatType), false))\n",
    "  .json(inputFilePath)\n",
    "\n",
    "// 3. Store all vector data in the s3 bucket to prepare for loading. \n",
    "df.repartition(1)\n",
    "  .write\n",
    "  .format(\"mjson\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(outputDir)\n",
    "\n",
    "// 4. As the vector data has been stored in the s3 bucket as files, here we list the directory and get the file paths\n",
    "// to prepare input of Zilliz Cloud Import Data API call.\n",
    "val hadoopConfig = spark.sparkContext.hadoopConfiguration\n",
    "val directory = new Path(outputDir)\n",
    "val fs = FileSystem.get(directory.toUri, hadoopConfig)\n",
    "val files = fs.listStatus(directory)\n",
    "val ouputPath = files.filter(file => {\n",
    "    file.getPath.getName.endsWith(\".json\")\n",
    "})(0)\n",
    "val completeJsonPath = ouputPath.getPath\n",
    "println(s\"completeJsonPath: ${completeJsonPath}\")\n",
    "\n",
    "// 5. Make a call to Zilliz Cloud Import Data API.\n",
    "val importApiUrl = s\"https://controller.api.${region}.zillizcloud.com/v1/vector/collections/import\"\n",
    "val postData =\n",
    "  s\"\"\"\n",
    "    |{\n",
    "    |  \"clusterId\": \"${clusterId}\",\n",
    "    |  \"collectionName\": \"${collectionName}\",\n",
    "    |  \"objectUrl\": \"${completeJsonPath}\",\n",
    "    |  \"accessKey\": \"${s3ak}\",\n",
    "    |  \"secretKey\": \"${s3sk}\"\n",
    "    |}\n",
    "    |\"\"\".stripMargin\n",
    "val url = new URL(importApiUrl)\n",
    "val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n",
    "\n",
    "try {\n",
    "  // Set up the request method and headers\n",
    "  connection.setRequestMethod(\"POST\")\n",
    "  connection.setRequestProperty(\"Authorization\", s\"Bearer ${apiKey}\")\n",
    "  connection.setRequestProperty(\"Content-Type\", \"application/json\")\n",
    "  connection.setRequestProperty(\"accept\", \"application/json\")\n",
    "  connection.setDoOutput(true)\n",
    "\n",
    "  // Write the POST data to the connection\n",
    "  val out = new DataOutputStream(connection.getOutputStream)\n",
    "  out.writeBytes(postData)\n",
    "  out.flush()\n",
    "  out.close()\n",
    "\n",
    "  // Read the response\n",
    "  val inputStream = new BufferedReader(new InputStreamReader(connection.getInputStream))\n",
    "  var inputLine: String = null\n",
    "  val response = new StringBuilder\n",
    "\n",
    "  while ({inputLine = inputStream.readLine(); inputLine != null}) {\n",
    "    response.append(inputLine)\n",
    "  }\n",
    "  inputStream.close()\n",
    "\n",
    "  // Print the response\n",
    "  println(s\"Bulkinsert Response code: ${connection.getResponseCode}\")\n",
    "  println(s\"Bulkinsert Response body: ${response.toString}\")\n",
    "} finally {\n",
    "  // Disconnect to release resources\n",
    "  connection.disconnect()\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-01-03 12:08:08",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
