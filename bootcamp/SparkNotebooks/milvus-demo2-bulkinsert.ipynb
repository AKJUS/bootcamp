{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c2ecb-6ea2-418d-b4e1-5f9c1dc18239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
    "import io.milvus.grpc.{DataType, ImportResponse}\n",
    "import io.milvus.param.bulkinsert.{BulkInsertParam, GetBulkInsertStateParam}\n",
    "import io.milvus.param.collection.{CreateCollectionParam, FieldType}\n",
    "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import org.slf4j.LoggerFactory\n",
    "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import java.net.URI\n",
    "import org.apache.log4j.Logger\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "import java.util\n",
    "\n",
    "var logger = Logger.getLogger(this.getClass())\n",
    "\n",
    "val sparkConf = new SparkConf().setMaster(\"local\")\n",
    "val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n",
    "// Fill in user's Milvus instance credentials.\n",
    "val host = \"127.0.0.1\"\n",
    "val port = 19530\n",
    "val username = \"root\"\n",
    "val password = \"Milvus\"\n",
    "// Specify the target Milvus collection name.\n",
    "val collectionName = \"spark_milvus_test\"\n",
    "// This file simulates a dataframe from user's vector generation job or a Delta table that contains vectors.\n",
    "val filePath = \"/Volumes/zilliz_test/default/sample_vectors/dim32_1k.json\"\n",
    "// The S3 bucket is an internal bucket of the Milvus instance, which the user has full control of.\n",
    "// The user needs to set up this bucket as \"storage crenditial\" by following\n",
    "// the instruction at https://docs.databricks.com/en/connect/unity-catalog/storage-credentials.html#step-2-give-databricks-the-iam-role-details\n",
    "// Here the user can specify the directory in the bucket to store vector data.\n",
    "// The vectors will be output to the s3 bucket in specific format that can be loaded to Zilliz Cloud efficiently.\n",
    "val outputPath = \"s3://your-s3-bucket-name/filesaa/spark_output\"\n",
    "\n",
    "// 1. Create Milvus collection through Milvus SDK\n",
    "val connectParam: ConnectParam = ConnectParam.newBuilder\n",
    "  .withHost(host)\n",
    "  .withPort(port)\n",
    "  .withAuthorization(username, password)\n",
    "  .build\n",
    "\n",
    "val client: MilvusClient = new MilvusServiceClient(connectParam)\n",
    "\n",
    "val field1Name: String = \"id_field\"\n",
    "val field2Name: String = \"str_field\"\n",
    "val field3Name: String = \"float_vector_field\"\n",
    "val fieldsSchema: util.List[FieldType] = new util.ArrayList[FieldType]\n",
    "\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withPrimaryKey(true)\n",
    "  .withAutoID(false)\n",
    "  .withDataType(DataType.Int64)\n",
    "  .withName(field1Name)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.VarChar)\n",
    "  .withName(field2Name)\n",
    "  .withMaxLength(65535)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.FloatVector)\n",
    "  .withName(field3Name)\n",
    "  .withDimension(32)\n",
    "  .build\n",
    ")\n",
    "\n",
    "// create collection\n",
    "val createParam: CreateCollectionParam = CreateCollectionParam.newBuilder\n",
    "  .withCollectionName(collectionName)\n",
    "  .withFieldTypes(fieldsSchema)\n",
    "  .build\n",
    "\n",
    "val createR: R[RpcStatus] = client.createCollection(createParam)\n",
    "\n",
    "logger.info(s\"create collection ${collectionName} resp: ${createR.toString}\")\n",
    "\n",
    "// 2. Read data from file to build vector dataframe. The schema of the dataframe must logically match the schema of vector db.\n",
    "val df = spark.read\n",
    "  .schema(new StructType()\n",
    "    .add(field1Name, IntegerType)\n",
    "    .add(field2Name, StringType)\n",
    "    .add(field3Name, ArrayType(FloatType), false))\n",
    "  .json(filePath)\n",
    "\n",
    "// 3. Store all vector data in the s3 bucket to prepare for loading. \n",
    "df.repartition(1)\n",
    "  .write\n",
    "  .format(\"mjson\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(outputPath)\n",
    "\n",
    "// 4. As the vector data has been stored in the s3 bucket as files, here we list the directory and get the file paths\n",
    "// to prepare input of Zilliz Cloud Import Data API call.\n",
    "val hadoopConfig = spark.sparkContext.hadoopConfiguration\n",
    "val directory = new Path(outputPath)\n",
    "val fs = FileSystem.get(directory.toUri, hadoopConfig)\n",
    "val files = fs.listStatus(directory)\n",
    "val ouputPath = files.filter(file => {\n",
    "    file.getPath.getName.endsWith(\".json\")\n",
    "})(0)\n",
    "def extractPathWithoutBucket(s3Path: String): String = {\n",
    "  val uri = new URI(s3Path)\n",
    "  val pathWithoutBucket = uri.getPath.drop(1)  // Drop the leading '/'\n",
    "  pathWithoutBucket\n",
    "}\n",
    "val ouputFilePathWithoutBucket = extractPathWithoutBucket(ouputPath.getPath.toString)\n",
    "\n",
    "// 5. Make a call to Milvus bulkinsert API.\n",
    "val bulkInsertFiles:List[String] = List(ouputFilePathWithoutBucket)\n",
    "val bulkInsertParam: BulkInsertParam = BulkInsertParam.newBuilder\n",
    "    .withCollectionName(collectionName)\n",
    "    .withFiles(bulkInsertFiles.asJava)\n",
    "    .build\n",
    "\n",
    "val bulkInsertR: R[ImportResponse] = client.bulkInsert(bulkInsertParam)\n",
    "logger.info(s\"bulkinsert ${collectionName} resp: ${bulkInsertR.toString}\")\n",
    "val taskId: Long = bulkInsertR.getData.getTasksList.get(0)\n",
    "\n",
    "var bulkloadState = client.getBulkInsertState(GetBulkInsertStateParam.newBuilder.withTask(taskId).build)\n",
    "while (bulkloadState.getData.getState.getNumber != 1 &&\n",
    "    bulkloadState.getData.getState.getNumber != 6 &&\n",
    "    bulkloadState.getData.getState.getNumber != 7 ) {\n",
    "    bulkloadState = client.getBulkInsertState(GetBulkInsertStateParam.newBuilder.withTask(taskId).build)\n",
    "    logger.info(s\"bulkinsert ${collectionName} resp: ${bulkInsertR.toString} state: ${bulkloadState}\")\n",
    "    Thread.sleep(3000)\n",
    "}\n",
    "if (bulkloadState.getData.getState.getNumber != 6) {\n",
    "    logger.error(s\"bulkinsert failed ${collectionName} state: ${bulkloadState}\")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "milvus-demo2-bulkinsert",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
