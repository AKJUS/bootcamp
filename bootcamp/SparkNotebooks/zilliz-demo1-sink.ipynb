{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c2ecb-6ea2-418d-b4e1-5f9c1dc18239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">R{status=0, data=status {\n",
       "}\n",
       "db_name: &quot;default&quot;\n",
       "coll_segIDs {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value {\n",
       "    data: 446152789476959054\n",
       "  }\n",
       "}\n",
       "flush_coll_segIDs {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value {\n",
       "  }\n",
       "}\n",
       "coll_seal_times {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value: 1704181057\n",
       "}\n",
       "}\n",
       "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
       "import io.milvus.grpc.DataType\n",
       "import io.milvus.grpc.{DataType, FlushResponse}\n",
       "import io.milvus.param.collection.{CreateCollectionParam, FieldType, FlushParam}\n",
       "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
       "import org.slf4j.LoggerFactory\n",
       "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
       "import java.util\n",
       "sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7371bdd\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3f34177b\n",
       "uri: String = https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535\n",
       "token: String = db_admin:Sb3$BMsHpHuK$0Te\n",
       "collectionName: String = databricks_milvus_insert_demo\n",
       "filePath: String = /Volumes/zilliz_test/default/sample_vectors/row.json\n",
       "connectParam: io.milvus.param.ConnectParam = ConnectParam(host=in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com, port=19535, databaseName=null, uri=https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, token=db_admin:Sb3$BMsHpHuK$0Te, connectTimeoutMs=10000, keepAliveTimeMs=55000, keepAliveTimeoutMs=20000, keepAliveWithoutCalls=false, rpcDeadlineMs=0, secure=true, idleTimeoutMs=86400000, authorization=ZGJfYWRtaW46U2IzJEJNc0hwSHVLJDBUZQ==, clientKeyPath=null, clientPemPath=null, caPemPath=null, serverPemPath=null, serverName=null)\n",
       "client: io.milvus.client.MilvusClient = io.milvus.client.MilvusServiceClient@4fe31fc2\n",
       "field1Name: String = id_field\n",
       "field2Name: String = str_field\n",
       "field3Name: String = float_vector_field\n",
       "fieldsSchema: java.util.List[io.milvus.param.collection.FieldType] = [FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}]\n",
       "createParam: io.milvus.param.collection.CreateCollectionParam = CreateCollectionParam(collectionName=databricks_milvus_insert_demo, shardsNum=0, description=, fieldTypes=[FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}], partitionsNum=0, consistencyLevel=BOUNDED, databaseName=null, enableDynamicField=false)\n",
       "createR: io.milvus.param.R[io.milvus.param.RpcStatus] = R{status=0, data=RpcStatus{msg='Success'}}\n",
       "df: org.apache.spark.sql.DataFrame = [id_field: int, str_field: string ... 1 more field]\n",
       "milvusOptions: scala.collection.immutable.Map[String,String] = Map(milvus.uri -&gt; https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, milvus.token -&gt; db_admin:Sb3$BMsHpHuK$0Te, milvus.collectionName -&gt; databricks_milvus_insert_demo)\n",
       "flushParam: io.milvus.param.collection.FlushParam = FlushParam{collectionNames='[databricks_milvus_insert_demo]', syncFlush=true, syncFlushWaitingInterval=500, syncFlushWaitingTimeout=60}\n",
       "flushR: io.milvus.param.R[io.milvus.grpc.FlushResponse] =\n",
       "R{status=0, data=status {\n",
       "}\n",
       "db_name: &quot;default&quot;\n",
       "coll_segIDs {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value {\n",
       "    data: 446152789476959054\n",
       "  }\n",
       "}\n",
       "flush_coll_segIDs {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value {\n",
       "  }\n",
       "}\n",
       "coll_seal_times {\n",
       "  key: &quot;databricks_milvus_insert_demo&quot;\n",
       "  value: 1704181057\n",
       "}\n",
       "}\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">R{status=0, data=status {\n}\ndb_name: &quot;default&quot;\ncoll_segIDs {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value {\n    data: 446152789476959054\n  }\n}\nflush_coll_segIDs {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value {\n  }\n}\ncoll_seal_times {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value: 1704181057\n}\n}\nimport io.milvus.client.{MilvusClient, MilvusServiceClient}\nimport io.milvus.grpc.DataType\nimport io.milvus.grpc.{DataType, FlushResponse}\nimport io.milvus.param.collection.{CreateCollectionParam, FieldType, FlushParam}\nimport io.milvus.param.{ConnectParam, R, RpcStatus}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{SaveMode, SparkSession}\nimport org.slf4j.LoggerFactory\nimport zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\nimport java.util\nsparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7371bdd\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3f34177b\nuri: String = https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535\ntoken: String = db_admin:Sb3$BMsHpHuK$0Te\ncollectionName: String = databricks_milvus_insert_demo\nfilePath: String = /Volumes/zilliz_test/default/sample_vectors/row.json\nconnectParam: io.milvus.param.ConnectParam = ConnectParam(host=in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com, port=19535, databaseName=null, uri=https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, token=db_admin:Sb3$BMsHpHuK$0Te, connectTimeoutMs=10000, keepAliveTimeMs=55000, keepAliveTimeoutMs=20000, keepAliveWithoutCalls=false, rpcDeadlineMs=0, secure=true, idleTimeoutMs=86400000, authorization=ZGJfYWRtaW46U2IzJEJNc0hwSHVLJDBUZQ==, clientKeyPath=null, clientPemPath=null, caPemPath=null, serverPemPath=null, serverName=null)\nclient: io.milvus.client.MilvusClient = io.milvus.client.MilvusServiceClient@4fe31fc2\nfield1Name: String = id_field\nfield2Name: String = str_field\nfield3Name: String = float_vector_field\nfieldsSchema: java.util.List[io.milvus.param.collection.FieldType] = [FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}]\ncreateParam: io.milvus.param.collection.CreateCollectionParam = CreateCollectionParam(collectionName=databricks_milvus_insert_demo, shardsNum=0, description=, fieldTypes=[FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}], partitionsNum=0, consistencyLevel=BOUNDED, databaseName=null, enableDynamicField=false)\ncreateR: io.milvus.param.R[io.milvus.param.RpcStatus] = R{status=0, data=RpcStatus{msg='Success'}}\ndf: org.apache.spark.sql.DataFrame = [id_field: int, str_field: string ... 1 more field]\nmilvusOptions: scala.collection.immutable.Map[String,String] = Map(milvus.uri -&gt; https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, milvus.token -&gt; db_admin:Sb3$BMsHpHuK$0Te, milvus.collectionName -&gt; databricks_milvus_insert_demo)\nflushParam: io.milvus.param.collection.FlushParam = FlushParam{collectionNames='[databricks_milvus_insert_demo]', syncFlush=true, syncFlushWaitingInterval=500, syncFlushWaitingTimeout=60}\nflushR: io.milvus.param.R[io.milvus.grpc.FlushResponse] =\nR{status=0, data=status {\n}\ndb_name: &quot;default&quot;\ncoll_segIDs {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value {\n    data: 446152789476959054\n  }\n}\nflush_coll_segIDs {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value {\n  }\n}\ncoll_seal_times {\n  key: &quot;databricks_milvus_insert_demo&quot;\n  value: 1704181057\n}\n}\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id_field",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "str_field",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "float_vector_field",
            "nullable": true,
            "type": {
             "containsNull": true,
             "elementType": "float",
             "type": "array"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
    "import io.milvus.grpc.DataType\n",
    "import io.milvus.grpc.{DataType, FlushResponse}\n",
    "import io.milvus.param.collection.{CreateCollectionParam, FieldType, FlushParam}\n",
    "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import org.slf4j.LoggerFactory\n",
    "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
    "\n",
    "import java.util\n",
    "\n",
    "val sparkConf = new SparkConf().setMaster(\"local\")\n",
    "val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n",
    "// Fill in user's Zilliz Cloud credentials.\n",
    "val uri = \"https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535\"\n",
    "val token = \"db_admin:xxxx\"\n",
    "// Specify the target Zilliz Cloud vector database collection name.\n",
    "val collectionName = \"databricks_milvus_insert_demo\"\n",
    "// This file simulates a dataframe from user's vector generation job or a Delta table that contains vectors.\n",
    "val filePath = \"/Volumes/zilliz_test/default/sample_vectors/dim32_1k.json\"\n",
    "\n",
    "// 1. Create Zilliz Cloud vector db collection through SDK, and define the schema of the collection.\n",
    "val connectParam: ConnectParam = ConnectParam.newBuilder\n",
    "  .withUri(uri)\n",
    "  .withToken(token)\n",
    "  .build\n",
    "\n",
    "val client: MilvusClient = new MilvusServiceClient(connectParam)\n",
    "\n",
    "val field1Name: String = \"id_field\"\n",
    "val field2Name: String = \"str_field\"\n",
    "val field3Name: String = \"float_vector_field\"\n",
    "val fieldsSchema: util.List[FieldType] = new util.ArrayList[FieldType]\n",
    "\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withPrimaryKey(true)\n",
    "  .withAutoID(false)\n",
    "  .withDataType(DataType.Int64)\n",
    "  .withName(field1Name)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.VarChar)\n",
    "  .withName(field2Name)\n",
    "  .withMaxLength(65535)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.FloatVector)\n",
    "  .withName(field3Name)\n",
    "  .withDimension(32)\n",
    "  .build\n",
    ")\n",
    "\n",
    "// create collection\n",
    "val createParam: CreateCollectionParam = CreateCollectionParam.newBuilder\n",
    "  .withCollectionName(collectionName)\n",
    "  .withFieldTypes(fieldsSchema)\n",
    "  .build\n",
    "\n",
    "val createR: R[RpcStatus] = client.createCollection(createParam)\n",
    "\n",
    "// log.info(s\"create collection ${collectionName} resp: ${createR.toString}\")\n",
    "\n",
    "// 2. Read data from file to build vector dataframe. The schema of the dataframe must logically match the schema of vector db.\n",
    "val df = spark.read\n",
    "  .schema(new StructType()\n",
    "    .add(field1Name, IntegerType)\n",
    "    .add(field2Name, StringType)\n",
    "    .add(field3Name, ArrayType(FloatType), false))\n",
    "  .json(filePath)\n",
    "\n",
    "// 3. Configure output target\n",
    "val milvusOptions = Map(\n",
    "  MILVUS_URI -> uri,\n",
    "  MILVUS_TOKEN -> token,\n",
    "  MILVUS_COLLECTION_NAME -> collectionName,\n",
    ")\n",
    "\n",
    "// 4. Insert data to Zilliz Cloud vector db collection\n",
    "df.write\n",
    "  .options(milvusOptions)\n",
    "  .format(\"milvus\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save()\n",
    "\n",
    "// flush data (The following implementation will insert the vector data row by row through Milvus SDK Insert API)\n",
    "val flushParam: FlushParam = FlushParam.newBuilder\n",
    "  .addCollectionName(collectionName)\n",
    "  .build\n",
    "val flushR: R[FlushResponse] = client.flush(flushParam)\n",
    "println(flushR)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "zilliz-demo1-sink",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
