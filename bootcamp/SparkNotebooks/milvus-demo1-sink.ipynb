{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c2ecb-6ea2-418d-b4e1-5f9c1dc18239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
       "import io.milvus.grpc.DataType\n",
       "import io.milvus.param.collection.{CreateCollectionParam, FieldType}\n",
       "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
       "import org.slf4j.LoggerFactory\n",
       "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
       "import java.util\n",
       "sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2967c117\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5669e1a7\n",
       "host: String = 52.13.11.185\n",
       "port: Int = 19530\n",
       "collectionName: String = hello_spark_milvus3\n",
       "filePath: String = /Volumes/zilliz_test/default/sample_vectors/row.json\n",
       "connectParam: io.milvus.param.ConnectParam = ConnectParam(host=52.13.11.185, port=19530, databaseName=default, uri=null, token=null, connectTimeoutMs=10000, keepAliveTimeMs=55000, keepAliveTimeoutMs=20000, keepAliveWithoutCalls=false, rpcDeadlineMs=0, secure=false, idleTimeoutMs=86400000, authorization=cm9vdDpNaWx2dXM=, clientKeyPath=null, clientPemPath=null, caPemPath=null, serverPemPath=null, serverName=null)\n",
       "client: io.milvus.client.MilvusClient = io.milvus.client.MilvusServiceClient@21e4f472\n",
       "field1Name: String = id_field\n",
       "field2Name: String = str_field\n",
       "field3Name: String = float_vector_field\n",
       "fieldsSchema: java.util.List[io.milvus.param.collection.FieldType] = [FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}]\n",
       "createParam: io.milvus.param.collection.CreateCollectionParam = CreateCollectionParam(collectionName=hello_spark_milvus3, shardsNum=0, description=, fieldTypes=[FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}], partitionsNum=0, consistencyLevel=BOUNDED, databaseName=null, enableDynamicField=false)\n",
       "createR: io.milvus.param.R[io.milvus.param.RpcStatus] = R{status=0, data=RpcStatus{msg='Success'}}\n",
       "df: org.apache.spark.sql.DataFrame = [id_field: int, str_field: string ... 1 more field]\n",
       "milvusOptions: scala.collection.immutable.Map[String,String] = Map(milvus.port -&gt; 19530, milvus.host -&gt; 52.13.11.185, milvus.token -&gt; db_admin:Sb3$BMsHpHuK$0Te, milvus.uri -&gt; https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, milvus.collectionName -&gt; hello_spark_milvus3)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import io.milvus.client.{MilvusClient, MilvusServiceClient}\nimport io.milvus.grpc.DataType\nimport io.milvus.param.collection.{CreateCollectionParam, FieldType}\nimport io.milvus.param.{ConnectParam, R, RpcStatus}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{SaveMode, SparkSession}\nimport org.slf4j.LoggerFactory\nimport zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\nimport java.util\nsparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2967c117\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5669e1a7\nhost: String = 52.13.11.185\nport: Int = 19530\ncollectionName: String = hello_spark_milvus3\nfilePath: String = /Volumes/zilliz_test/default/sample_vectors/row.json\nconnectParam: io.milvus.param.ConnectParam = ConnectParam(host=52.13.11.185, port=19530, databaseName=default, uri=null, token=null, connectTimeoutMs=10000, keepAliveTimeMs=55000, keepAliveTimeoutMs=20000, keepAliveWithoutCalls=false, rpcDeadlineMs=0, secure=false, idleTimeoutMs=86400000, authorization=cm9vdDpNaWx2dXM=, clientKeyPath=null, clientPemPath=null, caPemPath=null, serverPemPath=null, serverName=null)\nclient: io.milvus.client.MilvusClient = io.milvus.client.MilvusServiceClient@21e4f472\nfield1Name: String = id_field\nfield2Name: String = str_field\nfield3Name: String = float_vector_field\nfieldsSchema: java.util.List[io.milvus.param.collection.FieldType] = [FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}]\ncreateParam: io.milvus.param.collection.CreateCollectionParam = CreateCollectionParam(collectionName=hello_spark_milvus3, shardsNum=0, description=, fieldTypes=[FieldType{name='id_field', type='Int64', primaryKey=true, partitionKey=false, autoID=false, params={}}, FieldType{name='str_field', type='VarChar', primaryKey=false, partitionKey=false, autoID=false, params={max_length=65535}}, FieldType{name='float_vector_field', type='FloatVector', primaryKey=false, partitionKey=false, autoID=false, params={dim=8}}], partitionsNum=0, consistencyLevel=BOUNDED, databaseName=null, enableDynamicField=false)\ncreateR: io.milvus.param.R[io.milvus.param.RpcStatus] = R{status=0, data=RpcStatus{msg='Success'}}\ndf: org.apache.spark.sql.DataFrame = [id_field: int, str_field: string ... 1 more field]\nmilvusOptions: scala.collection.immutable.Map[String,String] = Map(milvus.port -&gt; 19530, milvus.host -&gt; 52.13.11.185, milvus.token -&gt; db_admin:Sb3$BMsHpHuK$0Te, milvus.uri -&gt; https://in01-4d0ef88234738f5.aws-us-west-2.vectordb.zillizcloud.com:19535, milvus.collectionName -&gt; hello_spark_milvus3)\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id_field",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "str_field",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "float_vector_field",
            "nullable": true,
            "type": {
             "containsNull": true,
             "elementType": "float",
             "type": "array"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
    "import io.milvus.grpc.DataType\n",
    "import io.milvus.param.collection.{CreateCollectionParam, FieldType}\n",
    "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import org.slf4j.LoggerFactory\n",
    "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
    "\n",
    "import java.util\n",
    "\n",
    "val sparkConf = new SparkConf().setMaster(\"local\")\n",
    "val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n",
    "// Fill in user's Milvus instance credentials.\n",
    "val host = \"52.13.11.185\"\n",
    "val port = 19530\n",
    "val username = \"root\"\n",
    "val password = \"Milvus\"\n",
    "// Specify the target Milvus collection name.\n",
    "val collectionName = \"hello_spark_milvus3\"\n",
    "// This file simulates a dataframe from user's vector generation job or a Delta table that contains vectors.\n",
    "val filePath = \"/Volumes/zilliz_test/default/sample_vectors/dim32_1k.json\"\n",
    "\n",
    "// 1. Create Milvus collection through Milvus SDK\n",
    "val connectParam: ConnectParam = ConnectParam.newBuilder\n",
    "  .withHost(host)\n",
    "  .withPort(port)\n",
    "  .withAuthorization(username, password)\n",
    "  .build\n",
    "\n",
    "val client: MilvusClient = new MilvusServiceClient(connectParam)\n",
    "\n",
    "val field1Name: String = \"id_field\"\n",
    "val field2Name: String = \"str_field\"\n",
    "val field3Name: String = \"float_vector_field\"\n",
    "val fieldsSchema: util.List[FieldType] = new util.ArrayList[FieldType]\n",
    "\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withPrimaryKey(true)\n",
    "  .withAutoID(false)\n",
    "  .withDataType(DataType.Int64)\n",
    "  .withName(field1Name)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.VarChar)\n",
    "  .withName(field2Name)\n",
    "  .withMaxLength(65535)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.FloatVector)\n",
    "  .withName(field3Name)\n",
    "  .withDimension(32)\n",
    "  .build\n",
    ")\n",
    "\n",
    "// create collection\n",
    "val createParam: CreateCollectionParam = CreateCollectionParam.newBuilder\n",
    "  .withCollectionName(collectionName)\n",
    "  .withFieldTypes(fieldsSchema)\n",
    "  .build\n",
    "\n",
    "val createR: R[RpcStatus] = client.createCollection(createParam)\n",
    "\n",
    "// log.info(s\"create collection ${collectionName} resp: ${createR.toString}\")\n",
    "\n",
    "// 2. Read data from file to build vector dataframe. The schema of the dataframe must logically match the schema of vector db.\n",
    "val df = spark.read\n",
    "  .schema(new StructType()\n",
    "    .add(field1Name, IntegerType)\n",
    "    .add(field2Name, StringType)\n",
    "    .add(field3Name, ArrayType(FloatType), false))\n",
    "  .json(filePath)\n",
    "\n",
    "// 3. Configure output target\n",
    "val milvusOptions = Map(\n",
    "  MILVUS_URI -> uri,\n",
    "  MILVUS_TOKEN -> token,\n",
    "  MILVUS_HOST -> host,\n",
    "  MILVUS_PORT -> port.toString,\n",
    "  MILVUS_COLLECTION_NAME -> collectionName,\n",
    ")\n",
    "\n",
    "// 4. Insert data to milvus collection\n",
    "df.write\n",
    "  .options(milvusOptions)\n",
    "  .format(\"milvus\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save()\n",
    "\n",
    "// flush data (The following implementation will insert the vector data row by row through Milvus SDK Insert API)\n",
    "val flushParam: FlushParam = FlushParam.newBuilder\n",
    "  .addCollectionName(collectionName)\n",
    "  .build\n",
    "val flushR: R[FlushResponse] = client.flush(flushParam)\n",
    "println(flushR)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "milvus-demo1-sink",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
