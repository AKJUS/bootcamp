{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c2ecb-6ea2-418d-b4e1-5f9c1dc18239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import io.milvus.client.{MilvusClient, MilvusServiceClient}\n",
    "import io.milvus.grpc.DataType\n",
    "import io.milvus.param.collection.{CreateCollectionParam, FieldType}\n",
    "import io.milvus.param.{ConnectParam, R, RpcStatus}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import org.slf4j.LoggerFactory\n",
    "import zilliztech.spark.milvus.MilvusOptions.{MILVUS_COLLECTION_NAME, MILVUS_HOST, MILVUS_PORT, MILVUS_TOKEN, MILVUS_URI}\n",
    "\n",
    "import java.util\n",
    "\n",
    "val sparkConf = new SparkConf().setMaster(\"local\")\n",
    "val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n",
    "// Fill in user's Milvus instance credentials.\n",
    "val host = \"127.0.0.1\"\n",
    "val port = 19530\n",
    "val username = \"root\"\n",
    "val password = \"Milvus\"\n",
    "// Specify the target Milvus collection name.\n",
    "val collectionName = \"hello_spark_milvus3\"\n",
    "// This file simulates a dataframe from user's vector generation job or a Delta table that contains vectors.\n",
    "val filePath = \"/Volumes/zilliz_test/default/sample_vectors/dim32_1k.json\"\n",
    "\n",
    "// 1. Create Milvus collection through Milvus SDK\n",
    "val connectParam: ConnectParam = ConnectParam.newBuilder\n",
    "  .withHost(host)\n",
    "  .withPort(port)\n",
    "  .withAuthorization(username, password)\n",
    "  .build\n",
    "\n",
    "val client: MilvusClient = new MilvusServiceClient(connectParam)\n",
    "\n",
    "val field1Name: String = \"id_field\"\n",
    "val field2Name: String = \"str_field\"\n",
    "val field3Name: String = \"float_vector_field\"\n",
    "val fieldsSchema: util.List[FieldType] = new util.ArrayList[FieldType]\n",
    "\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withPrimaryKey(true)\n",
    "  .withAutoID(false)\n",
    "  .withDataType(DataType.Int64)\n",
    "  .withName(field1Name)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.VarChar)\n",
    "  .withName(field2Name)\n",
    "  .withMaxLength(65535)\n",
    "  .build\n",
    ")\n",
    "fieldsSchema.add(FieldType.newBuilder\n",
    "  .withDataType(DataType.FloatVector)\n",
    "  .withName(field3Name)\n",
    "  .withDimension(32)\n",
    "  .build\n",
    ")\n",
    "\n",
    "// create collection\n",
    "val createParam: CreateCollectionParam = CreateCollectionParam.newBuilder\n",
    "  .withCollectionName(collectionName)\n",
    "  .withFieldTypes(fieldsSchema)\n",
    "  .build\n",
    "\n",
    "val createR: R[RpcStatus] = client.createCollection(createParam)\n",
    "\n",
    "// log.info(s\"create collection ${collectionName} resp: ${createR.toString}\")\n",
    "\n",
    "// 2. Read data from file to build vector dataframe. The schema of the dataframe must logically match the schema of vector db.\n",
    "val df = spark.read\n",
    "  .schema(new StructType()\n",
    "    .add(field1Name, IntegerType)\n",
    "    .add(field2Name, StringType)\n",
    "    .add(field3Name, ArrayType(FloatType), false))\n",
    "  .json(filePath)\n",
    "\n",
    "// 3. Configure output target\n",
    "val milvusOptions = Map(\n",
    "  MILVUS_URI -> uri,\n",
    "  MILVUS_TOKEN -> token,\n",
    "  MILVUS_HOST -> host,\n",
    "  MILVUS_PORT -> port.toString,\n",
    "  MILVUS_COLLECTION_NAME -> collectionName,\n",
    ")\n",
    "\n",
    "// 4. Insert data to milvus collection\n",
    "df.write\n",
    "  .options(milvusOptions)\n",
    "  .format(\"milvus\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save()\n",
    "\n",
    "// flush data (The following implementation will insert the vector data row by row through Milvus SDK Insert API)\n",
    "val flushParam: FlushParam = FlushParam.newBuilder\n",
    "  .addCollectionName(collectionName)\n",
    "  .build\n",
    "val flushR: R[FlushResponse] = client.flush(flushParam)\n",
    "println(flushR)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "milvus-demo1-sink",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
