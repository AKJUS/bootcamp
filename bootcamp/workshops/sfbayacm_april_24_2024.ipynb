{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c3444",
   "metadata": {},
   "source": [
    "# ReadtheDocs Retrieval Augmented Generation (RAG) using Zilliz Free Tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd11a",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use Milvus documentation pages to create a chatbot about our product.  The chatbot is going to follow RAG steps to retrieve chunks of data using Semantic Vector Search, then the Question + Context will be fed as a Prompt to a LLM to generate an answer.\n",
    "\n",
    "Many RAG demos use OpenAI for the Embedding Model and ChatGPT for the Generative AI model.  **In this notebook, we will demo a fully open source RAG stack.**\n",
    "\n",
    "Using open-source Q&A with retrieval saves money since we make free calls to our own data almost all the time - retrieval, evaluation, and development iterations.  We only make a paid call to OpenAI once for the final chat generation step. \n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/rag_image.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2509fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab install these libraries in this order:\n",
    "!python -m pip install torch transformers sentence-transformers langchain\n",
    "# !python -m pip install pymilvus 'pymilvus[model]'\n",
    "# !python -m pip install unstructured openai tqdm numpy ipykernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7570b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries.\n",
    "import sys, os, time, pprint\n",
    "import numpy as np\n",
    "\n",
    "# Import custom functions for splitting and search.\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "import milvus_utilities as _utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059b674",
   "metadata": {},
   "source": [
    "## Download Milvus documentation.\n",
    "\n",
    "The data we‚Äôll use is our own product documentation web pages.  ReadTheDocs is an open-source free software documentation hosting platform, where documentation is written with the Sphinx document generator.\n",
    "\n",
    "The code block below downloads the web pages into a local directory called `rtdocs`.  \n",
    "\n",
    "I've already uploaded the `rtdocs` data folder to github, so you should see it if you cloned my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25686cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO DOWNLOAD THE DOCS.\n",
    "\n",
    "# # !pip install -U langchain\n",
    "# from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# DOCS_PAGE=\"https://milvus.io/docs/\"\n",
    "\n",
    "# loader = RecursiveUrlLoader(DOCS_PAGE)\n",
    "# docs = loader.load()\n",
    "\n",
    "# num_documents = len(docs)\n",
    "# print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b232dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO READ THE DOCS FROM A LOCAL DIRECTORY.\n",
    "\n",
    "# Read docs into LangChain\n",
    "# !pip install -U langchain\n",
    "# !pip install unstructured\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load HTML files from a local directory\n",
    "path = \"rtdocs/\"\n",
    "loader = DirectoryLoader(path, glob='*.html')\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb844837",
   "metadata": {},
   "source": [
    "## Start up a Zilliz free tier cluster.\n",
    "\n",
    "Code in this notebook uses fully-managed Milvus on [Ziliz Cloud free trial](https://cloud.zilliz.com/login).  \n",
    "  1. Choose the default \"Starter\" option when you provision > Create collection > Give it a name > Create cluster and collection.  \n",
    "  2. On the Cluster main page, copy your `API Key` and store it locally in a .env variable.  See note below how to do that.\n",
    "  3. Also on the Cluster main page, copy the `Public Endpoint URI`.\n",
    "\n",
    "üí° Note: To keep your tokens private, best practice is to use an **env variable**.  See [how to save api key in env variable](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety). <br>\n",
    "\n",
    "üëâüèº In Jupyter, you need a .env file (in same dir as notebooks) containing lines like this:\n",
    "- ZILLIZ_API_KEY=f370c...\n",
    "- OPENAI_API_KEY=sk-H...\n",
    "- VARIABLE_NAME=value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1. CONNECT TO ZILLIZ CLOUD\n",
    "\n",
    "# !pip install pymilvus==2.3.7 #python sdk for milvus\n",
    "import os\n",
    "import pymilvus\n",
    "print(f\"pymilvus version: {pymilvus.__version__}\")\n",
    "from pymilvus import connections, utility\n",
    "TOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n",
    "\n",
    "# Connect to Zilliz cloud using endpoint URI and API key TOKEN.\n",
    "# TODO change this.\n",
    "CLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "CLUSTER_ENDPOINT=\"https://in03-48a5b11fae525c9.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "connections.connect(\n",
    "  alias='default',\n",
    "  #  Public endpoint obtained from Zilliz Cloud\n",
    "  uri=CLUSTER_ENDPOINT,\n",
    "  # API key or a colon-separated cluster username and password\n",
    "  token=TOKEN,\n",
    ")\n",
    "\n",
    "# Check if the server is ready and get colleciton name.\n",
    "print(f\"Type of server: {utility.get_server_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d6622",
   "metadata": {},
   "source": [
    "## Load the Embedding Model checkpoint and use it to create vector embeddings\n",
    "**Embedding model:**  We will use the open-source [sentence transformers](https://www.sbert.net/docs/pretrained_models.html) available on HuggingFace to encode the documentation text.  We will download the model from HuggingFace and run it locally. \n",
    "\n",
    "üí°Tip:  A good way to choose a sentence transformer model is to check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).  Sort descending by column \"Retrieval Average\" and choose the best-performing small model.\n",
    "\n",
    "Two model parameters of note below:\n",
    "1. EMBEDDING_DIM refers to the dimensionality or length of the embedding vector. In this case, the embeddings generated for EACH token in the input text will have the SAME length = 1024. This size of embedding is often associated with BERT-based models, where the embeddings are used for downstream tasks such as classification, question answering, or text generation. <br><br>\n",
    "2. MAX_SEQ_LENGTH is the maximum Context Length the encoder model can handle for input sequences. In this case, if sequences longer than 512 tokens are given to the model, everything longer will be (silently!) chopped off.  This is the reason why a chunking strategy is needed to segment input texts into chunks with lengths that will fit in the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2be7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2. DOWNLOAD AN OPEN SOURCE EMBEDDING MODEL.\n",
    "\n",
    "# Import torch.\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize torch settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# Load the model from huggingface model hub.\n",
    "# python -m pip install -U angle-emb\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "print(type(encoder))\n",
    "print(encoder)\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length() \n",
    "# # Assume tokens are 3 characters long.\n",
    "# MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "# HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "# Test with 512 sequence length.\n",
    "MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "# Inspect model parameters.\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "print(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus collection\n",
    "\n",
    "You can think of a collection in Milvus like a \"table\" in SQL databases.  The **collection** will contain the \n",
    "- **Schema** (or [no-schema Milvus client](https://milvus.io/docs/using_milvusclient.md)).  \n",
    "üí° You'll need the vector `EMBEDDING_DIM` parameter from your embedding model.\n",
    "Typical values are:\n",
    "   - 1024 for sbert embedding models\n",
    "   - 1536 for ada-002 OpenAI embedding models\n",
    "- **Vector index** for efficient vector search\n",
    "- **Vector distance metric** for measuring nearest neighbor vectors\n",
    "- **Consistency level**\n",
    "In Milvus, transactional consistency is possible; however, according to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), some latency must be sacrificed. üí° Searching movie reviews is not mission-critical, so [`eventually`](https://milvus.io/docs/consistency.md) consistent is fine here.\n",
    "\n",
    "## Add a Vector Index\n",
    "\n",
    "The vector index determines the vector **search algorithm** used to find the closest vectors in your data to the query a user submits.  \n",
    "\n",
    "Most vector indexes use different sets of parameters depending on whether the database is:\n",
    "- **inserting vectors** (creation mode) - vs - \n",
    "- **searching vectors** (search mode) \n",
    "\n",
    "Scroll down the [docs page](https://milvus.io/docs/index.md) to see a table listing different vector indexes available on Milvus.  For example:\n",
    "- FLAT - deterministic exhaustive search\n",
    "- IVF_FLAT or IVF_SQ8 - Hash index (stochastic approximate search)\n",
    "- HNSW - Graph index (stochastic approximate search)\n",
    "- AUTOINDEX - Automatically determined based on OSS vs [Zilliz cloud](https://docs.zilliz.com/docs/autoindex-explained), type of GPU, size of data.\n",
    "\n",
    "Besides a search algorithm, we also need to specify a **distance metric**, that is, a definition of what is considered \"close\" in vector space.  In the cell below, the [`HNSW`](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) search index is chosen.  Its possible distance metrics are one of:\n",
    "- L2 - L2-norm\n",
    "- IP - Dot-product\n",
    "- COSINE - Angular distance\n",
    "\n",
    "üí° Most use cases work better with normalized embeddings, in which case L2 is useless (every vector has length=1) and IP and COSINE are the same.  Only choose L2 if you plan to keep your embeddings unnormalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3. CREATE A NO-SCHEMA MILVUS COLLECTION AND DEFINE THE DATABASE INDEX.\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "# Set the Milvus collection name.\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "\n",
    "# Add custom HNSW search index to the collection.\n",
    "# M = max number graph connections per layer. Large M = denser graph.\n",
    "# Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "M = 16\n",
    "# efConstruction = num_candidate_nearest_neighbors per layer. \n",
    "# Use Rule of thumb: int. 8~512, efConstruction = M * 2.\n",
    "efConstruction = M * 2\n",
    "# Create the search index for local Milvus server.\n",
    "INDEX_PARAMS = dict({\n",
    "    'M': M,               \n",
    "    \"efConstruction\": efConstruction })\n",
    "index_params = {\n",
    "    \"index_type\": \"HNSW\", \n",
    "    \"metric_type\": \"COSINE\", \n",
    "    \"params\": INDEX_PARAMS\n",
    "    }\n",
    "\n",
    "# Use no-schema Milvus client uses flexible json key:value format.\n",
    "# https://milvus.io/docs/using_milvusclient.md\n",
    "mc = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    # API key or a colon-separated cluster username and password\n",
    "    token=TOKEN)\n",
    "\n",
    "# Check if collection already exists, if so drop it.\n",
    "has = utility.has_collection(COLLECTION_NAME)\n",
    "if has:\n",
    "    drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# Create the collection.\n",
    "mc.create_collection(COLLECTION_NAME, \n",
    "                     EMBEDDING_DIM,\n",
    "                     consistency_level=\"Eventually\", \n",
    "                     auto_id=True,  \n",
    "                     overwrite=True,\n",
    "                     # skip setting params below, if using AUTOINDEX\n",
    "                     params=index_params\n",
    "                    )\n",
    "\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")\n",
    "print(mc.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60423a5",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  In this demo, I will use:\n",
    "- **Strategy** = Use markdown header hierarchies.  Keep markdown sections together unless they are too long.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `HTMLHeaderTextSplitter` to split markdown sections.\n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively.\n",
    "\n",
    "\n",
    "Notice below, each chunk is grounded with the document source page.  <br>\n",
    "In addition, header titles are kept together with the chunk of markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STEP 4. PREPARE DATA: CHUNK AND EMBED\n",
    "\n",
    "# !python -m pip install lxml\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the headers to split on for the HTMLHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "# Create an instance of the HTMLHeaderTextSplitter\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Specify chunk size and overlap.\n",
    "# chunk_size = MAX_SEQ_LENGTH - HF_EOS_TOKEN_LENGTH\n",
    "chunk_size = 512\n",
    "chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "print(f\"chunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}\")\n",
    "\n",
    "# Create an instance of the RecursiveCharacterTextSplitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# Split the HTML text using the HTMLHeaderTextSplitter.\n",
    "start_time = time.time()\n",
    "html_header_splits = []\n",
    "for doc in docs:\n",
    "    splits = html_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        # Add the source URL and header values to the metadata\n",
    "        metadata = {}\n",
    "        new_text = split.page_content\n",
    "        for header_name, metadata_header_name in headers_to_split_on:\n",
    "            # Handle exception if h1 does not exist.\n",
    "            try:\n",
    "                header_value = new_text.split(\"¬∂ \")[0].strip()[:100]\n",
    "                metadata[header_name] = header_value\n",
    "            except:\n",
    "                break\n",
    "            # Handle exception if h2 does not exist.\n",
    "            try:\n",
    "                new_text = new_text.split(\"¬∂ \")[1].strip()[:50]\n",
    "            except:\n",
    "                break\n",
    "        split.metadata = {\n",
    "            **metadata,\n",
    "            \"source\": doc.metadata[\"source\"]\n",
    "        }\n",
    "        # Add the header to the text\n",
    "        split.page_content = split.page_content\n",
    "    html_header_splits.extend(splits)\n",
    "\n",
    "    # # TODO - Uncomment to save each doc.page_content as a file under OUTPUT_DIR.\n",
    "    # OUTPUT_DIR = \"output\"\n",
    "    # # Set filename to first 50 characters of h1 header.\n",
    "    # filename = doc.metadata[\"source\"].split(\"/\")[-1].split(\".\")[0][:50]\n",
    "    # with open(f\"{OUTPUT_DIR}/{filename}.html\", \"w\") as f:\n",
    "    #     f.write(doc.page_content)\n",
    "\n",
    "# Split the documents further into smaller, recursive chunks.\n",
    "chunks = child_splitter.split_documents(html_header_splits)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "print(f\"docs: {len(docs)}, split into: {len(html_header_splits)}\")\n",
    "print(f\"split into chunks: {len(chunks)}, type: list of {type(chunks[0])}\") \n",
    "\n",
    "# Inspect a chunk.\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)\n",
    "\n",
    "# # TODO - Uncomment to print child splits with their associated header metadata.\n",
    "# print()\n",
    "# for child in chunks:\n",
    "#     print(f\"Content: {child.page_content}\")\n",
    "#     print(f\"Metadata: {child.metadata}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512130a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the metadata urls\n",
    "for doc in chunks:\n",
    "    new_url = doc.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"rtdocs\", \"https:/\")\n",
    "    doc.metadata.update({\"source\": new_url})\n",
    "\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd8153",
   "metadata": {},
   "source": [
    "## Insert data into Milvus\n",
    "\n",
    "For each original text chunk, we'll write the quadruplet (`vector, text, source, h1, h2`) into the database.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/db_insert.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "**The Milvus Client wrapper can only handle loading data from a list of dictionaries.**\n",
    "\n",
    "Otherwise, in general, Milvus supports loading data from:\n",
    "- pandas dataframes \n",
    "- list of dictionaries\n",
    "\n",
    "Below, we use the embedding model provided by HuggingFace, download its checkpoint, and run it locally as the encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5. INSERT CHUNKS AND EMBEDDINGS IN ZILLIZ.\n",
    "\n",
    "# Convert chunks to a list of dictionaries.\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "\n",
    "    # Generate embeddings using encoder from HuggingFace.\n",
    "    embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "    # embeddings = F.normalize(embeddings, p=2, dim=1) #use torch\n",
    "    embeddings = np.array(embeddings / np.linalg.norm(embeddings)) #use numpy\n",
    "    converted_values = list(map(np.float32, embeddings))[0]\n",
    "    \n",
    "    # Only use h1, h2. Truncate the metadata in case too long.\n",
    "    try:\n",
    "        h2 = chunk.metadata['h2'][:50]\n",
    "    except:\n",
    "        h2 = \"\"\n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    chunk_dict = {\n",
    "        'vector': converted_values,\n",
    "        'chunk': chunk.page_content,\n",
    "        'source': chunk.metadata['source'],\n",
    "        'h1': chunk.metadata['h1'][:50],\n",
    "        'h2': h2,\n",
    "    }\n",
    "    chunk_list.append(chunk_dict)\n",
    "\n",
    "# Insert data into the Milvus collection.\n",
    "print(\"Start inserting entities\")\n",
    "start_time = time.time()\n",
    "insert_result = mc.insert(\n",
    "    COLLECTION_NAME,\n",
    "    data=chunk_list,\n",
    "    progress_bar=True)\n",
    "end_time = time.time()\n",
    "print(f\"Milvus Client insert time for {len(chunk_list)} vectors: {end_time - start_time} seconds\")\n",
    "\n",
    "# Milvus Client insert time for 156 vectors: 1.283660888671875 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO - Uncomment to print.\n",
    "# chunk_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa628f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PyMilvus utility API calls.\n",
    "\n",
    "# # Count rows, incurs a call to .flush() first.\n",
    "# This API call is not supported by Milvus Client.\n",
    "# print(f\"Count rows: {mc.num_entities(COLLECTION_NAME)}\")\n",
    "\n",
    "# View collection info, incurs a call to .flush() first.\n",
    "start_time = time.time()\n",
    "pprint.pprint(mc.describe_collection(COLLECTION_NAME))\n",
    "end_time = time.time()\n",
    "print(f\"timing: {end_time - start_time} seconds\")\n",
    "print()\n",
    "\n",
    "# Count rows without incurring call to .flush().\n",
    "start_time = time.time()\n",
    "res = mc.query( collection_name=COLLECTION_NAME, \n",
    "               filter=\"\", \n",
    "               output_fields = [\"count(*)\"], )\n",
    "pprint.pprint(res)\n",
    "end_time = time.time()\n",
    "print(f\"timing: {end_time - start_time} seconds\")\n",
    "\n",
    "# View rows without incurring call to .flush().\n",
    "OUTPUT_FIELDS = [\"id\", \"h1\", \"h2\", \"source\", \"chunk\"]\n",
    "res = mc.query( collection_name=COLLECTION_NAME, \n",
    "               filter=\"id >= 0\", \n",
    "               output_fields = OUTPUT_FIELDS, )\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c589ff",
   "metadata": {},
   "source": [
    "## Ask a question about your data\n",
    "\n",
    "So far in this demo notebook: \n",
    "1. Your custom data has been mapped into a vector embedding space\n",
    "2. Those vector embeddings have been saved into a vector database\n",
    "\n",
    "Next, you can ask a question about your custom data!\n",
    "\n",
    "üí° In LLM vocabulary:\n",
    "> **Query** is the generic term for user questions.  \n",
    "A query is a list of multiple individual questions, up to maybe 1000 different questions!\n",
    "\n",
    "> **Question** usually refers to a single user question.  \n",
    "In our example below, the user question is \"What is AUTOINDEX in Milvus Client?\"\n",
    "\n",
    "> **Semantic Search** = very fast search of the entire knowledge base to find the `TOP_K` documentation chunks with the closest embeddings to the user's query.\n",
    "\n",
    "üí° The same model should always be used for consistency for all the embeddings data and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample question about your data.\n",
    "QUESTION1 = \"What do the parameters for HNSW mean?\"\n",
    "QUESTION2 = \"What are good default values for HNSW parameters with 25K vectors dim 1024?\"\n",
    "QUESTION3 = \"What is the default AUTOINDEX distance metric in Milvus Client?\"\n",
    "QUERY = [QUESTION1, QUESTION2, QUESTION3]\n",
    "\n",
    "# Inspect the length of the query.\n",
    "QUERY_LENGTH = len(QUESTION2)\n",
    "print(f\"query length: {QUERY_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT A PARTICULAR QUESTION TO ASK.\n",
    "\n",
    "SAMPLE_QUESTION = QUESTION1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea29411",
   "metadata": {},
   "source": [
    "## Execute a vector search\n",
    "\n",
    "Search Milvus using [PyMilvus API](https://milvus.io/docs/search.md).\n",
    "\n",
    "üí° By their nature, vector searches are \"semantic\" searches.  For example, if you were to search for \"leaky faucet\": \n",
    "> **Traditional Key-word Search** - either or both words \"leaky\", \"faucet\" would have to match some text in order to return a web page or link text to the document.\n",
    "\n",
    "> **Semantic search** - results containing words \"drippy\" \"taps\" would be returned as well because these words mean the same thing even though they are different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89642119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVAL USING MILVUS API.\n",
    "\n",
    "# # Not needed with Milvus Client API.\n",
    "# mc.load()\n",
    "\n",
    "# Embed the question using the same encoder.\n",
    "query_embeddings = _utils.embed_query(encoder, [SAMPLE_QUESTION])\n",
    "TOP_K = 2\n",
    "\n",
    "# Return top k results with HNSW index.\n",
    "SEARCH_PARAMS = dict({\n",
    "    # Re-use index param for num_candidate_nearest_neighbors.\n",
    "    \"ef\": INDEX_PARAMS['efConstruction']\n",
    "    })\n",
    "\n",
    "# Define output fields to return.\n",
    "OUTPUT_FIELDS = [\"id\", \"h1\", \"h2\", \"source\", \"chunk\"]\n",
    "\n",
    "# Run semantic vector search using your query and the vector database.\n",
    "start_time = time.time()\n",
    "results = mc.search(\n",
    "    COLLECTION_NAME,\n",
    "    data=query_embeddings, \n",
    "    search_params=SEARCH_PARAMS,\n",
    "    output_fields=OUTPUT_FIELDS, \n",
    "    # Milvus can utilize metadata in boolean expressions to filter search.\n",
    "    # filter=\"id >= 0 && source == 'https://pymilvus.readthedocs.io/en/latest/param.html'\",\n",
    "    limit=TOP_K,\n",
    "    consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus Client search time for {len(chunk_list)} vectors: {elapsed_time} seconds\")\n",
    "\n",
    "# Inspect search result.\n",
    "print(f\"type: {type(results[0])}, count: {len(results[0])}\")\n",
    "\n",
    "# Milvus Client search time for 156 vectors: 0.1264362335205078 seconds\n",
    "# type: <class 'list'>, count: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble and inspect the search result\n",
    "\n",
    "The search result is in the variable `results[0]` of type `'pymilvus.orm.search.SearchResult'`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble retrieved context and context metadata.\n",
    "METADATA_FIELDS = [f for f in OUTPUT_FIELDS if f != 'chunk']\n",
    "formatted_results, context, context_metadata = _utils.client_assemble_retrieved_context(\n",
    "    results, metadata_fields=METADATA_FIELDS, num_shot_answers=3)\n",
    "print(f\"Length context: {len(context[0])}, Number of contexts: {len(context)}\")\n",
    "\n",
    "# TODO - Uncomment to loop through each context and metadata and print.\n",
    "for i in range(len(context)):\n",
    "    print(f\"Retrieved result #{i+1}\")\n",
    "    print(f\"distance = {formatted_results[i][0]}\")\n",
    "    print(f\"Context: {context[i][:150]}\")\n",
    "    print(f\"Metadata: {context_metadata[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be0931",
   "metadata": {},
   "source": [
    "# Add Memory\n",
    "\n",
    "- [LangMem](https://langchain-ai.github.io/long-term-memory/#langmem)\n",
    "- [Example notebook](https://langchain-ai.github.io/long-term-memory/quick_start/#3-query-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e341e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --quiet langmem openai\n",
    "LANGMEM_API_URL = os.getenv(\"LANGMEM_API_URL\")\n",
    "LANGMEM_API_KEY = os.getenv(\"LANGMEM_API_KEY\")\n",
    "LANGMEM_API_URL='https://long-term-memory-shared-for-f31898bf7c4572600f7ef-vz4y4ooboq-uc.a.run.app'\n",
    "OPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "import openai\n",
    "from langmem import AsyncClient, Client\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Connect to OpenAI and to Langmem.\n",
    "oai_client = openai.AsyncClient()\n",
    "langmem_client = AsyncClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Append State\n",
    "# As the name suggests, the user_append_state is an append-only state (meaning the profile is never overwritten) that \n",
    "# lets you define schema(s) to represent individual memories which you can later query semantically.\n",
    "\n",
    "class CoreBelief(BaseModel):\n",
    "    belief: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"The belief the user has about the world, themselves, or anything else.\",\n",
    "    )\n",
    "    why: str = Field(description=\"Why the user believes this.\")\n",
    "    context: str = Field(\n",
    "        description=\"The raw context from the conversation that leads you to conclude that the user believes this.\"\n",
    "    )\n",
    "\n",
    "\n",
    "belief_function = await langmem_client.create_memory_function(\n",
    "    CoreBelief, target_type=\"user_append_state\"\n",
    ")\n",
    "\n",
    "\n",
    "class FormativeEvent(BaseModel):\n",
    "    event: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"The event that occurred. Must be important enough to be formative for the user.\",\n",
    "    )\n",
    "    impact: str = Field(default=\"\", description=\"How this event influenced the user.\")\n",
    "\n",
    "\n",
    "event_function = await langmem_client.create_memory_function(\n",
    "    FormativeEvent, target_type=\"user_append_state\"\n",
    ")\n",
    "\n",
    "# class UserPreference:\n",
    "#    preference: str = Field(description=\"The preference.\")\n",
    "#    why: str = Field(description=\"Why the user prefers this.\")\n",
    "#    context: str = Field(description=\"The context in which this prefernce is appropriate\")\n",
    "#    source_comment: str = Field(description=\"The raw user utterance where you identified this preference.\")\n",
    "\n",
    "# client = Client()\n",
    "# memory_function = await client.create_memory_function(\n",
    "#    UserPreference,\n",
    "#    target_type=\"user_append_state\",\n",
    "#    custom_instructions=\"Extract as many preferences from the conversation as you are able.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c05ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Starting a conversation\n",
    "# Memories are formed whenever your chat bot posts messages to the service.\n",
    "\n",
    "# Whenever a a user ID is provided in the message metadata, LangMem will \n",
    "# automatically create a new user entry and start tracking memories for that user.\n",
    "\n",
    "import uuid\n",
    "\n",
    "johnny_user_id = uuid.uuid4()\n",
    "jimmy_user_id = uuid.uuid4()\n",
    "jimmy_username = f\"jimmy-{uuid.uuid4().hex[:4]}\"\n",
    "johnny_username = f\"johnny-{uuid.uuid4().hex[:4]}\"\n",
    "\n",
    "print(f\"johnny_user_id: {johnny_user_id}\")\n",
    "print(f\"jimmy_user_id: {jimmy_user_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is an example conversation between 1 or more users and an AI\n",
    "\n",
    "# Unique for a given converstaion\n",
    "thread_id = uuid.uuid4()\n",
    "print(f\"thread_id: {thread_id}\")\n",
    "\n",
    "\n",
    "async def completion(messages: list):\n",
    "    stripped_messages = [\n",
    "        {k: v for k, v in m.items() if k != \"metadata\"} for m in messages\n",
    "    ]\n",
    "    return await oai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=stripped_messages\n",
    "    )\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        # Names are optional but should be consistent with a given user id, if provided\n",
    "        \"name\": jimmy_username,\n",
    "        \"content\": \"Hey johnny have i ever told you about my older bro steve?\",\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"no, you didn't, but I think he was friends with my younger sister sueann\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"yeah me and him used to play stickball down in the park before school started. I think it was in 1980\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"That was totally 1979! I remember because i was stuck at home all summer.\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": \"Jeanne\",\n",
    "        # If the user ID isn't provided, we treat this as a guest message and won't assign memories to the user\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"That was so long ago. I have gotten old and gained 200 pounds since then. I can't even remember who was president. @ai, who was the president in 1980?\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"The president of the United States in 1980 was Jimmy Carter.\",\n",
    "        \"role\": \"assistant\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Wow ya i forgot that. Stickleball really impacted my life. It's how i first met Jeanne! wonder how my life would have turned out if it hadn't happened that way.\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Yeah wow. That was a big year! @ai could you remind me what else was going on that year?\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get the completion Next-Reply in this conversation chain.\n",
    "# Q. How does it know the it's the role of the assistant?\n",
    "result = await completion(messages)\n",
    "messages.append(result.choices[0].message)\n",
    "pprint.pprint(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the messages, we can share them with LangMem.\n",
    "await langmem_client.add_messages(thread_id=thread_id, messages=messages)\n",
    "\n",
    "# LangMem will automatically process memories after some delay (~60 seconds), \n",
    "# but we can eagerly process the memories as well.\n",
    "await langmem_client.trigger_all_for_thread(thread_id=thread_id)\n",
    "# You could also trigger for a single user if you'd like\n",
    "# await langmem_client.trigger_all_for_user(user_id=jimmy_user_id)\n",
    "\n",
    "# You can fetch all the messages in a LangMem thread through that thread's \n",
    "# # GET endpoint. In this way, LangMem can act as a generic chat bot backend.\n",
    "# messages = langmem_client.list_messages(thread_id=thread_id)\n",
    "# # async for message in messages:\n",
    "# #     print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Query Memory\n",
    "# You can also query the user memory, once it's updated. This may take a few moments - please be patient üòä\n",
    "# To query the unstructured semantic memory, you can provide query text and the number of memories to return.\n",
    "\n",
    "SAMPLE_USER = jimmy_user_id\n",
    "\n",
    "# Wait a few moments for the memories to process. If this is empty, you'll likely have to wait a bit longer\n",
    "mems = None\n",
    "while not mems:\n",
    "    mem_response = await langmem_client.query_user_memory(\n",
    "        user_id=SAMPLE_USER, \n",
    "        text=\"stickleball\", \n",
    "        k=3\n",
    "    )\n",
    "    mems = mem_response[\"memories\"]\n",
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f03601",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_USER = johnny_user_id\n",
    "\n",
    "# Wait a few moments for the memories to process. If this is empty, you'll likely have to wait a bit longer\n",
    "mems = None\n",
    "while not mems:\n",
    "    mem_response = await langmem_client.query_user_memory(\n",
    "        user_id=SAMPLE_USER, \n",
    "        text=\"stickleball\", \n",
    "        k=3\n",
    "    )\n",
    "    mems = mem_response[\"memories\"]\n",
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a similar way, you can include\n",
    "# different `user_append_state` memory results\n",
    "# in the ranked response\n",
    "mems = await langmem_client.query_user_memory(\n",
    "    user_id=SAMPLE_USER,\n",
    "    text=\"stickleball\",\n",
    "    k=3,\n",
    "    memory_function_ids=[belief_function[\"id\"], event_function[\"id\"]],\n",
    ")\n",
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE:  This only works if you defined Thread memory.\n",
    "\n",
    "# # You can list all the thread summary memories for a given thread as well!\n",
    "# await langmem_client.list_thread_memory(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use in a later conversations\n",
    "# As you can see, we've extracted some useful information from the previous conversation. \n",
    "# We imagine you would fetch these facts in later conversations to provide your bot with \n",
    "# additional helpful context about the user.\n",
    "\n",
    "async def completion_with_memory(messages: list, user_id: uuid.UUID):\n",
    "    memories = await langmem_client.query_user_memory(\n",
    "        user_id=user_id, text=messages[-1][\"content\"], k=3\n",
    "    )\n",
    "    facts = \"\\n\".join([mem[\"text\"] for mem in memories[\"memories\"]])\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a helpful assistant. You know the following facts about the user with which you are conversing.\\n\\n{facts}\",\n",
    "    }\n",
    "    return await completion([system_prompt] + messages)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"content\": \"Hi there! I'm curious what you remember. What's my brother's name?\",\n",
    "        \"metadata\": {\"user_id\": jimmy_user_id},\n",
    "    }\n",
    "]\n",
    "res = await completion_with_memory(messages, user_id=jimmy_user_id)\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cleanup\n",
    "\n",
    "# functions = langmem_client.list_memory_functions()\n",
    "\n",
    "# async for func in functions:\n",
    "#     if func[\"type\"] == \"user_semantic_memory\":\n",
    "#         continue\n",
    "#     await langmem_client.delete_memory_function(func[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6060ce",
   "metadata": {},
   "source": [
    "## Use an LLM to Generate a chat response to the user's question using the Retrieved Context.\n",
    "\n",
    "Below, we'll use an open, very tiny generative AI model, or LLM, available on HuggingFace.  Many demos use OpenAI as the LLM choice instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate all the context together by space.\n",
    "contexts_combined = ' '.join(context)\n",
    "# Separate all the sources together by comma.\n",
    "source_values = [item['source'] for item in context_metadata]\n",
    "source_combined = ', '.join(source_values)\n",
    "print(f\"Length long text to summarize: {len(contexts_combined)}\")\n",
    "\n",
    "# Define the question and context\n",
    "no_context = \"The quick brown fox jumped over the lazy dog.\"\n",
    "no_prompt = f\"{no_context}\" + \"\\n\" + f\"{SAMPLE_QUESTION}\"\n",
    "full_prompt_baseline = \"\\<human>\\: \" + no_prompt + \"\\n\" + \"\\<bot>\\:\"\n",
    "\n",
    "my_prompt = f\"{contexts_combined}\" + \"\\n\" + f\"{SAMPLE_QUESTION}\"\n",
    "full_prompt = \"\\<human>\\: \" + my_prompt + \"\\n\" + \"\\<bot>\\:\"\n",
    "\n",
    "# pprint.pprint(full_prompt_baseline)\n",
    "# pprint.pprint(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINING THE LLM: ASK A QUESTION WITHOUT ANY RETRIEVED CONTEXT.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Using sheared-llama from LLMWare.\n",
    "# https://huggingface.co/llmware/bling-sheared-llama-1.3b-0.1\n",
    "# Load the model and tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/bling-sheared-llama-1.3b-0.1\")  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"llmware/bling-sheared-llama-1.3b-0.1\")\n",
    "\n",
    "# Encode the inputs for question-answering.\n",
    "inputs = tokenizer(full_prompt_baseline, return_tensors=\"pt\")  \n",
    "start_of_output = len(inputs.input_ids[0])\n",
    "\n",
    "# Generate the answer using the model\n",
    "outputs = model.generate(\n",
    "        inputs.input_ids.to(DEVICE),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        max_new_tokens=100,\n",
    "        )\n",
    "output_only = tokenizer.decode(outputs[0][start_of_output:],skip_special_tokens=True)  \n",
    "\n",
    "# Post-processing due to fine-tuning artifacts. \n",
    "eot1 = output_only.find(\"–•—Ä–æ–Ω–æ\")\n",
    "eot2 = output_only.find(\"textt\")\n",
    "# print(eot1, eot2)\n",
    "eot_index = -1\n",
    "eot_index = min(i for i in [eot1, eot2] if i >= 0)\n",
    "# print(eot_index)\n",
    "if eot_index > -1:\n",
    "    answer = output_only[:eot_index]\n",
    "else:\n",
    "    answer = output_only\n",
    "\n",
    "# Print the generated answer\n",
    "pprint.pprint(answer)\n",
    "\n",
    "# Not a good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff76685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING THE SAME LLM: ASK THE SAME QUESTION WITH RETRIEVED CONTEXT.\n",
    "\n",
    "# Encode the inputs for question-answering.\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\")  \n",
    "start_of_output = len(inputs.input_ids[0])\n",
    "\n",
    "# Generate the answer using the model\n",
    "outputs = model.generate(\n",
    "        inputs.input_ids.to(DEVICE),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        max_new_tokens=150,\n",
    "        )\n",
    "output_only = tokenizer.decode(outputs[0][start_of_output:],skip_special_tokens=True)  \n",
    "\n",
    "# Post-processing due to fine-tuning artifacts. \n",
    "eot1 = output_only.find(\"<textt\")\n",
    "eot2 = output_only.find(\"–•—Ä–æ–Ω–æ\")\n",
    "eot3 = output_only.find(\"<|endoftext\")\n",
    "# print(eot1, eot2, eot3)\n",
    "eot_index = -1\n",
    "eot_index = min(i for i in [eot1, eot2, eot3] if i >= 0)\n",
    "# print(eot_index)\n",
    "if eot_index > -1:\n",
    "    answer = output_only[:eot_index-1]\n",
    "else:\n",
    "    answer = output_only\n",
    "\n",
    "# Print the generated answer\n",
    "pprint.pprint(answer)\n",
    "\n",
    "# Better answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to generate a more human-like chat response to the user's question \n",
    "\n",
    "We've practiced retrieval for free on our own data using open-source LLMs.  <br>\n",
    "\n",
    "Now let's make a call to the paid OpenAI GPT.\n",
    "\n",
    "üí° Note: For use cases that need to always be factually grounded, use very low temperature values while more creative tasks can benefit from higher temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613686a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"Use the Context below to answer the user's question. \n",
    "Be clear, factual, complete, concise.\n",
    "If the answer is not in the Context, say \"I don't know\". \n",
    "Otherwise answer with fewer than 4 sentences and cite the grounding sources.\n",
    "Context: {contexts_combined}\n",
    "Answer: The answer to the question.\n",
    "Grounding sources: {source_combined}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL!! THIS COSTS MONEY!!\n",
    "import openai, pprint\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define the generation llm model to use.\n",
    "# https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "# Customers using the pinned gpt-3.5-turbo model alias will be automatically upgraded to gpt-3.5-turbo-0125 two weeks after this model launches.\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0.1\n",
    "RANDOM_SEED = 415\n",
    "\n",
    "# See how to save api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "openai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Generate response using the OpenAI API.\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    seed=RANDOM_SEED,\n",
    "    frequency_penalty=2,\n",
    ")\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "# Print all answers in the response.\n",
    "for i, choice in enumerate(response.choices, 1):\n",
    "    pprint.pprint(f\"Answer: {choice.message.content}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Question1: What do the parameters for HNSW mean?\n",
    "# Answer:  Looks perfect!\n",
    "# Best answer:  M: maximum degree of nodes in a layer of the graph. \n",
    "# efConstruction: number of nearest neighbors to consider when connecting nodes in the graph.\n",
    "# ef: number of nearest neighbors to consider when searching for similar vectors. \n",
    "\n",
    "# Question2: What are good default values for HNSW parameters with 25K vectors dim 1024?\n",
    "# Answer: M=16, efConstruction=500, and ef=64\n",
    "# Best answer:  M=16, efConstruction=32, ef=32\n",
    "\n",
    "# Question3: what is the default distance metric used in AUTOINDEX in Milvus?\n",
    "# Answer: L2 \n",
    "# Trick answer:  IP inner product, not yet updated in documentation still says L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4eeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=\"gpt-3.5-turbo\"\n",
    "\n",
    "# Question: What do the parameters for HNSW mean?\n",
    "('Answer: The parameters for HNSW are as follows:\\n'\n",
    " '- M: Maximum degree of the node, limiting the connections each node can have '\n",
    " 'in the graph. Range is [2, 2048].\\n'\n",
    " '- efConstruction: Parameter used during index building to specify a search '\n",
    " 'range.\\n'\n",
    " '- ef: Parameter used when searching for targets to specify a search range.\\n'\n",
    " '\\n'\n",
    " 'Sources:\\n'\n",
    " 'https://index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2b2dd",
   "metadata": {},
   "source": [
    "## Use Anthropic to generate a more human-like chat response to the user's question \n",
    "\n",
    "We've practiced retrieval for free on our own data using open-source LLMs.  <br>\n",
    "\n",
    "Now let's make a call to the paid Claude3. [List of model types](https://docs.anthropic.com/claude/docs/models-overview)\n",
    "- Opus - most expensive\n",
    "- Sonnet\n",
    "- Haiku - least expensive!\n",
    "\n",
    "Prompt engineering tutorials\n",
    "- [Interactive](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=150872633)\n",
    "- [Static](https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit#gid=869808629)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf66e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"Use the Context below to answer the user's question. Be clear, factual, complete, concise.\n",
    "If the answer is not in the Context, say \"I don't know\". \n",
    "Otherwise answer with fewer than 4 sentences and cite the grounding sources.\n",
    "Context: {contexts_combined}\n",
    "Answer: The answer to the question.\n",
    "Grounding sources: {source_combined}\n",
    "\"\"\"\n",
    "\n",
    "pprint.pprint(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install anthropic\n",
    "ANTHROPIC_API_KEY=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# # Model names\n",
    "# claude-3-opus-20240229\n",
    "# claude-3-sonnet-20240229\n",
    "# claude-3-haiku-20240307\n",
    "CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "print(f\"Model: {CLAUDE_MODEL}\")\n",
    "print()\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=CLAUDE_MODEL,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": SAMPLE_QUESTION}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Answer:\")\n",
    "pprint.pprint(message.content[0].text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a90717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install anthropic\n",
    "ANTHROPIC_API_KEY=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "print(f\"Model: {CLAUDE_MODEL}\")\n",
    "print()\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=CLAUDE_MODEL,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": SAMPLE_QUESTION}\n",
    "    ]\n",
    ")\n",
    "print(\"Answer:\")\n",
    "pprint.pprint(message.content[0].text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=\"claude-3-haiku-20240307\"\n",
    "\n",
    "# Question: What do the parameters for HNSW mean?\n",
    "# Answer:\n",
    "('According to the context provided:  The HNSW (Hierarchical Navigable Small '\n",
    " 'World Graph) algorithm has two key parameters:  1. M - This is the maximum '\n",
    " 'degree of the nodes in the graph structure. It controls the maximum number '\n",
    " 'of connections each node can have. The valid range for M is (2, 2048).  2. '\n",
    " 'efConstruction (when building the index) and ef (when searching targets) - '\n",
    " 'These parameters specify the search range, controlling the trade-off between '\n",
    " 'search time and recall rate. A higher value can improve the recall rate but '\n",
    " 'will increase the search time.  The context does not provide any additional '\n",
    " 'details on what these parameters specifically mean or how they impact the '\n",
    " 'performance of the HNSW algorithm. The information given is limited to the '\n",
    " 'parameter ranges and their general effect on the algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=\"claude-3-sonnet-20240229\"\n",
    "\n",
    "# Question: What do the parameters for HNSW mean?\n",
    "# Answer:\n",
    "('The parameters M and ef/efConstruction control the behavior of the HNSW '\n",
    " '(Hierarchical Navigable Small World) algorithm used for indexing and '\n",
    " 'searching.  M specifies the maximum number of connections (edges) that each '\n",
    " 'node in the HNSW graph can have. A higher M value allows more connections, '\n",
    " 'which can improve recall rate (finding more relevant results) but increases '\n",
    " 'search time.  ef and efConstruction determine how many nodes in each layer '\n",
    " 'of the HNSW graph should be explored during searching and index construction '\n",
    " 'respectively. Higher values increase the search range and can improve '\n",
    " 'accuracy, but also increase computation time.  Sources: [1] '\n",
    " 'https://index.html [2] https://index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=\"claude-3-opus-20240229\"\n",
    "\n",
    "# Question: What do the parameters for HNSW mean?\n",
    "# Answer:\n",
    "('According to the context, the HNSW algorithm has two key parameters:  1. M: '\n",
    " 'The maximum degree of the node on each layer of the graph, which can be set '\n",
    " 'between 2 and 2048. [1]  2. efConstruction (when building index) or ef (when '\n",
    " 'searching targets): These parameters specify the search range to improve '\n",
    " 'performance. [1]  Grounding sources: [1] https://index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e81e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collection\n",
    "# utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,transformers,sentence_transformers,pymilvus,langchain,openai --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
