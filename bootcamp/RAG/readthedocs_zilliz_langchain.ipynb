{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c3444",
   "metadata": {},
   "source": [
    "# ReadtheDocs Retrieval Augmented Generation (RAG) using Zilliz Free Tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd11a",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use Milvus documentation pages to create a chatbot about our product.  The chatbot is going to follow RAG steps to retrieve chunks of data using Semantic Vector Search, then the Question + Context will be fed as a Prompt to a LLM to generate an answer.\n",
    "\n",
    "Many RAG demos use OpenAI for the Embedding Model and ChatGPT for the Generative AI model.  **In this notebook, we will demo a fully open source RAG stack.**\n",
    "\n",
    "Using open-source Q&A with retrieval saves money since we make free calls to our own data almost all the time - retrieval, evaluation, and development iterations.  We only make a paid call to OpenAI once for the final chat generation step. \n",
    "\n",
    "<div>\n",
    "<img src=\"../../../images/rag_image.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7570b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab install these libraries in this order:\n",
    "# !pip install pymilvus, langchain, torch, transformers, python-dotenv, accelerate\n",
    "\n",
    "# Import common libraries.\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059b674",
   "metadata": {},
   "source": [
    "## Download Milvus documentation to a local directory.\n",
    "\n",
    "The data weâ€™ll use is our own product documentation web pages.  ReadTheDocs is an open-source free software documentation hosting platform, where documentation is written with the Sphinx document generator.\n",
    "\n",
    "The code block below downloads the web pages into a local directory called `rtdocs`.  \n",
    "\n",
    "I've already uploaded the `rtdocs` data folder to github, so you should see it if you cloned my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dcdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to download readthedocs pages locally.\n",
    "\n",
    "# DOCS_PAGE=\"https://pymilvus.readthedocs.io/en/latest/\"\n",
    "# !echo $DOCS_PAGE\n",
    "\n",
    "# # Specify encoding to handle non-unicode characters in documentation.\n",
    "# !wget -r -A.html -P rtdocs --header=\"Accept-Charset: UTF-8\" $DOCS_PAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67e382",
   "metadata": {},
   "source": [
    "## Start up a Zilliz free tier cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb844837",
   "metadata": {},
   "source": [
    "Code in this notebook uses fully-managed Milvus on [Ziliz Cloud free trial](https://cloud.zilliz.com/login).  Choose the default \"Starter\" option when you provision > Create collection > Give it a name > Create cluster and collection.\n",
    "\n",
    "ðŸ’¡ Note: To keep your tokens private, best practice is to use an **env variable**.  See [how to save api key in env variable](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety). <br>\n",
    "\n",
    "In Jupyter, you also need a .env file (in same dir as notebooks) containing lines like this:\n",
    "- VARIABLE_NAME=value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of server: zilliz_cloud\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymilvus #python sdk for milvus\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n",
    "\n",
    "# Connect to Zilliz cloud using enpoint URI and API key TOKEN.\n",
    "# TODO change this before checking into github.\n",
    "CLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "connections.connect(\n",
    "  alias='default',\n",
    "  #  Public endpoint obtained from Zilliz Cloud\n",
    "  uri=CLUSTER_ENDPOINT,\n",
    "  # API key or a colon-separated cluster username and password\n",
    "  token=TOKEN,\n",
    ")\n",
    "\n",
    "# Check if the server is ready and get colleciton name.\n",
    "print(f\"Type of server: {utility.get_server_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d6622",
   "metadata": {},
   "source": [
    "## Load the Embedding Model checkpoint and use it to create vector embeddings\n",
    "**Embedding model:**  We will use the open-source [sentence transformers](https://www.sbert.net/docs/pretrained_models.html) available on HuggingFace to encode the documentation text.  We will download the model from HuggingFace and run it locally. \n",
    "\n",
    "Two model parameters of note below:\n",
    "1. EMBEDDING_LENGTH refers to the dimensionality or length of the embedding vector. In this case, the embeddings generated for EACH token in the input text will have the SAME length = 768. This size of embedding is often associated with BERT-based models, where the embeddings are used for downstream tasks such as classification, question answering, or text generation. <br><br>\n",
    "2. MAX_SEQ_LENGTH is the maximum length the encoder model can handle for input sequences. In this case, if sequences longer than 512 tokens are given to the model, everything longer will be (silently!) chopped off.  This is the reason why a chunking strategy is needed to segment input texts into chunks with lengths that will fit in the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2be7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "model_name: BAAI/bge-base-en-v1.5\n",
      "EMBEDDING_LENGTH: 768\n",
      "MAX_SEQ_LENGTH: 512\n"
     ]
    }
   ],
   "source": [
    "# Import torch.\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize torch settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# Load the model from huggingface model hub.\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "print(type(encoder))\n",
    "print(encoder)\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "MAX_SEQ_LENGTH = encoder.get_max_seq_length() \n",
    "HF_EOS_TOKEN_LENGTH = 1\n",
    "EMBEDDING_LENGTH = encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "# Inspect model parameters.\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"EMBEDDING_LENGTH: {EMBEDDING_LENGTH}\")\n",
    "print(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus collection\n",
    "\n",
    "You can think of a collection in Milvus like a \"table\" in SQL databases.  The **collection** will contain the \n",
    "- **Schema** (or no-schema Milvus Client).  \n",
    "ðŸ’¡ You'll need the vector `EMBEDDING_LENGTH` parameter from your embedding model.\n",
    "Typical values are:\n",
    "   - 768 for sbert embedding models\n",
    "   - 1536 for ada-002 OpenAI embedding models\n",
    "- **Vector index** for efficient vector search\n",
    "- **Vector distance metric** for measuring nearest neighbor vectors\n",
    "- **Consistency level**\n",
    "In Milvus, transactional consistency is possible; however, according to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), some latency must be sacrificed. ðŸ’¡ Searching movie reviews is not mission-critical, so [`eventually`](https://milvus.io/docs/consistency.md) consistent is fine here.\n",
    "\n",
    "Some supported [data types](https://milvus.io/docs/schema.md) for Milvus schemas are:\n",
    "- INT64 - primary key\n",
    "- FLOAT_VECTOR - embedings = list of `numpy.ndarray` of `numpy.float32` numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 768\n",
      "Successfully created collection: `MilvusDocs`\n",
      "Schema: {'auto_id': True, 'description': 'The schema for docs pages', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import (\n",
    "    FieldSchema, DataType, \n",
    "    CollectionSchema, Collection)\n",
    "\n",
    "# 1. Name your collection.\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "\n",
    "# 2. Use embedding length from the embedding model.\n",
    "print(f\"Embedding length: {EMBEDDING_LENGTH}\")\n",
    "\n",
    "# 3. Define a minimum expandable schema.\n",
    "fields = [\n",
    "    FieldSchema(\"pk\", DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=EMBEDDING_LENGTH),\n",
    "]\n",
    "schema = CollectionSchema(\n",
    "\t\tfields,\n",
    "\t\tdescription=\"The schema for docs pages\",\n",
    "\t\tenable_dynamic_field=True\n",
    ")\n",
    "\n",
    "# 4. Check if collection already exists, if so drop it.\n",
    "has = utility.has_collection(COLLECTION_NAME)\n",
    "if has:\n",
    "    drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# 5. Create the collection.\n",
    "mc = Collection(COLLECTION_NAME, schema, consistency_level=\"Eventually\")\n",
    "\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")\n",
    "print(f\"Schema: {mc.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Vector Index\n",
    "\n",
    "The vector index determines the vector **search algorithm** used to find the closest vectors in your data to the query a user submits.  \n",
    "\n",
    "Most vector indexes use different sets of parameters depending on whether the database is:\n",
    "- **inserting vectors** (creation mode) - vs - \n",
    "- **searching vectors** (search mode) \n",
    "\n",
    "Scroll down the [docs page](https://milvus.io/docs/index.md) to see a table listing different vector indexes available on Milvus.  For example:\n",
    "- FLAT - deterministic exhaustive search\n",
    "- IVF_FLAT or IVF_SQ8 - Hash index (stochastic approximate search)\n",
    "- HNSW - Graph index (stochastic approximate search)\n",
    "- AUTOINDEX - Automatically determined by Milvus based on local vs cloud, type of GPU, size of data.\n",
    "\n",
    "Besides a search algorithm, we also need to specify a **distance metric**, that is, a definition of what is considered \"close\" in vector space.  In the cell below, the [`HNSW`](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) search index is chosen.  Its possible distance metrics are one of:\n",
    "- L2 - L2-norm\n",
    "- IP - Dot-product\n",
    "- COSINE - Angular distance\n",
    "\n",
    "ðŸ’¡ Most use cases work better with normalized embeddings, in which case L2 is useless (every vector has length=1) and IP and COSINE are the same.  Only choose L2 if you plan to keep your embeddings unnormalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loading_progress': '100%'}\n"
     ]
    }
   ],
   "source": [
    "# 5. Drop the index, in case it already exists.\n",
    "mc.drop_index()\n",
    "\n",
    "# 6. Add a default search index to the collection.\n",
    "index_params = {\n",
    "    \"index_type\": \"AUTOINDEX\",\n",
    "    \"metric_type\": \"COSINE\", \n",
    "    # No params for AUTOINDEX\n",
    "    # \"params\": {}\n",
    "    }\n",
    "\n",
    "# 7. Specify column name which contains the vector.\n",
    "mc.create_index(\n",
    "    field_name=\"vector\", \n",
    "    index_params=index_params)\n",
    "\n",
    "# Get loading progress\n",
    "mc.load()\n",
    "progress = utility.loading_progress(COLLECTION_NAME)\n",
    "print(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6861beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 15 documents\n"
     ]
    }
   ],
   "source": [
    "## Read docs into LangChain\n",
    "#!pip install langchain \n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader(\"rtdocs/pymilvus.readthedocs.io/en/latest/\", features=\"html.parser\")\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60423a5",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  In this demo, I will use:\n",
    "- **Strategy** = Use markdown header hierarchies.  Keep markdown sections together unless they are too long.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `HTMLHeaderTextSplitter` to split markdown sections.\n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively.\n",
    "\n",
    "\n",
    "Notice below, each chunk is grounded with the document source page.  <br>\n",
    "In addition, header titles are kept together with the chunk of markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking time: 0.023195981979370117\n",
      "docs: 15, split into: 15\n",
      "split into chunks: 159, type: list of <class 'langchain.schema.document.Document'>\n",
      "\n",
      "Looking at a sample chunk...\n",
      "InstallationÂ¶ Installing via pipÂ¶ PyMilvus is in the Python Package Index. PyMilvus only support pyt\n",
      "{'h1': 'Installation', 'h2': 'Installing via pip', 'source': 'rtdocs/pymilvus.readthedocs.io/en/latest/install.html'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the headers to split on for the HTMLHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "# Create an instance of the HTMLHeaderTextSplitter\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Use the embedding model parameters.\n",
    "chunk_size = MAX_SEQ_LENGTH - HF_EOS_TOKEN_LENGTH\n",
    "chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "\n",
    "# Create an instance of the RecursiveCharacterTextSplitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# Split the HTML text using the HTMLHeaderTextSplitter.\n",
    "start_time = time.time()\n",
    "html_header_splits = []\n",
    "for doc in docs:\n",
    "    splits = html_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        # Add the source URL and header values to the metadata\n",
    "        metadata = {}\n",
    "        new_text = split.page_content\n",
    "        for header_name, metadata_header_name in headers_to_split_on:\n",
    "            header_value = new_text.split(\"Â¶ \")[0].strip()\n",
    "            metadata[header_name] = header_value\n",
    "            try:\n",
    "                new_text = new_text.split(\"Â¶ \")[1].strip()\n",
    "            except:\n",
    "                break\n",
    "        split.metadata = {\n",
    "            **metadata,\n",
    "            \"source\": doc.metadata[\"source\"]\n",
    "        }\n",
    "        # Add the header to the text\n",
    "        split.page_content = split.page_content\n",
    "    html_header_splits.extend(splits)\n",
    "\n",
    "# Split the documents further into smaller, recursive chunks.\n",
    "chunks = child_splitter.split_documents(html_header_splits)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "print(f\"docs: {len(docs)}, split into: {len(html_header_splits)}\")\n",
    "print(f\"split into chunks: {len(chunks)}, type: list of {type(chunks[0])}\") \n",
    "\n",
    "# Inspect a chunk.\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)\n",
    "\n",
    "# # TODO - remove this before saving in github.\n",
    "# # Print the child splits with their associated header metadata\n",
    "# print()\n",
    "# for child in chunks:\n",
    "#     print(f\"Content: {child.page_content}\")\n",
    "#     print(f\"Metadata: {child.metadata}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512130a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h1': 'Installation', 'h2': 'Installing via pip', 'source': 'https://pymilvus.readthedocs.io/en/latest/install.html'}\n",
      "InstallationÂ¶ Installing via pipÂ¶ PyMilvus is in the Python Package Index. PyMilvus only support pyt\n"
     ]
    }
   ],
   "source": [
    "# Clean up the metadata urls\n",
    "for doc in chunks:\n",
    "    new_url = doc.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"rtdocs\", \"https:/\")\n",
    "    doc.metadata.update({\"source\": new_url})\n",
    "\n",
    "print(chunks[0].metadata)\n",
    "print(chunks[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd8153",
   "metadata": {},
   "source": [
    "## Insert data into Milvus\n",
    "\n",
    "For each original text chunk, we'll write the quadruplet (`vector, text, source, h1, h2`) into the database.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../../images/db_insert.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milvus and Milvus Lite support loading data from:\n",
    "- pandas dataframes \n",
    "- list of dictionaries\n",
    "\n",
    "Below, we use the embedding model provided by HuggingFace, download its checkpoint, and run it locally as the encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chunks to a list of dictionaries.\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "\n",
    "    # Generate embeddings using encoder from HuggingFace.\n",
    "    embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    converted_values = list(map(np.float32, embeddings))[0]\n",
    "    \n",
    "    # Only use h1, h2. Truncate the metadata in case too long.\n",
    "    try:\n",
    "        h2 = chunk.metadata['h2'][:50]\n",
    "    except:\n",
    "        h2 = \"\"\n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    chunk_dict = {\n",
    "        'vector': converted_values,\n",
    "        'text': chunk.page_content,\n",
    "        'source': chunk.metadata['source'],\n",
    "        'h1': chunk.metadata['h1'][:50],\n",
    "        'h2': h2,\n",
    "    }\n",
    "    chunk_list.append(chunk_dict)\n",
    "\n",
    "# # TODO - remove this before saving in github.\n",
    "# for chunk in chunk_list[:1]:\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51ff139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting entities\n",
      "Milvus insert time for 159 vectors: 0.7005021572113037 seconds\n",
      "[{\"name\":\"_default\",\"collection_name\":\"MilvusDocs\",\"description\":\"\"}]\n"
     ]
    }
   ],
   "source": [
    "# Insert data into the Milvus collection.\n",
    "\n",
    "print(\"Start inserting entities\")\n",
    "start_time = time.time()\n",
    "insert_result = mc.insert(chunk_list)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Milvus insert time for {len(chunk_list)} vectors: {end_time - start_time} seconds\")\n",
    "\n",
    "# After final entity is inserted, call flush to stop growing segments left in memory.\n",
    "mc.flush() \n",
    "\n",
    "# Inspect results.\n",
    "print(mc.partitions) # list[Partition] objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfb115",
   "metadata": {},
   "source": [
    "## Run a Semantic Search\n",
    "\n",
    "Now we can search all the documentation embeddings to find the `TOP_K` documentation chunks with the closest embeddings to a user's query.\n",
    "\n",
    "ðŸ’¡ The same model should always be used for consistency for all the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c589ff",
   "metadata": {},
   "source": [
    "## Ask a question about your data\n",
    "\n",
    "So far in this demo notebook: \n",
    "1. Your custom data has been mapped into a vector embedding space\n",
    "2. Those vector embeddings have been saved into a vector database\n",
    "\n",
    "Next, you can ask a question about your custom data!\n",
    "\n",
    "ðŸ’¡ In LLM vocabulary:\n",
    "> **Query** is the generic term for user questions.  \n",
    "A query is a list of multiple individual questions, up to maybe 1000 different questions!\n",
    "\n",
    "> **Question** usually refers to a single user question.  \n",
    "In our example below, the user question is \"What is AUTOINDEX in Milvus Client?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7f41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query length: 54\n"
     ]
    }
   ],
   "source": [
    "# Define a sample question about your data.\n",
    "QUESTION = \"what is the default distance metric used in AUTOINDEX?\"\n",
    "QUERY = [QUESTION]\n",
    "\n",
    "# Inspect the length of the query.\n",
    "QUERY_LENGTH = len(QUERY[0])\n",
    "print(f\"query length: {QUERY_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea29411",
   "metadata": {},
   "source": [
    "## Execute a vector search\n",
    "\n",
    "Search Milvus using [PyMilvus API](https://milvus.io/docs/search.md).\n",
    "\n",
    "ðŸ’¡ By their nature, vector searches are \"semantic\" searches.  For example, if you were to search for \"leaky faucet\": \n",
    "> **Traditional Key-word Search** - either or both words \"leaky\", \"faucet\" would have to match some text in order to return a web page or link text to the document.\n",
    "\n",
    "> **Semantic search** - results containing words \"drippy\" \"taps\" would be returned as well because these words mean the same thing even though they are different words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89642119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded milvus collection into memory.\n",
      "Milvus search time: 0.0449519157409668 sec\n",
      "type: <class 'pymilvus.client.abstract.SearchResult'>, count: 5\n"
     ]
    }
   ],
   "source": [
    "# RETRIEVAL USING MILVUS API.\n",
    "\n",
    "# Before conducting a search based on a query, you need to load the data into memory.\n",
    "mc.load()\n",
    "print(\"Loaded milvus collection into memory.\")\n",
    "\n",
    "# Embed the question using the same encoder.\n",
    "embedded_question = torch.tensor(encoder.encode(QUERY))\n",
    "# Normalize embeddings to unit length.\n",
    "embedded_question = F.normalize(embedded_question, p=2, dim=1)\n",
    "# Convert the embeddings to list of list of np.float32.\n",
    "embedded_question = list(map(np.float32, embedded_question))\n",
    "\n",
    "# Return top k results with AUTOINDEX.\n",
    "TOP_K = 5\n",
    "\n",
    "# Run semantic vector search using your query and the vector database.\n",
    "start_time = time.time()\n",
    "results = mc.search(\n",
    "    data=embedded_question, \n",
    "    anns_field=\"vector\", \n",
    "    # No params for AUTOINDEX\n",
    "    param={},\n",
    "    # Milvus can utilize metadata to enhance the search experience in boolean expressions.\n",
    "    # expr=\"\",\n",
    "    output_fields=[\"h1\", \"h2\", \"text\", \"source\"], \n",
    "    limit=TOP_K,\n",
    "    consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus search time: {elapsed_time} sec\")\n",
    "\n",
    "# Inspect search result.\n",
    "print(f\"type: {type(results)}, count: {len(results[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble and inspect the search result\n",
    "\n",
    "The search result is in the variable `results[0]` of type `'pymilvus.orm.search.SearchResult'`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th query result\n",
      "id: 445766022949312658, distance: 0.708217978477478, entity: {'text': \"index_file_size (int) â€“ Segment size. See Storage Concepts. metric_type (MetricType) â€“ Distance Metrics type. Valued form MetricType. See Distance Metrics. A demo is as follow: param={'collection_name': 'name', 'dimension': 16, 'index_file_size': 1024 # Optional, default 1024ï¼Œ 'metric_type': MetricType.L2 # Optional, default MetricType.L2 } timeout (float) â€“ An optional duration of time in seconds to allow for the RPC. When timeout is set to None, client waits until server responses or error occurs.\", 'source': 'https://pymilvus.readthedocs.io/en/latest/api.html', 'h1': 'API reference', 'h2': 'Client'}\n",
      "id: 445766022949312769, distance: 0.690363883972168, entity: {'text': 'A metric for binary vectors, only support :attr:`~milvus.IndexType.FLAT` index. #: See `Substructure `_. SUPERSTRUCTURE = 7 def __repr__(self): return \"\".format(self.__class__.__name__, self._name_) def __str__(self): return self._name_', 'source': 'https://pymilvus.readthedocs.io/en/latest/_modules/milvus/client/types.html', 'h1': 'Source code for milvus.client.types from enum impo', 'h2': ''}\n",
      "id: 445766022949312619, distance: 0.688593864440918, entity: {'text': 'you can refer to Storage Concepts for more information about segments and index_file_size. metric_type:Milvus compute distance between two vectors, you can refer to Distance Metrics for more information. Now we can create a collection: >>> collection_name = \\'demo_film_tutorial\\' >>> collection_param = { ... \"collection_name\": collection_name, ... \"dimension\": 8, ... \"index_file_size\": 2048, ... \"metric_type\": MetricType.L2 ... } >>> client.create_collection(collection_param) Status(code=0, message=\\'Create', 'source': 'https://pymilvus.readthedocs.io/en/latest/tutorial.html', 'h1': 'Tutorial', 'h2': 'This is a basic introduction to Milvus by PyMilvus'}\n",
      "id: 445766022949312621, distance: 0.6716917157173157, entity: {'text': \"metric_type=) The attributes of collection can be extracted from info. >>> info.collection_name 'demo_film_tutorial' >>> info.dimension 8 >>> info.index_file_size 2048 >>> info.metric_type This tutorial is a basic intro tutorial, building index wonâ€™t be covered by this tutorial. If you want to go further into Milvus with indexes, itâ€™s recommended to check our index examples. If youâ€™re already known about indexes from index examples, and you want a full lists of params supported by PyMilvus, you check out\", 'source': 'https://pymilvus.readthedocs.io/en/latest/tutorial.html', 'h1': 'Tutorial', 'h2': 'This is a basic introduction to Milvus by PyMilvus'}\n",
      "id: 445766022949312711, distance: 0.6690606474876404, entity: {'text': \"-- Segment size. See `Storage Concepts `_. * *metric_type* (``MetricType``) -- Distance Metrics type. Valued form :class:`~milvus.MetricType`. See `Distance Metrics `_. A demo is as follow: .. code-block:: python param={'collection_name': 'name', 'dimension': 16, 'index_file_size': 1024 # Optional, default 1024ï¼Œ 'metric_type': MetricType.L2 # Optional, default MetricType.L2 } :param timeout: An optional duration of time in seconds to allow for the RPC. When timeout is set to None, client waits until\", 'source': 'https://pymilvus.readthedocs.io/en/latest/_modules/milvus/client/stub.html', 'h1': 'Source code for milvus.client.stub # -*- coding: U', 'h2': ''}\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "# TODO - remove this before saving in github.\n",
    "for n, hits in enumerate(results):\n",
    "    print(f\"{n}th query result\")\n",
    "    for hit in hits:\n",
    "        print(hit)\n",
    "\n",
    "# Assemble the context as a stuffed string.\n",
    "context = \"\"\n",
    "i = 0\n",
    "for r in results[0]:\n",
    "    text = r.entity.text\n",
    "    if i == 0:  # only first result\n",
    "        context += f\"{text} \"\n",
    "    i += 1\n",
    "print(len(context))\n",
    "\n",
    "# Also save the context metadata to retrieve along with the answer.\n",
    "context_metadata = {\n",
    "    \"h1\": results[0][0].entity.h1,\n",
    "    \"h2\": results[0][0].entity.h2,\n",
    "    \"source\": results[0][0].entity.source,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6060ce",
   "metadata": {},
   "source": [
    "## Use an LLM to Generate a chat response to the user's question using the Retrieved Context.\n",
    "\n",
    "Below, we'll use an open, very tiny generative AI model, or LLM, available on HuggingFace.  Many demos use OpenAI as the LLM choice instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_grounding_sources(answer, context_metadata):\n",
    "    \"\"\"Assemble the answer and grounding sources into a string\"\"\"\n",
    "    grounded_answer = f\"Answer: {answer}\\n\"\n",
    "    grounded_answer += \"Grounding sources and citations:\\n\"\n",
    "    try:\n",
    "        grounded_answer += f\"'h1': {context_metadata['h1']}, 'h2':{context_metadata['h2']}\\n\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grounded_answer += f\"'source': {context_metadata['source']}\"\n",
    "    except:\n",
    "        pass\n",
    "    return grounded_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e7fa0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the default distance metric used in AUTOINDEX?\n",
      "Answer: lazy dog\n"
     ]
    }
   ],
   "source": [
    "# BASELINING THE LLM: ASK A QUESTION WITHOUT ANY RETRIEVED CONTEXT.\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the Hugging Face auto-regressive LLM checkpoint.\n",
    "tiny_llm = \"deepset/tinyroberta-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tiny_llm)\n",
    "\n",
    "# context cannot be empty so just put random text in it.\n",
    "QA_input = {\n",
    "    'question': QUESTION,\n",
    "    'context': 'The quick brown fox jumped over the lazy dog'\n",
    "}\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model=tiny_llm, \n",
    "               tokenizer=tokenizer)\n",
    "result = nlp(QA_input)\n",
    "\n",
    "# Print the question and answer.\n",
    "print(f\"Question: {QUESTION}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "\n",
    "# The baseline LLM chat is not very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a68e87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the default distance metric used in AUTOINDEX?\n",
      "Answer: MetricType.L2\n",
      "Grounding sources and citations:\n",
      "'h1': API reference, 'h2':Client\n",
      "'source': https://pymilvus.readthedocs.io/en/latest/api.html\n"
     ]
    }
   ],
   "source": [
    "# NOW ASK THE SAME LLM THE SAME QUESTION USING THE RETRIEVED CONTEXT.\n",
    "QA_input = {\n",
    "    'question': QUESTION,\n",
    "    'context': context,\n",
    "}\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model=tiny_llm, \n",
    "               tokenizer=tokenizer)\n",
    "result = nlp(QA_input)\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "answer = assemble_grounding_sources(result['answer'], context_metadata)\n",
    "print(f\"Question: {QUESTION}\")\n",
    "print(answer)\n",
    "\n",
    "# That answer looks a little better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to generate a more human-like chat response to the user's question \n",
    "\n",
    "We've practiced retrieval for free on our own data using open-source LLMs.  <br>\n",
    "\n",
    "Now let's make a call to the paid OpenAI GPT.\n",
    "\n",
    "ðŸ’¡ Note: Weâ€™re using a temperature of 0.0 to enable reproducible experiments. For use cases that need to always be factually grounded, use very low temperature values while more creative tasks can benefit from higher temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# See how to save api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Define the generation llm model to use.\n",
    "LLM_NAME = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_response(response):\n",
    "    return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "\n",
    "def generate_response(\n",
    "    llm, temperature=0.0, \n",
    "    grounding_sources=None,\n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=llm,\n",
    "            temperature=temperature,\n",
    "            api_key=openai.api_key,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "        )\n",
    "        answer = prepare_response(response=response)\n",
    "    \n",
    "        # Add the grounding sources and citations.\n",
    "        answer = assemble_grounding_sources(answer, grounding_sources)\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the default distance metric used in AUTOINDEX?\n",
      "Answer: The default distance metric used in AUTOINDEX is L2.\n",
      "Grounding sources and citations:\n",
      "'h1': API reference, 'h2':Client\n",
      "'source': https://pymilvus.readthedocs.io/en/latest/api.html\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "response = generate_response(\n",
    "    llm=LLM_NAME,\n",
    "    temperature=0.0,\n",
    "    grounding_sources=context_metadata,\n",
    "    system_content=\"Answer the question using the context provided. Be succinct.\",\n",
    "    user_content=f\"question: {QUESTION}, context: {context}\")\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {QUESTION}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e81e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collection\n",
    "utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c777937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Christy Bergman\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.15.0\n",
      "\n",
      "torch                : 2.0.1\n",
      "transformers         : 4.34.1\n",
      "sentence_transformers: 2.2.2\n",
      "pymilvus             : 2.3.3\n",
      "langchain            : 0.0.322\n",
      "openai               : 0.28.0\n",
      "\n",
      "conda environment: py310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,transformers,sentence_transformers,pymilvus,langchain,openai --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
