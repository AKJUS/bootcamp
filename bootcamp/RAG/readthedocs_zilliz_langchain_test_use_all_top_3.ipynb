{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c3444",
   "metadata": {},
   "source": [
    "# ReadtheDocs Retrieval Augmented Generation (RAG) using Zilliz Free Tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd11a",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use Milvus documentation pages to create a chatbot about our product.  The chatbot is going to follow RAG steps to retrieve chunks of data using Semantic Vector Search, then the Question + Context will be fed as a Prompt to a LLM to generate an answer.\n",
    "\n",
    "Many RAG demos use OpenAI for the Embedding Model and ChatGPT for the Generative AI model.  **In this notebook, we will demo a fully open source RAG stack.**\n",
    "\n",
    "Using open-source Q&A with retrieval saves money since we make free calls to our own data almost all the time - retrieval, evaluation, and development iterations.  We only make a paid call to OpenAI once for the final chat generation step. \n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/rag_image.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7570b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab install these libraries in this order:\n",
    "# !pip install pymilvus, langchain, torch, transformers, python-dotenv, accelerate\n",
    "\n",
    "# Import common libraries.\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059b674",
   "metadata": {},
   "source": [
    "## Download Milvus documentation to a local directory.\n",
    "\n",
    "The data weâ€™ll use is our own product documentation web pages.  ReadTheDocs is an open-source free software documentation hosting platform, where documentation is written with the Sphinx document generator.\n",
    "\n",
    "The code block below downloads the web pages into a local directory called `rtdocs`.  \n",
    "\n",
    "I've already uploaded the `rtdocs` data folder to github, so you should see it if you cloned my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dcdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to download readthedocs pages locally.\n",
    "\n",
    "# DOCS_PAGE=\"https://pymilvus.readthedocs.io/en/latest/\"\n",
    "# !echo $DOCS_PAGE\n",
    "\n",
    "# # Specify encoding to handle non-unicode characters in documentation.\n",
    "# !wget -r -A.html -P rtdocs --header=\"Accept-Charset: UTF-8\" $DOCS_PAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67e382",
   "metadata": {},
   "source": [
    "## Start up a Zilliz free tier cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb844837",
   "metadata": {},
   "source": [
    "Code in this notebook uses fully-managed Milvus on [Ziliz Cloud free trial](https://cloud.zilliz.com/login).  Choose the default \"Starter\" option when you provision > Create collection > Give it a name > Create cluster and collection.\n",
    "\n",
    "ðŸ’¡ Note: To keep your tokens private, best practice is to use an **env variable**.  See [how to save api key in env variable](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety). <br>\n",
    "\n",
    "In Jupyter, you also need a .env file (in same dir as notebooks) containing lines like this:\n",
    "- VARIABLE_NAME=value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of server: zilliz_cloud\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymilvus #python sdk for milvus\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n",
    "\n",
    "# Connect to Zilliz cloud using enpoint URI and API key TOKEN.\n",
    "# TODO change this before checking into github.\n",
    "CLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "connections.connect(\n",
    "  alias='default',\n",
    "  #  Public endpoint obtained from Zilliz Cloud\n",
    "  uri=CLUSTER_ENDPOINT,\n",
    "  # API key or a colon-separated cluster username and password\n",
    "  token=TOKEN,\n",
    ")\n",
    "\n",
    "# Check if the server is ready and get colleciton name.\n",
    "print(f\"Type of server: {utility.get_server_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d6622",
   "metadata": {},
   "source": [
    "## Load the Embedding Model checkpoint and use it to create vector embeddings\n",
    "**Embedding model:**  We will use the open-source [sentence transformers](https://www.sbert.net/docs/pretrained_models.html) available on HuggingFace to encode the documentation text.  We will download the model from HuggingFace and run it locally. \n",
    "\n",
    "Two model parameters of note below:\n",
    "1. EMBEDDING_LENGTH refers to the dimensionality or length of the embedding vector. In this case, the embeddings generated for EACH token in the input text will have the SAME length = 768. This size of embedding is often associated with BERT-based models, where the embeddings are used for downstream tasks such as classification, question answering, or text generation. <br><br>\n",
    "2. MAX_SEQ_LENGTH is the maximum length the encoder model can handle for input sequences. In this case, if sequences longer than 512 tokens are given to the model, everything longer will be (silently!) chopped off.  This is the reason why a chunking strategy is needed to segment input texts into chunks with lengths that will fit in the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2be7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "model_name: BAAI/bge-base-en-v1.5\n",
      "EMBEDDING_LENGTH: 768\n",
      "MAX_SEQ_LENGTH: 512\n"
     ]
    }
   ],
   "source": [
    "# Import torch.\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize torch settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# Load the model from huggingface model hub.\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "print(type(encoder))\n",
    "print(encoder)\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "MAX_SEQ_LENGTH = encoder.get_max_seq_length() \n",
    "HF_EOS_TOKEN_LENGTH = 1\n",
    "EMBEDDING_LENGTH = encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "# Inspect model parameters.\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"EMBEDDING_LENGTH: {EMBEDDING_LENGTH}\")\n",
    "print(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus collection\n",
    "\n",
    "You can think of a collection in Milvus like a \"table\" in SQL databases.  The **collection** will contain the \n",
    "- **Schema** (or no-schema Milvus Client).  \n",
    "ðŸ’¡ You'll need the vector `EMBEDDING_LENGTH` parameter from your embedding model.\n",
    "Typical values are:\n",
    "   - 768 for sbert embedding models\n",
    "   - 1536 for ada-002 OpenAI embedding models\n",
    "- **Vector index** for efficient vector search\n",
    "- **Vector distance metric** for measuring nearest neighbor vectors\n",
    "- **Consistency level**\n",
    "In Milvus, transactional consistency is possible; however, according to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), some latency must be sacrificed. ðŸ’¡ Searching movie reviews is not mission-critical, so [`eventually`](https://milvus.io/docs/consistency.md) consistent is fine here.\n",
    "\n",
    "Some supported [data types](https://milvus.io/docs/schema.md) for Milvus schemas are:\n",
    "- INT64 - primary key\n",
    "- FLOAT_VECTOR - embedings = list of `numpy.ndarray` of `numpy.float32` numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 768\n",
      "Successfully created collection: `MilvusDocs`\n",
      "Schema: {'auto_id': True, 'description': 'The schema for docs pages', 'fields': [{'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import (\n",
    "    FieldSchema, DataType, \n",
    "    CollectionSchema, Collection)\n",
    "\n",
    "# 1. Name your collection.\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "\n",
    "# 2. Use embedding length from the embedding model.\n",
    "print(f\"Embedding length: {EMBEDDING_LENGTH}\")\n",
    "\n",
    "# 3. Define a minimum expandable schema.\n",
    "fields = [\n",
    "    FieldSchema(\"pk\", DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=EMBEDDING_LENGTH),\n",
    "]\n",
    "schema = CollectionSchema(\n",
    "\t\tfields,\n",
    "\t\tdescription=\"The schema for docs pages\",\n",
    "\t\tenable_dynamic_field=True\n",
    ")\n",
    "\n",
    "# 4. Check if collection already exists, if so drop it.\n",
    "has = utility.has_collection(COLLECTION_NAME)\n",
    "if has:\n",
    "    drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# 5. Create the collection.\n",
    "mc = Collection(COLLECTION_NAME, schema, consistency_level=\"Eventually\")\n",
    "\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")\n",
    "print(f\"Schema: {mc.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Vector Index\n",
    "\n",
    "The vector index determines the vector **search algorithm** used to find the closest vectors in your data to the query a user submits.  \n",
    "\n",
    "Most vector indexes use different sets of parameters depending on whether the database is:\n",
    "- **inserting vectors** (creation mode) - vs - \n",
    "- **searching vectors** (search mode) \n",
    "\n",
    "Scroll down the [docs page](https://milvus.io/docs/index.md) to see a table listing different vector indexes available on Milvus.  For example:\n",
    "- FLAT - deterministic exhaustive search\n",
    "- IVF_FLAT or IVF_SQ8 - Hash index (stochastic approximate search)\n",
    "- HNSW - Graph index (stochastic approximate search)\n",
    "- AUTOINDEX - Automatically determined based on OSS vs [Zilliz cloud](https://docs.zilliz.com/docs/autoindex-explained), type of GPU, size of data.\n",
    "\n",
    "Besides a search algorithm, we also need to specify a **distance metric**, that is, a definition of what is considered \"close\" in vector space.  In the cell below, the [`HNSW`](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) search index is chosen (Milvus OSS default AUTOINDEX).  Its possible distance metrics are one of:\n",
    "- L2 - L2-norm\n",
    "- IP - Dot-product\n",
    "- COSINE - Angular distance\n",
    "\n",
    "ðŸ’¡ Most use cases work better with normalized embeddings, in which case L2 is useless (every vector has length=1) and IP and COSINE are the same.  Only choose L2 if you plan to keep your embeddings unnormalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loading_progress': '100%'}\n"
     ]
    }
   ],
   "source": [
    "# 5. Drop the index, in case it already exists.\n",
    "mc.drop_index()\n",
    "\n",
    "# 6. Add a default search index to the collection.\n",
    "index_params = {\n",
    "    \"index_type\": \"AUTOINDEX\",\n",
    "    \"metric_type\": \"COSINE\", \n",
    "    # No params for AUTOINDEX\n",
    "    # \"params\": {}\n",
    "    }\n",
    "\n",
    "# 7. Specify column name which contains the vector.\n",
    "mc.create_index(\n",
    "    field_name=\"vector\", \n",
    "    index_params=index_params)\n",
    "\n",
    "# Get loading progress\n",
    "mc.load()\n",
    "progress = utility.loading_progress(COLLECTION_NAME)\n",
    "print(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6861beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8 documents\n"
     ]
    }
   ],
   "source": [
    "## Read docs into LangChain\n",
    "#!pip install langchain \n",
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader(\"rtdocs/pymilvus.readthedocs.io/en/latest/\"\n",
    "                           , encoding=\"utf-8\"\n",
    "                           , features=\"html.parser\")\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60423a5",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  In this demo, I will use:\n",
    "- **Strategy** = Use markdown header hierarchies.  Keep markdown sections together unless they are too long.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `HTMLHeaderTextSplitter` to split markdown sections.\n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively.\n",
    "\n",
    "\n",
    "Notice below, each chunk is grounded with the document source page.  <br>\n",
    "In addition, header titles are kept together with the chunk of markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking time: 0.012528181076049805\n",
      "docs: 8, split into: 8\n",
      "split into chunks: 156, type: list of <class 'langchain.schema.document.Document'>\n",
      "\n",
      "Looking at a sample chunk...\n",
      "InstallationÂ¶ Installing via pipÂ¶ PyMilvus is in the Python Package Index. PyMilvus only support pyt\n",
      "{'h1': 'Installation', 'h2': 'Installing via pip', 'source': 'rtdocs/pymilvus.readthedocs.io/en/latest/install.html'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the headers to split on for the HTMLHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "# Create an instance of the HTMLHeaderTextSplitter\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Use the embedding model parameters.\n",
    "chunk_size = MAX_SEQ_LENGTH - HF_EOS_TOKEN_LENGTH\n",
    "chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "\n",
    "# Create an instance of the RecursiveCharacterTextSplitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# Split the HTML text using the HTMLHeaderTextSplitter.\n",
    "start_time = time.time()\n",
    "html_header_splits = []\n",
    "for doc in docs:\n",
    "    soup = BeautifulSoup(doc.page_content, 'html.parser')\n",
    "    splits = html_splitter.split_text(str(soup))\n",
    "    for split in splits:\n",
    "        # Add the source URL and header values to the metadata\n",
    "        metadata = {}\n",
    "        new_text = split.page_content\n",
    "        for header_name, metadata_header_name in headers_to_split_on:\n",
    "            header_value = new_text.split(\"Â¶ \")[0].strip()\n",
    "            metadata[header_name] = header_value\n",
    "            try:\n",
    "                new_text = new_text.split(\"Â¶ \")[1].strip()\n",
    "            except:\n",
    "                break\n",
    "        split.metadata = {\n",
    "            **metadata,\n",
    "            \"source\": doc.metadata[\"source\"]\n",
    "        }\n",
    "        # Add the header to the text\n",
    "        split.page_content = split.page_content\n",
    "    html_header_splits.extend(splits)\n",
    "\n",
    "# Split the documents further into smaller, recursive chunks.\n",
    "chunks = child_splitter.split_documents(html_header_splits)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "print(f\"docs: {len(docs)}, split into: {len(html_header_splits)}\")\n",
    "print(f\"split into chunks: {len(chunks)}, type: list of {type(chunks[0])}\") \n",
    "\n",
    "# Inspect a chunk.\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)\n",
    "\n",
    "# # TODO - remove this before saving in github.\n",
    "# # Print the child splits with their associated header metadata\n",
    "# print()\n",
    "# for child in chunks:\n",
    "#     print(f\"Content: {child.page_content}\")\n",
    "#     print(f\"Metadata: {child.metadata}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512130a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InstallationÂ¶ Installing via pipÂ¶ PyMilvus is in the Python Package Index. PyMilvus only support pyt\n",
      "{'h1': 'Installation', 'h2': 'Installing via pip', 'source': 'https://pymilvus.readthedocs.io/en/latest/install.html'}\n"
     ]
    }
   ],
   "source": [
    "# Clean up the metadata urls\n",
    "for doc in chunks:\n",
    "    new_url = doc.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"rtdocs\", \"https:/\")\n",
    "    doc.metadata.update({\"source\": new_url})\n",
    "\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd8153",
   "metadata": {},
   "source": [
    "## Insert data into Milvus\n",
    "\n",
    "For each original text chunk, we'll write the quadruplet (`vector, text, source, h1, h2`) into the database.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/db_insert.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milvus and Milvus Lite support loading data from:\n",
    "- pandas dataframes \n",
    "- list of dictionaries\n",
    "\n",
    "Below, we use the embedding model provided by HuggingFace, download its checkpoint, and run it locally as the encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chunks to a list of dictionaries.\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "\n",
    "    # Generate embeddings using encoder from HuggingFace.\n",
    "    embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    converted_values = list(map(np.float32, embeddings))[0]\n",
    "    \n",
    "    # Only use h1, h2. Truncate the metadata in case too long.\n",
    "    try:\n",
    "        h2 = chunk.metadata['h2'][:50]\n",
    "    except:\n",
    "        h2 = \"\"\n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    chunk_dict = {\n",
    "        'vector': converted_values,\n",
    "        'text': chunk.page_content,\n",
    "        'source': chunk.metadata['source'],\n",
    "        'h1': chunk.metadata['h1'][:50],\n",
    "        'h2': h2,\n",
    "    }\n",
    "    chunk_list.append(chunk_dict)\n",
    "\n",
    "# # TODO - remove this before saving in github.\n",
    "# for chunk in chunk_list[:1]:\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51ff139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting entities\n",
      "Milvus insert time for 156 vectors: 0.3762798309326172 seconds\n",
      "[{\"name\":\"_default\",\"collection_name\":\"MilvusDocs\",\"description\":\"\"}]\n"
     ]
    }
   ],
   "source": [
    "# Insert data into the Milvus collection.\n",
    "\n",
    "print(\"Start inserting entities\")\n",
    "start_time = time.time()\n",
    "insert_result = mc.insert(chunk_list)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Milvus insert time for {len(chunk_list)} vectors: {end_time - start_time} seconds\")\n",
    "\n",
    "# After final entity is inserted, call flush to stop growing segments left in memory.\n",
    "mc.flush() \n",
    "\n",
    "# Inspect results.\n",
    "print(mc.partitions) # list[Partition] objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c589ff",
   "metadata": {},
   "source": [
    "## Ask a question about your data\n",
    "\n",
    "So far in this demo notebook: \n",
    "1. Your custom data has been mapped into a vector embedding space\n",
    "2. Those vector embeddings have been saved into a vector database\n",
    "\n",
    "Next, you can ask a question about your custom data!\n",
    "\n",
    "ðŸ’¡ In LLM vocabulary:\n",
    "> **Query** is the generic term for user questions.  \n",
    "A query is a list of multiple individual questions, up to maybe 1000 different questions!\n",
    "\n",
    "> **Question** usually refers to a single user question.  \n",
    "In our example below, the user question is \"What is AUTOINDEX in Milvus Client?\"\n",
    "\n",
    "> **Semantic Search** = search all the documentation embeddings to find the `TOP_K` documentation chunks with the closest embeddings to the user's query.\n",
    "\n",
    "ðŸ’¡ The same model should always be used for consistency for all the embeddings data and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7f41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query length: 74\n"
     ]
    }
   ],
   "source": [
    "# Define a sample question about your data.\n",
    "QUESTION1 = \"What do the parameters for HNSW mean?\"\n",
    "QUESTION2 = \"What are good default values for HNSW parameters with 25K vectors dim 768?\"\n",
    "QUESTION3 = \"Default distance metric used in AUTOINDEX?\"\n",
    "QUERY = [QUESTION1, QUESTION2, QUESTION3]\n",
    "\n",
    "# Inspect the length of the query.\n",
    "QUERY_LENGTH = len(QUESTION2)\n",
    "print(f\"query length: {QUERY_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT A PARTICULAR QUESTION TO ASK.\n",
    "\n",
    "SAMPLE_QUESTION = QUESTION1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea29411",
   "metadata": {},
   "source": [
    "## Execute a vector search\n",
    "\n",
    "Search Milvus using [PyMilvus API](https://milvus.io/docs/search.md).\n",
    "\n",
    "ðŸ’¡ By their nature, vector searches are \"semantic\" searches.  For example, if you were to search for \"leaky faucet\": \n",
    "> **Traditional Key-word Search** - either or both words \"leaky\", \"faucet\" would have to match some text in order to return a web page or link text to the document.\n",
    "\n",
    "> **Semantic search** - results containing words \"drippy\" \"taps\" would be returned as well because these words mean the same thing even though they are different words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89642119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded milvus collection into memory.\n",
      "Milvus search time: 0.05107903480529785 sec\n",
      "type: <class 'pymilvus.client.abstract.SearchResult'>, count: 3\n"
     ]
    }
   ],
   "source": [
    "# RETRIEVAL USING MILVUS API.\n",
    "\n",
    "# Before conducting a search based on a query, you need to load the data into memory.\n",
    "mc.load()\n",
    "print(\"Loaded milvus collection into memory.\")\n",
    "\n",
    "# Embed the question using the same encoder.\n",
    "embedded_question = torch.tensor(encoder.encode([SAMPLE_QUESTION]))\n",
    "# Normalize embeddings to unit length.\n",
    "embedded_question = F.normalize(embedded_question, p=2, dim=1)\n",
    "# Convert the embeddings to list of list of np.float32.\n",
    "embedded_question = list(map(np.float32, embedded_question))\n",
    "\n",
    "# Return top k results with AUTOINDEX.\n",
    "TOP_K = 3\n",
    "\n",
    "# Run semantic vector search using your query and the vector database.\n",
    "start_time = time.time()\n",
    "results = mc.search(\n",
    "    data=embedded_question, \n",
    "    anns_field=\"vector\", \n",
    "    # No params for AUTOINDEX\n",
    "    param={},\n",
    "    # Milvus can utilize metadata to enhance the search experience in boolean expressions.\n",
    "    # expr=\"\",\n",
    "    output_fields=[\"h1\", \"h2\", \"text\", \"source\"], \n",
    "    limit=TOP_K,\n",
    "    consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus search time: {elapsed_time} sec\")\n",
    "\n",
    "# Inspect search result.\n",
    "print(f\"type: {type(results)}, count: {len(results[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble and inspect the search result\n",
    "\n",
    "The search result is in the variable `results[0]` of type `'pymilvus.orm.search.SearchResult'`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_retrieved_context(retrieved_results, num_shot_answers=3):\n",
    "    \n",
    "    # Assemble the context as a stuffed string.\n",
    "    context = \"\"\n",
    "    i = 1\n",
    "    for r in retrieved_results[0]:\n",
    "        text = r.entity.text\n",
    "        if i <= num_shot_answers:  # only first n results\n",
    "            context += f\"{text} \"\n",
    "        i += 1\n",
    "    print(f\"Length of context: {len(context)}\")\n",
    "\n",
    "    # Also save the context metadata to retrieve along with the answer.\n",
    "    context_metadata = []\n",
    "    i = 1\n",
    "    for r in retrieved_results[0]:\n",
    "        if i <= num_shot_answers:\n",
    "            context_metadata.append({\n",
    "                \"h1\": r.entity.h1,\n",
    "                \"h2\": r.entity.h2,\n",
    "                \"source\": r.entity.source,\n",
    "            })\n",
    "        i += 1\n",
    "\n",
    "    return context, context_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of context: 1519\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "# # TODO - remove printing before saving in github.\n",
    "# for n, hits in enumerate(results):\n",
    "#     print(f\"{n}th query result\")\n",
    "#     for hit in hits:\n",
    "#         print(hit)\n",
    "\n",
    "# Assemble the context and context metadata.\n",
    "context, context_metadata = assemble_retrieved_context(results, num_shot_answers=TOP_K)\n",
    "print(len(context_metadata), len(context_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6060ce",
   "metadata": {},
   "source": [
    "## Use an LLM to Generate a chat response to the user's question using the Retrieved Context.\n",
    "\n",
    "Below, we'll use an open, very tiny generative AI model, or LLM, available on HuggingFace.  Many demos use OpenAI as the LLM choice instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_answer_sources(answer, context_metadata):\n",
    "    \"\"\"Assemble the answer and grounding sources into a string\"\"\"\n",
    "    grounded_answer = f\"Answer: {answer}\\n\"\n",
    "    grounded_answer += \"Grounding sources and citations:\\n\"\n",
    "\n",
    "    for metadata in context_metadata:\n",
    "        try:\n",
    "            grounded_answer += f\"'h1': {metadata['h1']}, 'h2':{metadata['h2']}\\n\"\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            grounded_answer += f\"'source': {metadata['source']}\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return grounded_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7fa0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do the parameters for HNSW mean?\n",
      "Answer: lazy dog\n"
     ]
    }
   ],
   "source": [
    "# BASELINING THE LLM: ASK A QUESTION WITHOUT ANY RETRIEVED CONTEXT.\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the Hugging Face auto-regressive LLM checkpoint.\n",
    "tiny_llm = \"deepset/tinyroberta-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tiny_llm)\n",
    "\n",
    "# context cannot be empty so just put random text in it.\n",
    "QA_input = {\n",
    "    'question': SAMPLE_QUESTION,\n",
    "    'context': 'The quick brown fox jumped over the lazy dog'\n",
    "}\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model=tiny_llm, \n",
    "               tokenizer=tokenizer)\n",
    "result = nlp(QA_input)\n",
    "\n",
    "# Print the question and answer.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "\n",
    "# The baseline LLM chat is not very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68e87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do the parameters for HNSW mean?\n",
      "Answer: M: Maximum degree of the node\n",
      "Grounding sources and citations:\n",
      "'h1': Index, 'h2':Milvus support to create index to accelerate vecto\n",
      "'source': https://pymilvus.readthedocs.io/en/latest/param.html'h1': Source code for milvus.client.types from enum impo, 'h2':\n",
      "'source': https://pymilvus.readthedocs.io/en/latest/_modules/milvus/client/types.html'h1': Index, 'h2':Milvus support to create index to accelerate vecto\n",
      "'source': https://pymilvus.readthedocs.io/en/latest/param.html\n"
     ]
    }
   ],
   "source": [
    "# NOW ASK THE SAME LLM THE SAME QUESTION USING THE RETRIEVED CONTEXT.\n",
    "QA_input = {\n",
    "    'question': SAMPLE_QUESTION,\n",
    "    'context': context,\n",
    "}\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model=tiny_llm, \n",
    "               tokenizer=tokenizer)\n",
    "result = nlp(QA_input)\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "answer = assemble_answer_sources(result['answer'], context_metadata)\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "print(answer)\n",
    "\n",
    "# That answer looks a little better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI to generate a more human-like chat response to the user's question \n",
    "\n",
    "We've practiced retrieval for free on our own data using open-source LLMs.  <br>\n",
    "\n",
    "Now let's make a call to the paid OpenAI GPT.\n",
    "\n",
    "ðŸ’¡ Note: Weâ€™re using a temperature of 0.0 to enable reproducible experiments. For use cases that need to always be factually grounded, use very low temperature values while more creative tasks can benefit from higher temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, pprint\n",
    "from openai import OpenAI\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Define the generation llm model to use.\n",
    "LLM_NAME = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "# See how to save api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "openai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_response(response):\n",
    "    return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "\n",
    "def generate_response(\n",
    "    llm, temperature=0.0, \n",
    "    grounding_sources=None,\n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=llm,\n",
    "            temperature=temperature,\n",
    "            api_key=openai.api_key,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "        )\n",
    "        answer = prepare_response(response=response)\n",
    "    \n",
    "        # Add the grounding sources and citations.\n",
    "        answer = assemble_answer_sources(answer, grounding_sources)\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do the parameters for HNSW mean?\n",
      "('Answer: The parameters for HNSW include M, which is the maximum degree of '\n",
      " 'the node, and efConstruction, which takes effect in the stage of index '\n",
      " 'construction. The M parameter specifies the maximum degree of nodes on each '\n",
      " 'layer of the graph, and the efConstruction parameter specifies a search '\n",
      " 'range during index construction. The ef parameter is used for specifying a '\n",
      " 'search range when searching targets. These parameters affect the performance '\n",
      " 'of the HNSW index in Milvus.\\n'\n",
      " '\\n'\n",
      " 'Sources:\\n'\n",
      " '- https://pymilvus.readthedocs.io/en/latest/param.html')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CAREFUL!! THIS COSTS MONEY!!\n",
    "# Generate response\n",
    "\n",
    "prompt = f\"\"\"Answer the question using the context provided. Be succinct.\n",
    "Echo in the answer the Grounding Sources.\n",
    "Grounding sources: {context_metadata}\n",
    "\"\"\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "                {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"question: {SAMPLE_QUESTION}, context: {context}\",\n",
    "        }\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    ")\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "# Print all choices in the response\n",
    "for i, choice in enumerate(response.choices, 1):\n",
    "    # Print the answer\n",
    "    pprint.pprint(f\"Answer: {choice.message.content}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Question1: What do the parameters for HNSW mean?\n",
    "# Answer: Correct for M, close for efConstruction and ef.\n",
    "# Best answer:  M: maximum degree of nodes in a layer of the graph. \n",
    "# efConstruction: number of nearest neighbors to consider when connecting nodes in the graph.\n",
    "# ef: number of nearest neighbors to consider when searching for similar vectors. \n",
    "\n",
    "# Question2: What are good default values for HNSW parameters with 25K vectors dim 768?\n",
    "# Answer: M=16, efConstruction=500, and ef=64\n",
    "# Best answer:  M=16, efConstruction=32, ef=32\n",
    "\n",
    "# Question3: what is the default distance metric used in AUTOINDEX in Milvus?\n",
    "# Answer: L2 \n",
    "# Trick answer:  IP inner product, not yet updated in documentation still says L2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New!! Demo Zilliz Pipelines\n",
    "\n",
    "Zilliz Pipelines is an easy way to automate data embedding and vector indexing.  It simplifies the process of getting your data into a vector database and querying it!\n",
    "\n",
    "Available in a Web UI or restful API call.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/zilliz_pipeline_s3.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "The basic workflow is:\n",
    "1. Upload your data to S3 or GCS.\n",
    "2. Open a browser to `https://cloud.zilliz.com/` and create a Cluster.\n",
    "3. Navigate through the left-side menu to `Pipelines` and follow guided steps to upload your data.\n",
    "4. Query in the web console or using API calls (shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 446011086634220126, 'distance': 0.8051614761352539, 'source': 'https://pymilvus.readthedocs.io/en/latest/api.html', 'doc_name': 'api.html', 'chunk_text': 'IVF SQ8 Hybrid index. See IVF_SQ8H.\\n\\n`IVF_PQ` _= 6_Â¶\\n\\n    \\n\\nIVF PQ index. See IVF_PQ.\\n\\n`HNSW` _= 11_Â¶\\n\\n    \\n\\nHNSW index. See HNSW.\\n\\n`ANNOY` _= 12_Â¶\\n\\n    \\n\\nANNOY index. See ANNOY.\\n\\n`IVFLAT` _= 2_Â¶\\n\\n    \\n\\nAlternative name for IVF_FLAT. Reserved for compatibility.\\n\\n`IVF_SQ8_H` _= 5_Â¶\\n\\n    \\n\\nAlternative name for IVF_SQ8H. Reserved for compatibility.\\n\\n## Metric TypeÂ¶\\n\\n_class_`milvus.``MetricType`[source]Â¶\\n\\n    \\n\\nMetric type enum.\\n\\n`INVALID` _= 0_Â¶\\n\\n    \\n\\nInvalid metric type.\\n\\n`L2` _= 1_Â¶\\n\\n    \\n\\nEuclidean distance. A metric for float vectors. See Euclidean distance.\\n\\n`IP` _= 2_Â¶\\n\\n    \\n\\nInner product. A metric for float vectors. See Inner Product.\\n\\n`HAMMING` _= 3_Â¶\\n\\n    \\n\\nHamming distance. A metric for binary vectors. See Hamming distance.\\n\\n`JACCARD` _= 4_Â¶\\n\\n    \\n\\nJaccard distance. A metric for binary vectors. See Jaccard distance.\\n\\n`TANIMOTO` _= 5_Â¶\\n\\n    \\n\\nTanimoto distance. A metric for binary vectors. See Tanimoto distance.\\n\\n`SUBSTRUCTURE` _= 6_Â¶\\n\\n    \\n\\nSuperstructure. A metric for binary vectors, only support `FLAT` index. See\\nSuperstructure.\\n\\n`SUPERSTRUCTURE` _= 7_Â¶\\n\\n    \\n\\nSubstructure. A metric for binary vectors, only support `FLAT` index. See\\nSubstructure.\\n\\nNext  Previous\\n\\n* * *\\n\\n(C) Copyright 2019-2021 Zilliz. All rights reserved.  Revision `73155ea1`.\\n\\nRead the Docs v: latest\\n\\nVersions\\n\\n    latest\\n    1.1\\n    1.0\\n    0.4.0\\n    0.3.0\\n\\nDownloads\\n\\n    pdf\\n    html\\n    epub\\n\\nOn Read the Docs\\n\\n     Project Home\\n     Builds\\n\\n* * *\\n\\nFree document hosting provided by Read the Docs.', 'chunk_id': 12}\n",
      "{'id': 446011086634255826, 'distance': 0.7528925538063049, 'source': 'https://pymilvus.readthedocs.io/en/latest/tutorial.html', 'doc_name': 'tutorial.html', 'chunk_text': '>>> collection_name = \\'demo_film_tutorial\\'\\n    >>> collection_param = {\\n    ...     \"collection_name\": collection_name,\\n    ...     \"dimension\": 8,\\n    ...     \"index_file_size\": 2048,\\n    ...     \"metric_type\": MetricType.L2\\n    ... }\\n    >>> client.create_collection(collection_param)\\n    Status(code=0, message=\\'Create collection successfully!\\')\\n    \\n\\nThen you can list collections and â€˜demo_film_tutorialâ€™ will be in the result.\\n\\n    \\n    \\n    >>> client.list_collections()\\n    (Status(code=0, message=\\'Show collections successfully!\\'), [\\'demo_film_tutorial\\'])\\n    \\n\\nYou can also get info of the collection.\\n\\n    \\n    \\n    >>> status, info = client.get_collection_info(collection_name)\\n    >>> info\\n    CollectionSchema(collection_name=\\'demo_film_tutorial\\', dimension=8, index_file_size=2048, metric_type=<MetricType: L2>)\\n    \\n\\nThe attributes of collection can be extracted from info.\\n\\n    \\n    \\n    >>> info.collection_name\\n    \\'demo_film_tutorial\\'\\n    \\n    \\n    \\n    >>> info.dimension\\n    8\\n    \\n    \\n    \\n    >>> info.index_file_size\\n    2048\\n    \\n    \\n    \\n    >>> info.metric_type\\n    <MetricType: L2>\\n    \\n\\nThis tutorial is a basic intro tutorial, building index wonâ€™t be covered by\\nthis tutorial. If you want to go further into Milvus with indexes, itâ€™s\\nrecommended to check our index examples.\\n\\nIf youâ€™re already known about indexes from `index examples`, and you want a\\nfull lists of params supported by PyMilvus, you check out Index chapter of the\\nPyMilvus documentation.', 'chunk_id': 2}\n",
      "{'id': 446011086634256006, 'distance': 0.7514341473579407, 'source': 'https://pymilvus.readthedocs.io/en/latest/_modules/milvus/client/stub.html', 'doc_name': 'stub.html', 'chunk_text': 'index_file_size = collection_param.get(\\'index_file_size\\', 1024)\\n            collection_param.pop(\\'index_file_size\\', None)\\n    \\n            metric_type = collection_param.get(\\'metric_type\\', MetricType.L2)\\n            collection_param.pop(\\'metric_type\\', None)\\n    \\n            check_pass_param(collection_name=collection_name, dimension=dim,\\n                             index_file_size=index_file_size, metric_type=metric_type)\\n    \\n            with self._connection() as handler:\\n                return handler.create_collection(collection_name, dim, index_file_size,\\n                                                 metric_type, collection_param, timeout)\\n    \\n    \\n    \\n    \\n    \\n    [docs]    @check_connect\\n        def has_collection(self, collection_name, timeout=30):\\n            \"\"\"\\n    \\n            Checks whether a collection exists.\\n    \\n            :param collection_name: Name of the collection to check.\\n            :type  collection_name: str\\n            :param timeout: An optional duration of time in seconds to allow for the RPC. When timeout\\n                            is set to None, client waits until server responses or error occurs.\\n            :type  timeout: float\\n    \\n            :return: The operation status and the flag indicating if collection exists. Succeed\\n                     if `Status.OK()` is `True`. If status is not OK, the flag is always `False`.\\n            :rtype: Status, bool\\n    \\n            \"\"\"\\n            check_pass_param(collection_name=collection_name)', 'chunk_id': 7}\n",
      "Length context: 4496, Len metadata: 3\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "# Define the URL, headers, and data\n",
    "url = \"https://controller.api.gcp-us-west1.zillizcloud.com/v1/pipelines/pipe-458714b66ffc8ff3ab1bf1/run\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "data = {\n",
    "    \"data\": {\n",
    "        \"query_text\": SAMPLE_QUESTION\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"limit\": TOP_K,\n",
    "        \"offset\": 0,\n",
    "        \"outputFields\": [\"chunk_text\", \"chunk_id\", \"doc_name\", \"source\"],\n",
    "        \"filter\": \"chunk_id >= 0\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# print(type(response))\n",
    "# # Print the response\n",
    "# pprint.pprint(response.json())\n",
    "\n",
    "# Assemble context from Pipeline retriever.\n",
    "pipeline_context = \"\"\n",
    "pipeline_context_metadata = []\n",
    "\n",
    "# Loop through each result in the response\n",
    "for result in response.json()['data']['result']:\n",
    "    print(result)\n",
    "    # Add the chunk_text to the context\n",
    "    pipeline_context += result['chunk_text'] + \" \"\n",
    "\n",
    "    # Add the source to the context metadata\n",
    "    pipeline_context_metadata.append({\n",
    "        \"source\": result['source'],\n",
    "    })\n",
    "\n",
    "print(f\"Length context: {len(pipeline_context)}, Len metadata: {len(pipeline_context_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do the parameters for HNSW mean?\n",
      "('Answer: The parameter for HNSW specifies the number of connection edges on '\n",
      " 'each subgraph node. By setting the parameter for HNSW, you determine the '\n",
      " 'trade-off between recall and index build time. You can find more information '\n",
      " 'on index parameters in the PyMilvus documentation.\\n'\n",
      " '\\n'\n",
      " 'Sources:\\n'\n",
      " '- [PyMilvus Documentation - '\n",
      " 'API](https://pymilvus.readthedocs.io/en/latest/api.html)\\n'\n",
      " '- [PyMilvus Documentation - '\n",
      " 'Tutorial](https://pymilvus.readthedocs.io/en/latest/tutorial.html)\\n'\n",
      " '- [PyMilvus Documentation - '\n",
      " 'Stub](https://pymilvus.readthedocs.io/en/latest/_modules/milvus/client/stub.html)')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CAREFUL!! THIS COSTS MONEY!!\n",
    "# Generate response\n",
    "prompt = f\"\"\"Answer the question using the context provided. Be succinct.\n",
    "Echo in the answer the Grounding sources.\n",
    "Grounding sources: {pipeline_context_metadata}\n",
    "\"\"\"\n",
    "\n",
    "response_pipeline = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "                {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"question: {SAMPLE_QUESTION}, context: {pipeline_context}\",\n",
    "        }\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    ")\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "# Print all choices in the response\n",
    "for i, choice in enumerate(response_pipeline.choices, 1):\n",
    "    pprint.pprint(f\"Answer: {choice.message.content}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Question1: What do the parameters for HNSW mean?\n",
    "# Answer: Only gave parameter 'M'.  Missed 'efConstruction' and 'ef'.\n",
    "# Best answer:  M: maximum degree of nodes in a layer of the graph. \n",
    "# efConstruction: number of nearest neighbors to consider when connecting nodes in the graph.\n",
    "# ef: number of nearest neighbors to consider when searching for similar vectors. \n",
    "\n",
    "# Question2: What are good default values for HNSW parameters with 25K vectors dim 768?\n",
    "# Answer: `M=16` and `efConstruction=200`\n",
    "# Best answer:  M=16, efConstruction=32, ef=32\n",
    "\n",
    "# Question3: what is the default distance metric used in AUTOINDEX in Milvus?\n",
    "# Answer: L2\n",
    "# Trick answer:  IP inner product, not yet updated in documentation still says L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0e81e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collection\n",
    "utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c777937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Christy Bergman\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.6\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "torch                : 2.1.1\n",
      "transformers         : 4.35.2\n",
      "sentence_transformers: 2.2.2\n",
      "pymilvus             : 2.3.4\n",
      "langchain            : 0.0.322\n",
      "openai               : 1.3.7\n",
      "\n",
      "conda environment: py311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,transformers,sentence_transformers,pymilvus,langchain,openai --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
